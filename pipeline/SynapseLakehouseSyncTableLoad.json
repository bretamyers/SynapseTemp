{
	"name": "SynapseLakehouseSyncTableLoad",
	"properties": {
		"description": "The pipeline that orchestrates the staging and loading of the data from the delta change data feed to the Synapse dedicate pool tables.",
		"activities": [
			{
				"name": "Filter - Deletes",
				"description": "Filter ChangeTypesArray parameter to tables that have delete as a change type.",
				"type": "Filter",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@pipeline().parameters.DatabricksOutput['ChangeTypes']",
						"type": "Expression"
					},
					"condition": {
						"value": "@contains(item(), 'delete')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "Filter - Update",
				"description": "Filter ChangeTypesArray parameter to tables that have updates as a change type.",
				"type": "Filter",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@pipeline().parameters.DatabricksOutput['ChangeTypes']",
						"type": "Expression"
					},
					"condition": {
						"value": "@contains(item(), 'update_postimage')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "Filter - Insert",
				"description": "Filter ChangeTypesArray parameter to tables that have insert as a change type.",
				"type": "Filter",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@pipeline().parameters.DatabricksOutput['ChangeTypes']",
						"type": "Expression"
					},
					"condition": {
						"value": "@contains(item(), 'insert')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "Filter - Full Load",
				"description": "Loop through tables that had full_load changes to the records.",
				"type": "Filter",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@pipeline().parameters.DatabricksOutput['ChangeTypes']",
						"type": "Expression"
					},
					"condition": {
						"value": "@contains(item(), 'full_load')",
						"type": "Expression"
					}
				}
			},
			{
				"name": "ForEach - Pool - Deletes",
				"description": "Loop through tables that had delete changes to the records. The pipeline is at the table grain already but the loop is a way to separate and wrap the different steps.",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "Filter - Insert",
						"dependencyConditions": [
							"Succeeded"
						]
					},
					{
						"activity": "Filter - Update",
						"dependencyConditions": [
							"Succeeded"
						]
					},
					{
						"activity": "Filter - Deletes",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('Filter - Deletes').output.value",
						"type": "Expression"
					},
					"activities": [
						{
							"name": "Get Resource Class Required",
							"description": "Profile the data that was loaded into the staging table and get determine the minimum static resource that should be used to load the data.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Execute COPY INTO Staging Statement",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat(replace(replace(replace(variables('GetResourceClass')\n, '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_delete'))\n, '{PipelineId}', pipeline().parameters.PipelineValue[0].PipelineRunId)\n)",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": true
							}
						},
						{
							"name": "Create Staging Table DDL - Serverless",
							"description": "Profile the data using the Synapse serverless engine and generate the optimal table DDL.",
							"type": "Lookup",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@replace(replace(replace(variables('SqlCommandCreateStagingTable'), '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_delete'))\n,  '{FolderPathFull}', concat(pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName'], '/_change_type=delete'))",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": "master"
									}
								},
								"firstRowOnly": false
							}
						},
						{
							"name": "Create Staging Tables",
							"description": "Execute the DDL for the creation of the staging table from the previous step in the Synapse dedicated pool.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Create Staging Table DDL - Serverless",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' '\n, activity('Create Staging Table DDL - Serverless').output.value[0].CreateTableDDL, '; SELECT 1 AS a')",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": false
							}
						},
						{
							"name": "Execute COPY INTO Staging Statement",
							"description": "Execute the COPY INTO statement to move the data from ADLS to the newly created staging table.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Create Staging Tables",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' COPY INTO ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_delete'\n, '] FROM '''\n, pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName']\n, '/_change_type=delete'\n, ''' WITH ( FILE_TYPE = ''PARQUET'', MAXERRORS = 0, COMPRESSION = ''snappy'', IDENTITY_INSERT = ''OFF'', CREDENTIAL = (IDENTITY = ''Managed Identity'')) OPTION (LABEL = ''COPY - ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_delete', '] - '\n, pipeline().parameters.PipelineValue[0].PipelineRunId, ''');SELECT 1 AS a')",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": false
							}
						},
						{
							"name": "Script - Delete",
							"description": "Execute the delete statement on the final table from the data that was staged.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Get Resource Class Required",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat(replace(replace(replace(replace(variables('sqlDelete')\n,'{SchemaName}', pipeline().parameters.DatabricksOutput['SchemaName'])\n,'{TableName}', pipeline().parameters.DatabricksOutput['TableName'])\n,'{UserName}', activity('Get Resource Class Required').output.firstRow.UserName)\n,'{KeyColumns}', pipeline().parameters.DatabricksOutput['KeyColumns'])\n,' SELECT 1 AS a'\n)",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": true
							}
						}
					]
				}
			},
			{
				"name": "ForEach - Pool - Updates",
				"description": "Loop through tables that had update changes to the records. The pipeline is at the table grain already but the loop is a way to separate and wrap the different steps.",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "ForEach - Pool - Deletes",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('Filter - Update').output.value",
						"type": "Expression"
					},
					"activities": [
						{
							"name": "Get Resource Class Required - Update",
							"description": "Profile the data that was loaded into the staging table and get determine the minimum static resource that should be used to load the data.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Execute COPY INTO Staging Statement - Update",
									"dependencyConditions": [
										"Succeeded"
									]
								},
								{
									"activity": "Alter Base Tables Column Data Types - Update",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat(replace(replace(replace(variables('GetResourceClass')\n, '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_update'))\n, '{PipelineId}', pipeline().parameters.PipelineValue[0].PipelineRunId)\n)",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": true
							}
						},
						{
							"name": "Create Staging Table DDL - Serverless - Update",
							"description": "Profile the data using the Synapse serverless engine and generate the optimal table DDL.",
							"type": "Lookup",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@replace(replace(replace(variables('SqlCommandCreateStagingTable'), '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_update'))\n,  '{FolderPathFull}', concat(pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName'], '/_change_type=update_postimage'))\n",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": "master"
									}
								},
								"firstRowOnly": false
							}
						},
						{
							"name": "Create Staging Tables - Update",
							"description": "Execute the DDL for the creation of the staging table from the previous step in the Synapse dedicated pool.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Create Staging Table DDL - Serverless - Update",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' '\n, activity('Create Staging Table DDL - Serverless - Update').output.value[0].CreateTableDDL, '; SELECT 1 AS a')",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": false
							}
						},
						{
							"name": "Execute COPY INTO Staging Statement - Update",
							"description": "Execute the COPY INTO statement to move the data from ADLS to the newly created staging table.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Create Staging Tables - Update",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' COPY INTO ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_update'\n, '] FROM '''\n, pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName']\n, '/_change_type=update_postimage'\n, ''' WITH ( FILE_TYPE = ''PARQUET'', MAXERRORS = 0, COMPRESSION = ''snappy'', IDENTITY_INSERT = ''OFF'', CREDENTIAL = (IDENTITY = ''Managed Identity'')) OPTION (LABEL = ''COPY - ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_update', '] - '\n, pipeline().parameters.PipelineValue[0].PipelineRunId, ''');SELECT 1 AS a')",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": false
							}
						},
						{
							"name": "Script - Update",
							"description": "Execute the update statement on the final table from the data that was staged.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Get Resource Class Required - Update",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat(replace(replace(replace(replace(variables('sqlUpdate')\n,'{SchemaName}', pipeline().parameters.DatabricksOutput['SchemaName'])\n,'{TableName}', pipeline().parameters.DatabricksOutput['TableName'])\n,'{KeyColumns}', pipeline().parameters.DatabricksOutput['KeyColumns'])\n,'{UserName}', activity('Get Resource Class Required - Update').output.firstRow.UserName)\n,' SELECT 1 AS a'\n)",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": true
							}
						},
						{
							"name": "Alter Base Tables Column Data Types - Update",
							"description": "Update the final tables data types for the column if the data staged has a larger datatype defined. For example, the final table has varchar(10) but the staging table has varchar(20). We then alter the final table to be varchar(20).",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Create Staging Tables - Update",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat(replace(replace(replace(variables('sqlAlterColumn')\n, '{SchemaName}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameBase}', pipeline().parameters.DatabricksOutput['TableName'])\n, '{TableNameStage}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_update'))\n, '; SELECT 1 AS a')\n",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": false
							}
						}
					]
				}
			},
			{
				"name": "ForEach - Pool - Inserts",
				"description": "Loop through tables that had insert changes to the records. The pipeline is at the table grain already but the loop is a way to separate and wrap the different steps.",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "ForEach - Pool - Updates",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('Filter - Insert').output.value",
						"type": "Expression"
					},
					"activities": [
						{
							"name": "Get Resource Class Required - Insert",
							"description": "Profile the data that was loaded into the staging table and get determine the minimum static resource that should be used to load the data.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Execute COPY INTO Staging Statement - Insert",
									"dependencyConditions": [
										"Succeeded"
									]
								},
								{
									"activity": "Alter Base Tables Column Data Types - Insert",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat(replace(replace(replace(variables('GetResourceClass')\n, '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_insert'))\n, '{PipelineId}', pipeline().parameters.PipelineValue[0].PipelineRunId)\n)",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": true
							}
						},
						{
							"name": "Create Staging Table DDL - Serverless - Insert",
							"description": "Profile the data using the Synapse serverless engine and generate the optimal table DDL.",
							"type": "Lookup",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@replace(replace(replace(variables('SqlCommandCreateStagingTable'), '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_insert'))\n, '{FolderPathFull}', concat(pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName'], '/_change_type=insert'))",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": "master"
									}
								},
								"firstRowOnly": false
							}
						},
						{
							"name": "Create Staging Tables - Insert",
							"description": "Execute the DDL for the creation of the staging table from the previous step in the Synapse dedicated pool.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Create Staging Table DDL - Serverless - Insert",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' '\n, activity('Create Staging Table DDL - Serverless - Insert').output.value[0].CreateTableDDL, '; SELECT 1 AS a')",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": false
							}
						},
						{
							"name": "Execute COPY INTO Staging Statement - Insert",
							"description": "Execute the COPY INTO statement to move the data from ADLS to the newly created staging table.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Create Staging Tables - Insert",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' COPY INTO ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_insert'\n, '] FROM '''\n, pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName']\n, '/_change_type=insert'\n, ''' WITH ( FILE_TYPE = ''PARQUET'', MAXERRORS = 0, COMPRESSION = ''snappy'', IDENTITY_INSERT = ''OFF'', CREDENTIAL = (IDENTITY = ''Managed Identity'')) OPTION (LABEL = ''COPY - ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_insert', '] - '\n, pipeline().parameters.PipelineValue[0].PipelineRunId, ''');SELECT 1 AS a')",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": false
							}
						},
						{
							"name": "Script - Insert",
							"description": "Execute the insert statement on the final table from the data that was staged.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Get Resource Class Required - Insert",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat(replace(replace(replace(replace(variables('sqlInsert')\n,'{SchemaName}', pipeline().parameters.DatabricksOutput['SchemaName'])\n,'{TableName}', pipeline().parameters.DatabricksOutput['TableName'])\n,'{KeyColumns}', pipeline().parameters.DatabricksOutput['KeyColumns'])\n,'{UserName}', activity('Get Resource Class Required - Insert').output.firstRow.UserName)\n,' SELECT 1 AS a'\n)\n",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": true
							}
						},
						{
							"name": "Alter Base Tables Column Data Types - Insert",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Create Staging Tables - Insert",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat(replace(replace(replace(variables('sqlAlterColumn')\n, '{SchemaName}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameBase}', pipeline().parameters.DatabricksOutput['TableName'])\n, '{TableNameStage}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_insert'))\n, '; SELECT 1 AS a')\n",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": false
							}
						}
					]
				}
			},
			{
				"name": "ForEach - Create and Load the Synapse Table",
				"description": "Loop through tables that had full_load changes to the records. The pipeline is at the table grain already but the loop is a way to separate and wrap the different steps.",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "Filter - Full Load",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('Filter - Full Load').output.value",
						"type": "Expression"
					},
					"isSequential": false,
					"activities": [
						{
							"name": "Get Resource Class Required - Full Load",
							"description": "Profile the data that was loaded into the staging table and get determine the minimum static resource that should be used to load the data.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Execute COPY INTO Staging Statement - Full Load",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat(replace(replace(replace(variables('GetResourceClass')\n, '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_full_load'))\n, '{PipelineId}', pipeline().parameters.PipelineValue[0].PipelineRunId)\n)",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": true
							}
						},
						{
							"name": "Create Staging Table DDL - Serverless - Full Load",
							"description": "Profile the data using the Synapse serverless engine and generate the optimal table DDL.",
							"type": "Lookup",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@replace(replace(replace(variables('SqlCommandCreateStagingTable'), '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_full_load'))\n,  '{FolderPathFull}', concat(pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName'], '/_change_type=full_load'))",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": "master"
									}
								},
								"firstRowOnly": false
							}
						},
						{
							"name": "Create Staging Tables - Full Load",
							"description": "Execute the DDL for the creation of the staging table from the previous step in the Synapse dedicated pool.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Create Staging Table DDL - Serverless - Full Load",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' '\n, activity('Create Staging Table DDL - Serverless - Full Load').output.value[0].CreateTableDDL, '; SELECT 1 AS a')",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": false
							}
						},
						{
							"name": "Execute COPY INTO Staging Statement - Full Load",
							"description": "Execute the COPY INTO statement to move the data from ADLS to the newly created staging table.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Create Staging Tables - Full Load",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' COPY INTO ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_full_load'\n, '] FROM '''\n, pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName']\n, '/_change_type=full_load'\n, ''' WITH ( FILE_TYPE = ''PARQUET'', MAXERRORS = 0, COMPRESSION = ''snappy'', IDENTITY_INSERT = ''OFF'', CREDENTIAL = (IDENTITY = ''Managed Identity'')) OPTION (LABEL = ''COPY - ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_full_load', '] - '\n, pipeline().parameters.PipelineValue[0].PipelineRunId, ''');SELECT 1 AS a')",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": false
							}
						},
						{
							"name": "Profile Staging Table",
							"description": "Profile the staging table and try to determine the best distribution and index.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Get Resource Class Required - Full Load",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat(replace(replace(replace(replace(replace(replace(replace(variables('CTAS')\n\t, '{ResourceClassName}', activity('Get Resource Class Required - Full Load').output.firstRow.UserName)\n\t, '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n\t, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_full_load'))\n\t, '{TableNameFinal}', pipeline().parameters.DatabricksOutput['TableName'])\n\t, '{PipelineRunId}', pipeline().parameters.PipelineValue[0].PipelineRunId)\n\t, '{PipelineStartDate}', pipeline().parameters.PipelineValue[0].PipelineStartDate)\n\t, '{PipelineStartDateTime}', pipeline().parameters.PipelineValue[0].PipelineStartDateTime)\n)",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								}
							}
						},
						{
							"name": "Create Final Tables - Auto Distribution and Index",
							"description": "Run the CTAS command to load the data from the staging table to the final table with the appropriate distribution and index defined.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Drop Table if Exists",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@CONCAT('/*Auto Distribution/Index*/ EXECUTE AS user = ''', activity('Get Resource Class Required - Full Load').output.firstRow.UserName\n, ''' IF OBJECT_ID(''', pipeline().parameters.DatabricksOutput['SchemaName'], '.', pipeline().parameters.DatabricksOutput['TableName'], ''', ''U'') IS NOT NULL DROP TABLE [', pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName']\n, ']; CREATE TABLE [', pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '] ', activity('Profile Staging Table').output.firstRow.DistributionIndex,' AS SELECT * FROM [', pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_full_load]  OPTION (LABEL = ''CTAS - [', pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '] - ', pipeline().parameters.PipelineValue[0].PipelineRunId, ''');SELECT 1 AS a')\n\n",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": false
							}
						},
						{
							"name": "Drop Table if Exists",
							"description": "Drop the staging table if it exists in Synapse dedicated pool.",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Profile Staging Table",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat('IF OBJECT_ID(''', pipeline().parameters.DatabricksOutput['SchemaName'], '.', pipeline().parameters.DatabricksOutput['TableName'], ''', ''U'') IS NOT NULL\n\tDROP TABLE [', pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '];'\n, ' SELECT 1 AS a')",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								},
								"firstRowOnly": false
							}
						}
					]
				}
			},
			{
				"name": "ForEach - Drop Temp Tables",
				"description": "If the DropTableFlag parameter is true, then drop the staging tables that are created during the loading process after the data has been loaded. This will keep the Synapse Dedicated Pool clean.",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "ForEach - Create and Load the Synapse Table",
						"dependencyConditions": [
							"Succeeded"
						]
					},
					{
						"activity": "ForEach - Pool - Inserts",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@if(bool(pipeline().parameters.DatabricksOutput['DropTableFlag'])\n, pipeline().parameters.DatabricksOutput['ChangeTypes']\n, take(array(''), 0)\n)",
						"type": "Expression"
					},
					"activities": [
						{
							"name": "Drop Staging Tables",
							"type": "Lookup",
							"dependsOn": [],
							"policy": {
								"timeout": "7.00:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "SqlDWSource",
									"sqlReaderQuery": {
										"value": "@concat('IF OBJECT_ID(''', pipeline().parameters.DatabricksOutput['SchemaName'], '.', pipeline().parameters.DatabricksOutput['TableName'], '_', if(equals(item(), 'update_postimage'), 'update', item()), ''', ''U'') IS NOT NULL\n    DROP TABLE [', pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_', if(equals(item(), 'update_postimage'), 'update', item()), '];'\n, ' SELECT 1 AS a')",
										"type": "Expression"
									},
									"queryTimeout": "24:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "DS_Synapse_Managed_Identity",
									"type": "DatasetReference",
									"parameters": {
										"ServerName": {
											"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
											"type": "Expression"
										},
										"DatabaseName": {
											"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
											"type": "Expression"
										}
									}
								}
							}
						}
					]
				}
			},
			{
				"name": "Spark - Synapse Lakehouse Sync Log Success",
				"description": "Call the Azure Databricks notebook to log in the _SynapseLakehouseSync delta table that the data was loaded successfully into Synapse.",
				"type": "DatabricksNotebook",
				"dependsOn": [
					{
						"activity": "ForEach - Drop Temp Tables",
						"dependencyConditions": [
							"Succeeded"
						]
					},
					{
						"activity": "Add TableRowCountSynapse to Json Object",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "7.00:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"notebookPath": "/Synapse Lakehouse Sync/Synapse Lakehouse Sync Tracking Table Log Success",
					"baseParameters": {
						"SynapseLakehouseSyncParameters": {
							"value": "@string(activity('Add TableRowCountSynapse to Json Object').output.value[0])",
							"type": "Expression"
						},
						"TableRowCountSynapse": {
							"value": "@string(activity('Log Row Count - SynapseLakehouseSync').output.value[0].TableRowCountSynapse)",
							"type": "Expression"
						}
					}
				},
				"linkedServiceName": {
					"referenceName": "LS_AzureDatabricks_Managed_Identity",
					"type": "LinkedServiceReference"
				}
			},
			{
				"name": "Log Row Count - SynapseLakehouseSync",
				"description": "Log the row count for the table in the logging.SynapseLakehouseSyncRecordCount logging table.",
				"type": "Lookup",
				"dependsOn": [
					{
						"activity": "ForEach - Create and Load the Synapse Table",
						"dependencyConditions": [
							"Succeeded"
						]
					},
					{
						"activity": "ForEach - Pool - Inserts",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "SqlDWSource",
						"sqlReaderQuery": {
							"value": "@concat('EXECUTE AS USER = ''Userstaticrc10''; \nDECLARE @RowInsertDateTime DATETIME2(0) = GETDATE()\n    ,@TableRowCountSynapse BIGINT = (SELECT COUNT_BIG(*) FROM ', pipeline().parameters.DatabricksOutput['SchemaName'], '.', pipeline().parameters.DatabricksOutput['TableName'], ')\nINSERT logging.SynapseLakehouseSync (PoolName, SchemaName, TableName, TableRowCountADLS, TableRowCountSynapse, RowInsertDateTime)\nVALUES (\n''', pipeline().parameters.DatabricksOutput['PoolName'],''',\n''', pipeline().parameters.DatabricksOutput['SchemaName'],''',\n''', pipeline().parameters.DatabricksOutput['TableName'],''', '\n, pipeline().parameters.DatabricksOutput.TableRowCountADLS\n, ', @TableRowCountSynapse, @RowInsertDateTime)\n; \n\nSELECT @TableRowCountSynapse AS TableRowCountSynapse\n')",
							"type": "Expression"
						},
						"queryTimeout": "24:00:00",
						"partitionOption": "None"
					},
					"dataset": {
						"referenceName": "DS_Synapse_Managed_Identity",
						"type": "DatasetReference",
						"parameters": {
							"ServerName": {
								"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
								"type": "Expression"
							},
							"DatabaseName": {
								"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
								"type": "Expression"
							}
						}
					},
					"firstRowOnly": false
				}
			},
			{
				"name": "Add TableRowCountSynapse to Json Object",
				"description": "Add the TableRowCountSynapse a json object.",
				"type": "Lookup",
				"dependsOn": [
					{
						"activity": "Log Row Count - SynapseLakehouseSync",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "SqlDWSource",
						"sqlReaderQuery": {
							"value": "@concat('\nDECLARE json NVARCHAR(MAX) = '''\n, string(pipeline().parameters.DatabricksOutput)\n, ''' \n\n;WITH cte AS\n(\n\tSELECT PoolName, SynapseLakehouseSyncKey, ', activity('Log Row Count - SynapseLakehouseSync').output.value[0], ' AS TableRowCountSynapse\n\t\t,SynapseSyncDataADLSFullPath, SynapseSyncDataDatabricksKeyVaultScope, SynapseSyncDataAzureKeyVaultSecretName\n\tFROM OPENJSON(json)\n\tWITH\n\t(\n\t\tPoolName NVARCHAR(1000) ''$.PoolName''\n\t\t,SynapseLakehouseSyncKey NVARCHAR(1000) ''$.SynapseLakehouseSyncKey''\n\t\t,SynapseSyncDataADLSFullPath NVARCHAR(1000) ''$.SynapseSyncDataADLSFullPath''\n\t\t,SynapseSyncDataDatabricksKeyVaultScope NVARCHAR(1000) ''$.SynapseSyncDataDatabricksKeyVaultScope''\n\t\t,SynapseSyncDataAzureKeyVaultSecretName NVARCHAR(1000) ''$.SynapseSyncDataAzureKeyVaultSecretName''\n\t)\n)\nSELECT *, (SELECT * FROM cte FOR JSON AUTO, WITHOUT_ARRAY_WRAPPER) AS JsonString\nFROM cte\n')",
							"type": "Expression"
						},
						"queryTimeout": "24:00:00",
						"partitionOption": "None"
					},
					"dataset": {
						"referenceName": "DS_Synapse_Managed_Identity",
						"type": "DatasetReference",
						"parameters": {
							"ServerName": {
								"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
								"type": "Expression"
							},
							"DatabaseName": "master"
						}
					},
					"firstRowOnly": false
				}
			}
		],
		"parameters": {
			"PipelineValue": {
				"type": "array",
				"defaultValue": [
					{
						"PipelineRunId": "d0aff143-70d5-428d-a5d9-a784efe71db8",
						"PipelineStartDate": "20220908",
						"PipelineStartDateTime": "2022-09-08 09:58:34"
					}
				]
			},
			"DatabricksOutput": {
				"type": "object",
				"defaultValue": {
					"SynapseSync_abfss": "abfss://data@synapsesynccav.dfs.core.windows.net/Sample",
					"SynapseSync_https": "https://synapsesynccav.dfs.core.windows.net/data/Sample",
					"ChangeTypes": [
						"full_load"
					],
					"ExistsFlagSynapse": "True",
					"PoolName": "DataWarehouse",
					"SchemaName": "AdventureWorks",
					"TableName": "DimProduct",
					"KeyColumns": "_Id",
					"DeltaDataADLSFullPath": "abfss://data@synapsesynccav.dfs.core.windows.net/Sample/AdventureWorks/DimProduct",
					"DeltaDataDatabricksKeyVaultScope": "DataLakeStorageKey",
					"DeltaDataAzureKeyVaultSecretName": "DataLakeStorageKey",
					"SynapseSyncDataADLSFullPath": "abfss://data@synapsesynccav.dfs.core.windows.net/Sample/",
					"SynapseSyncDataDatabricksKeyVaultScope": "DataLakeStorageKey",
					"SynapseSyncDataAzureKeyVaultSecretName": "DataLakeStorageKey",
					"DropTableFlag": "True",
					"SynapseLakehouseSyncKey": "b5c6db9cba09e87635703ecfb38d3717"
				}
			}
		},
		"variables": {
			"GetResourceClass": {
				"type": "String",
				"defaultValue": "DECLARE @DWU VARCHAR(8) \t,@SCHEMA_NAME VARCHAR(128) = '{SchemaNameStaging}' \t,@TABLE_NAME VARCHAR(128) = '{TableNameStaging}' \t,@Optmial_Resource_Class VARCHAR(100) \t,@Set_Resource_Class VARCHAR(1000)  SELECT @DWU = 'DW' + CAST(CASE  \t\t\tWHEN Mem > 4 \t\t\t\tTHEN Nodes * 500 \t\t\tELSE Mem * 100 \t\t\tEND AS VARCHAR(10)) + 'c' FROM ( \tSELECT Nodes = count(DISTINCT n.pdw_node_id) /*,Mem=max(i.committed_target_kb/1000/1000/60)*/ \t\t,Mem = CAST(max(i.committed_target_kb / 1000.0 / 1000 / 60) AS DECIMAL(10, 0)) \tFROM sys.dm_pdw_nodes n \tCROSS APPLY sys.dm_pdw_nodes_os_sys_info i \tWHERE type = 'COMPUTE' \t) AS A  IF OBJECT_ID('tempdb..#ref') IS NOT NULL BEGIN \tDROP TABLE #ref; END;  CREATE TABLE #ref \tWITH (DISTRIBUTION = ROUND_ROBIN) AS WITH alloc AS ( \t\tSELECT 'DW100c' AS DWU \t\t\t,4 AS max_queries \t\t\t,4 AS max_slots \t\t\t,1 AS slots_used_smallrc \t\t\t,1 AS slots_used_mediumrc \t\t\t,2 AS slots_used_largerc \t\t\t,4 AS slots_used_xlargerc \t\t\t,1 AS slots_used_staticrc10 \t\t\t,2 AS slots_used_staticrc20 \t\t\t,4 AS slots_used_staticrc30 \t\t\t,4 AS slots_used_staticrc40 \t\t\t,4 AS slots_used_staticrc50 \t\t\t,4 AS slots_used_staticrc60 \t\t\t,4 AS slots_used_staticrc70 \t\t\t,4 AS slots_used_staticrc80 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW200c' \t\t\t,8 \t\t\t,8 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,8 \t\t\t,8 \t\t\t,8 \t\t\t,8 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW300c' \t\t\t,12 \t\t\t,12 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,8 \t\t\t,8 \t\t\t,8 \t\t\t,8 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW400c' \t\t\t,16 \t\t\t,16 \t\t\t,1 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,16 \t\t\t,16 \t\t\t,16 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW500c' \t\t\t,20 \t\t\t,20 \t\t\t,1 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,16 \t\t\t,16 \t\t\t,16 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW1000c' \t\t\t,32 \t\t\t,40 \t\t\t,1 \t\t\t,4 \t\t\t,8 \t\t\t,28 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,32 \t\t\t,32 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW1500c' \t\t\t,32 \t\t\t,60 \t\t\t,1 \t\t\t,6 \t\t\t,13 \t\t\t,42 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,32 \t\t\t,32 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW2000c' \t\t\t,48 \t\t\t,80 \t\t\t,2 \t\t\t,8 \t\t\t,17 \t\t\t,56 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,64 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW2500c' \t\t\t,48 \t\t\t,100 \t\t\t,3 \t\t\t,10 \t\t\t,22 \t\t\t,70 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,64 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW3000c' \t\t\t,64 \t\t\t,120 \t\t\t,3 \t\t\t,12 \t\t\t,26 \t\t\t,84 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,64 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW5000c' \t\t\t,64 \t\t\t,200 \t\t\t,6 \t\t\t,20 \t\t\t,44 \t\t\t,140 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,128 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW6000c' \t\t\t,128 \t\t\t,240 \t\t\t,7 \t\t\t,24 \t\t\t,52 \t\t\t,168 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,128 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW7500c' \t\t\t,128 \t\t\t,300 \t\t\t,9 \t\t\t,30 \t\t\t,66 \t\t\t,210 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,128 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW10000c' \t\t\t,128 \t\t\t,400 \t\t\t,12 \t\t\t,40 \t\t\t,88 \t\t\t,280 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,128 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW15000c' \t\t\t,128 \t\t\t,600 \t\t\t,18 \t\t\t,60 \t\t\t,132 \t\t\t,420 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,128 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW30000c' \t\t\t,128 \t\t\t,1200 \t\t\t,36 \t\t\t,120 \t\t\t,264 \t\t\t,840 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,128 \t\t) \t,map AS ( \t\tSELECT CONVERT(VARCHAR(20), 'SloDWGroupSmall') AS wg_name \t\t\t,slots_used_smallrc AS slots_used \t\tFROM alloc \t\tWHERE DWU = @DWU \t\t \t\tUNION ALL \t\t \t\tSELECT CONVERT(VARCHAR(20), 'SloDWGroupMedium') AS wg_name \t\t\t,slots_used_mediumrc AS slots_used \t\tFROM alloc \t\tWHERE DWU = @DWU \t\t \t\tUNION ALL \t\t \t\tSELECT CONVERT(VARCHAR(20), 'SloDWGroupLarge') AS wg_name \t\t\t,slots_used_largerc AS slots_used \t\tFROM alloc \t\tWHERE DWU = @DWU \t\t \t\tUNION ALL \t\t \t\tSELECT CONVERT(VARCHAR(20), 'SloDWGroupXLarge') AS wg_name \t\t\t,slots_used_xlargerc AS slots_used \t\tFROM alloc \t\tWHERE DWU = @DWU \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC00' \t\t\t,1 \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC01' \t\t\t,2 \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC02' \t\t\t,4 \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC03' \t\t\t,8 \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC04' \t\t\t,16 \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC05' \t\t\t,32 \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC06' \t\t\t,64 \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC07' \t\t\t,128 \t\t) \t,ref AS ( \t\tSELECT a1.* \t\t\t,m1.wg_name AS wg_name_smallrc \t\t\t,m1.slots_used * 250 AS tgt_mem_grant_MB_smallrc \t\t\t,m2.wg_name AS wg_name_mediumrc \t\t\t,m2.slots_used * 250 AS tgt_mem_grant_MB_mediumrc \t\t\t,m3.wg_name AS wg_name_largerc \t\t\t,m3.slots_used * 250 AS tgt_mem_grant_MB_largerc \t\t\t,m4.wg_name AS wg_name_xlargerc \t\t\t,m4.slots_used * 250 AS tgt_mem_grant_MB_xlargerc \t\t\t,m5.wg_name AS wg_name_staticrc10 \t\t\t,m5.slots_used * 250 AS tgt_mem_grant_MB_staticrc10 \t\t\t,m6.wg_name AS wg_name_staticrc20 \t\t\t,m6.slots_used * 250 AS tgt_mem_grant_MB_staticrc20 \t\t\t,m7.wg_name AS wg_name_staticrc30 \t\t\t,m7.slots_used * 250 AS tgt_mem_grant_MB_staticrc30 \t\t\t,m8.wg_name AS wg_name_staticrc40 \t\t\t,m8.slots_used * 250 AS tgt_mem_grant_MB_staticrc40 \t\t\t,m9.wg_name AS wg_name_staticrc50 \t\t\t,m9.slots_used * 250 AS tgt_mem_grant_MB_staticrc50 \t\t\t,m10.wg_name AS wg_name_staticrc60 \t\t\t,m10.slots_used * 250 AS tgt_mem_grant_MB_staticrc60 \t\t\t,m11.wg_name AS wg_name_staticrc70 \t\t\t,m11.slots_used * 250 AS tgt_mem_grant_MB_staticrc70 \t\t\t,m12.wg_name AS wg_name_staticrc80 \t\t\t,m12.slots_used * 250 AS tgt_mem_grant_MB_staticrc80 \t\tFROM alloc a1 \t\tJOIN map m1 ON a1.slots_used_smallrc = m1.slots_used \t\t\tAND m1.wg_name = 'SloDWGroupSmall' \t\tJOIN map m2 ON a1.slots_used_mediumrc = m2.slots_used \t\t\tAND m2.wg_name = 'SloDWGroupMedium' \t\tJOIN map m3 ON a1.slots_used_largerc = m3.slots_used \t\t\tAND m3.wg_name = 'SloDWGroupLarge' \t\tJOIN map m4 ON a1.slots_used_xlargerc = m4.slots_used \t\t\tAND m4.wg_name = 'SloDWGroupXLarge' \t\tJOIN map m5 ON a1.slots_used_staticrc10 = m5.slots_used \t\t\tAND m5.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tJOIN map m6 ON a1.slots_used_staticrc20 = m6.slots_used \t\t\tAND m6.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tJOIN map m7 ON a1.slots_used_staticrc30 = m7.slots_used \t\t\tAND m7.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tJOIN map m8 ON a1.slots_used_staticrc40 = m8.slots_used \t\t\tAND m8.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tJOIN map m9 ON a1.slots_used_staticrc50 = m9.slots_used \t\t\tAND m9.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tJOIN map m10 ON a1.slots_used_staticrc60 = m10.slots_used \t\t\tAND m10.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tJOIN map m11 ON a1.slots_used_staticrc70 = m11.slots_used \t\t\tAND m11.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tJOIN map m12 ON a1.slots_used_staticrc80 = m12.slots_used \t\t\tAND m12.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tWHERE a1.DWU = @DWU \t\t)  SELECT DWU \t,max_queries \t,max_slots \t,slots_used \t,wg_name \t,tgt_mem_grant_MB \t,up1 AS rc \t,( \t\tROW_NUMBER() OVER ( \t\t\tPARTITION BY DWU ORDER BY DWU \t\t\t) \t\t) AS rc_id FROM ( \tSELECT DWU \t\t,max_queries \t\t,max_slots \t\t,slots_used \t\t,wg_name \t\t,tgt_mem_grant_MB \t\t,REVERSE(SUBSTRING(REVERSE(wg_names), 1, CHARINDEX('_', REVERSE(wg_names), 1) - 1)) AS up1 \t\t,REVERSE(SUBSTRING(REVERSE(tgt_mem_grant_MBs), 1, CHARINDEX('_', REVERSE(tgt_mem_grant_MBs), 1) - 1)) AS up2 \t\t,REVERSE(SUBSTRING(REVERSE(slots_used_all), 1, CHARINDEX('_', REVERSE(slots_used_all), 1) - 1)) AS up3 \tFROM ref AS r1 \tUNPIVOT(wg_name FOR wg_names IN ( \t\t\t\twg_name_smallrc \t\t\t\t,wg_name_mediumrc \t\t\t\t,wg_name_largerc \t\t\t\t,wg_name_xlargerc \t\t\t\t,wg_name_staticrc10 \t\t\t\t,wg_name_staticrc20 \t\t\t\t,wg_name_staticrc30 \t\t\t\t,wg_name_staticrc40 \t\t\t\t,wg_name_staticrc50 \t\t\t\t,wg_name_staticrc60 \t\t\t\t,wg_name_staticrc70 \t\t\t\t,wg_name_staticrc80 \t\t\t\t)) AS r2 \tUNPIVOT(tgt_mem_grant_MB FOR tgt_mem_grant_MBs IN ( \t\t\t\ttgt_mem_grant_MB_smallrc \t\t\t\t,tgt_mem_grant_MB_mediumrc \t\t\t\t,tgt_mem_grant_MB_largerc \t\t\t\t,tgt_mem_grant_MB_xlargerc \t\t\t\t,tgt_mem_grant_MB_staticrc10 \t\t\t\t,tgt_mem_grant_MB_staticrc20 \t\t\t\t,tgt_mem_grant_MB_staticrc30 \t\t\t\t,tgt_mem_grant_MB_staticrc40 \t\t\t\t,tgt_mem_grant_MB_staticrc50 \t\t\t\t,tgt_mem_grant_MB_staticrc60 \t\t\t\t,tgt_mem_grant_MB_staticrc70 \t\t\t\t,tgt_mem_grant_MB_staticrc80 \t\t\t\t)) AS r3 \tUNPIVOT(slots_used FOR slots_used_all IN ( \t\t\t\tslots_used_smallrc \t\t\t\t,slots_used_mediumrc \t\t\t\t,slots_used_largerc \t\t\t\t,slots_used_xlargerc \t\t\t\t,slots_used_staticrc10 \t\t\t\t,slots_used_staticrc20 \t\t\t\t,slots_used_staticrc30 \t\t\t\t,slots_used_staticrc40 \t\t\t\t,slots_used_staticrc50 \t\t\t\t,slots_used_staticrc60 \t\t\t\t,slots_used_staticrc70 \t\t\t\t,slots_used_staticrc80 \t\t\t\t)) AS r4 \t) a WHERE up1 = up2 \tAND up1 = up3; WITH dmv AS ( \tSELECT rp.name AS rp_name \t\t,rp.max_memory_kb * 1.0 / 1048576 AS rp_max_mem_GB \t\t,(rp.max_memory_kb * 1.0 / 1024) * (request_max_memory_grant_percent / 100) AS max_memory_grant_MB \t\t,(rp.max_memory_kb * 1.0 / 1048576) * (request_max_memory_grant_percent / 100) AS max_memory_grant_GB \t\t,wg.name AS wg_name \t\t,wg.importance AS importance \t\t,wg.request_max_memory_grant_percent AS request_max_memory_grant_percent \tFROM sys.dm_pdw_nodes_resource_governor_workload_groups wg \tJOIN sys.dm_pdw_nodes_resource_governor_resource_pools rp ON wg.pdw_node_id = rp.pdw_node_id \t\tAND wg.pool_id = rp.pool_id \tWHERE rp.name = 'SloDWPool' \tGROUP BY rp.name \t\t,rp.max_memory_kb \t\t,wg.name \t\t,wg.importance \t\t,wg.request_max_memory_grant_percent \t) /* Creating resource class name mapping.*/ \t,NAMES AS ( \tSELECT 'smallrc' AS resource_class \t\t,1 AS rc_id \t \tUNION ALL \t \tSELECT 'mediumrc' \t\t,2 \t \tUNION ALL \t \tSELECT 'largerc' \t\t,3 \t \tUNION ALL \t \tSELECT 'xlargerc' \t\t,4 \t \tUNION ALL \t \tSELECT 'staticrc10' \t\t,5 \t \tUNION ALL \t \tSELECT 'staticrc20' \t\t,6 \t \tUNION ALL \t \tSELECT 'staticrc30' \t\t,7 \t \tUNION ALL \t \tSELECT 'staticrc40' \t\t,8 \t \tUNION ALL \t \tSELECT 'staticrc50' \t\t,9 \t \tUNION ALL \t \tSELECT 'staticrc60' \t\t,10 \t \tUNION ALL \t \tSELECT 'staticrc70' \t\t,11 \t \tUNION ALL \t \tSELECT 'staticrc80' \t\t,12 \t) \t,base AS ( \tSELECT schema_name \t\t,table_name \t\t,SUM(column_count) AS column_count \t\t,CONVERT(BIGINT, ISNULL(SUM(short_string_column_count), 0)) AS short_string_column_count \t\t,CONVERT(BIGINT, ISNULL(SUM(long_string_column_count), 0)) AS long_string_column_count \tFROM ( \t\tSELECT sm.name AS schema_name \t\t\t,tb.name AS table_name \t\t\t,COUNT(co.column_id) AS column_count \t\t\t,CASE  \t\t\t\tWHEN co.system_type_id IN ( \t\t\t\t\t\t36 \t\t\t\t\t\t,43 \t\t\t\t\t\t,106 \t\t\t\t\t\t,108 \t\t\t\t\t\t,165 \t\t\t\t\t\t,167 \t\t\t\t\t\t,173 \t\t\t\t\t\t,175 \t\t\t\t\t\t,231 \t\t\t\t\t\t,239 \t\t\t\t\t\t) \t\t\t\t\tAND co.max_length <= 32 \t\t\t\t\tTHEN COUNT(co.column_id) \t\t\t\tEND AS short_string_column_count \t\t\t,CASE  \t\t\t\tWHEN co.system_type_id IN ( \t\t\t\t\t\t165 \t\t\t\t\t\t,167 \t\t\t\t\t\t,173 \t\t\t\t\t\t,175 \t\t\t\t\t\t,231 \t\t\t\t\t\t,239 \t\t\t\t\t\t) \t\t\t\t\tAND co.max_length > 32 \t\t\t\t\tAND co.max_length <= 8000 \t\t\t\t\tTHEN COUNT(co.column_id) \t\t\t\tEND AS long_string_column_count \t\tFROM sys.schemas AS sm \t\tJOIN sys.tables AS tb ON sm.[schema_id] = tb.[schema_id] \t\tJOIN sys.columns AS co ON tb.[object_id] = co.[object_id] \t\tWHERE tb.name = CASE  \t\t\t\tWHEN @TABLE_NAME IS NULL \t\t\t\t\tTHEN tb.name \t\t\t\tELSE @TABLE_NAME \t\t\t\tEND \t\t\tAND sm.name = CASE  \t\t\t\tWHEN @SCHEMA_NAME IS NULL \t\t\t\t\tTHEN sm.name \t\t\t\tELSE @SCHEMA_NAME \t\t\t\tEND \t\tGROUP BY sm.name \t\t\t,tb.name \t\t\t,co.system_type_id \t\t\t,co.max_length \t\t) a \tGROUP BY schema_name \t\t,table_name \t) \t,size AS ( \tSELECT schema_name \t\t,table_name \t\t,75497472 AS table_overhead \t\t,column_count * 1048576 * 8 AS column_size \t\t,short_string_column_count * 1048576 * 32 AS short_string_size \t\t,long_string_column_count * 16777216 AS long_string_size \tFROM base \t \tUNION \t \tSELECT CASE  \t\t\tWHEN COUNT(*) = 0 \t\t\t\tTHEN 'EMPTY' \t\t\tEND AS schema_name \t\t,CASE  \t\t\tWHEN COUNT(*) = 0 \t\t\t\tTHEN 'EMPTY' \t\t\tEND AS table_name \t\t,CASE  \t\t\tWHEN COUNT(*) = 0 \t\t\t\tTHEN 0 \t\t\tEND AS table_overhead \t\t,CASE  \t\t\tWHEN COUNT(*) = 0 \t\t\t\tTHEN 0 \t\t\tEND AS column_size \t\t,CASE  \t\t\tWHEN COUNT(*) = 0 \t\t\t\tTHEN 0 \t\t\tEND AS short_string_size \t\t,CASE  \t\t\tWHEN COUNT(*) = 0 \t\t\t\tTHEN 0 \t\t\tEND AS long_string_size \tFROM base \t) \t,load_multiplier AS ( \tSELECT CASE  \t\t\tWHEN FLOOR(8 * (CAST(CAST(REPLACE(REPLACE(@DWU, 'DW', ''), 'c', '') AS INT) AS FLOAT) / 6000)) > 0 /*AND CHARINDEX(@DWU, 'c') = 0*/ \t\t\t\tAND CHARINDEX('c', @DWU) = 0 \t\t\t\tTHEN FLOOR(8 * (CAST(CAST(REPLACE(REPLACE(@DWU, 'DW', ''), 'c', '') AS INT) AS FLOAT) / 6000)) \t\t\tELSE 1 \t\t\tEND AS multiplication_factor \t) SELECT @Optmial_Resource_Class = MAX(closest_rc_in_increasing_order) FROM ( \tSELECT schema_name AS schema_name \t\t,table_name AS table_name \t\t,MIN(rc.resource_class) AS closest_rc_in_increasing_order \tFROM size AS s \t\t,load_multiplier \t\t,#ref r1 \t\t,NAMES rc \tWHERE r1.rc_id = rc.rc_id \t\tAND rc.rc_id >= 5 /*only use static rc*/ \t\tAND r1.tgt_mem_grant_MB - CAST((table_overhead * 1.0 + column_size + short_string_size + long_string_size) * multiplication_factor / 1048576 AS DECIMAL(18, 2)) > 0 /*only get rc where we have enough memory*/ \t\tAND schema_name IS NOT NULL \t\tAND table_name IS NOT NULL \tGROUP BY schema_name \t\t,table_name \t) AS a;  DECLARE @TotalRowCount BIGINT;  SELECT @TotalRowCount = SUM(CONVERT(BIGINT, nps.[row_count])) FROM sys.schemas s JOIN sys.tables t ON s.[schema_id] = t.[schema_id] JOIN sys.indexes i ON t.[object_id] = i.[object_id] \tAND i.[index_id] <= 1 JOIN sys.pdw_table_distribution_properties tp ON t.[object_id] = tp.[object_id] JOIN sys.pdw_table_mappings tm ON t.[object_id] = tm.[object_id] JOIN sys.pdw_nodes_tables nt ON tm.[physical_name] = nt.[name] JOIN sys.dm_pdw_nodes pn ON nt.[pdw_node_id] = pn.[pdw_node_id] JOIN sys.pdw_distributions di ON nt.[distribution_id] = di.[distribution_id] JOIN ( \tSELECT [object_id] \t\t,[pdw_node_id] \t\t,[distribution_id] \t\t,SUM(row_count) AS row_count \tFROM ( \t\tSELECT [object_id] \t\t\t,[pdw_node_id] \t\t\t,[distribution_id] \t\t\t,MAX(row_count) AS row_count \t\tFROM sys.dm_pdw_nodes_db_partition_stats \t\tGROUP BY [object_id] \t\t\t,[pdw_node_id] \t\t\t,[distribution_id] \t\t) AS a \tGROUP BY [object_id] \t\t,[pdw_node_id] \t\t,[distribution_id] \t) AS nps ON nt.[object_id] = nps.[object_id] \tAND nt.[pdw_node_id] = nps.[pdw_node_id] \tAND nt.[distribution_id] = nps.[distribution_id] WHERE pn.[type] = 'COMPUTE' \tAND s.name = @SCHEMA_NAME \tAND t.name = @TABLE_NAME GROUP BY s.name \t,t.name;  SELECT CONCAT ( \t\t'User' \t\t,COALESCE(CASE  \t\t\t\tWHEN REPLACE(@Optmial_Resource_Class, 'staticrc', '') < 80 \t\t\t\t\tAND @TotalRowCount >= 60000000 \t\t\t\t\tTHEN CONCAT ( \t\t\t\t\t\t\t'staticrc' \t\t\t\t\t\t\t,REPLACE(@Optmial_Resource_Class, 'staticrc', '') + 10 \t\t\t\t\t\t\t) \t\t\t\tWHEN @TotalRowCount < 60000000 \t\t\t\t\tTHEN 'staticrc10' \t\t\t\tELSE @Optmial_Resource_Class \t\t\t\tEND, 'staticrc80') \t\t) AS UserName"
			},
			"SqlCommandCreateStagingTable": {
				"type": "String",
				"defaultValue": "IF OBJECT_ID('tempdb..#tables') IS NOT NULL \tDROP TABLE #tables;  CREATE TABLE #tables ( \tSchemaName NVARCHAR(100) \t,TableName NVARCHAR(100) \t,FolderPath NVARCHAR(1000) \t);  INSERT INTO #tables VALUES ( \t'{SchemaNameStaging}' \t,'{TableNameStaging}' \t,'{FolderPathFull}' \t)  IF OBJECT_ID('tempdb..#CreateViewsDDL') IS NOT NULL \tDROP TABLE #CreateViewsDDL;  CREATE TABLE #CreateViewsDDL ( \tSchemaName NVARCHAR(100) \t,ViewName NVARCHAR(100) \t,ViewDDL NVARCHAR(MAX) \t);  DECLARE @cnt INT = 1 DECLARE @sqlCreateView NVARCHAR(MAX) DECLARE @SchemaName NVARCHAR(100) DECLARE @TableName NVARCHAR(100) DECLARE @FolderPath NVARCHAR(1000)  SELECT @SchemaName = SchemaName \t,@TableName = TableName \t,@FolderPath = FolderPath \t,@sqlCreateView = CONCAT ( \t\t'sp_describe_first_result_set @tsql=N''SELECT * FROM OPENROWSET(BULK ''''' \t\t,FolderPath \t\t,''''' , FORMAT=''''PARQUET'''') AS r''' \t\t) FROM #tables;  IF OBJECT_ID('tempdb..#InformationSchemaTempTable', 'U') IS NOT NULL \tDROP TABLE #InformationSchemaTempTable;  CREATE TABLE #InformationSchemaTempTable ( \tis_hidden BIT NOT NULL \t,column_ordinal INT NOT NULL \t,name SYSNAME NULL \t,is_nullable BIT NOT NULL \t,system_type_id INT NOT NULL \t,system_type_name NVARCHAR(256) NULL \t,max_length SMALLINT NOT NULL \t,precision TINYINT NOT NULL \t,scale TINYINT NOT NULL \t,collation_name SYSNAME NULL \t,user_type_id INT NULL \t,user_type_database SYSNAME NULL \t,user_type_schema SYSNAME NULL \t,user_type_name SYSNAME NULL \t,assembly_qualified_type_name NVARCHAR(4000) \t,xml_collection_id INT NULL \t,xml_collection_database SYSNAME NULL \t,xml_collection_schema SYSNAME NULL \t,xml_collection_name SYSNAME NULL \t,is_xml_document BIT NOT NULL \t,is_case_sensitive BIT NOT NULL \t,is_fixed_length_clr_type BIT NOT NULL \t,source_server SYSNAME NULL \t,source_database SYSNAME NULL \t,source_schema SYSNAME NULL \t,source_table SYSNAME NULL \t,source_column SYSNAME NULL \t,is_identity_column BIT NULL \t,is_part_of_unique_key BIT NULL \t,is_updateable BIT NULL \t,is_computed_column BIT NULL \t,is_sparse_column_set BIT NULL \t,ordinal_in_order_by_list SMALLINT NULL \t,order_by_list_length SMALLINT NULL \t,order_by_is_descending SMALLINT NULL \t,tds_type_id INT NOT NULL \t,tds_length INT NOT NULL \t,tds_collation_id INT NULL \t,tds_collation_sort_id TINYINT NULL \t);  INSERT INTO #InformationSchemaTempTable EXEC (@sqlCreateView) /*SELECT * FROM #InformationSchemaTempTable*/  DECLARE @GetMaxValueStatement NVARCHAR(MAX) DECLARE @GetColumnList NVARCHAR(MAX)  SELECT @GetMaxValueStatement = CONVERT(NVARCHAR(MAX), CONCAT ( \t\t\t'SELECT ' \t\t\t,STRING_AGG(ColumnMaxLength, ',') \t\t\t,' FROM OPENROWSET(BULK ''' \t\t\t,@FolderPath \t\t\t,''' , FORMAT=''PARQUET'') WITH (' \t\t\t,STRING_AGG(CONVERT(NVARCHAR(MAX), ColumnDatatypeWithMax), ',') \t\t\t,') AS r' \t\t\t)) \t,@GetColumnList = STRING_AGG(QUOTENAME([name]), ',') FROM ( \tSELECT CASE  \t\t\tWHEN system_type_name LIKE ('%char%') \t\t\t\tOR system_type_name = 'varbinary(8000)' \t\t\t\tTHEN CONCAT ( \t\t\t\t\t\t'CONVERT(BIGINT, COALESCE(NULLIF(MAX(DATALENGTH(' \t\t\t\t\t\t,QUOTENAME([name]) \t\t\t\t\t\t,')), 0), 1)) AS ' \t\t\t\t\t\t,QUOTENAME([name]) \t\t\t\t\t\t) \t\t\tELSE CONCAT ( \t\t\t\t\t'COALESCE(CONVERT(BIGINT, SUM(0)), 0) AS ' \t\t\t\t\t,QUOTENAME([name]) \t\t\t\t\t) \t\t\tEND AS ColumnMaxLength \t\t,CASE  \t\t\tWHEN system_type_name LIKE ('%char%') \t\t\t\tTHEN CONCAT ( \t\t\t\t\t\tQUOTENAME([name]) \t\t\t\t\t\t,' ' \t\t\t\t\t\t,REPLACE(system_type_name, '8000', 'MAX') \t\t\t\t\t\t,' COLLATE Latin1_General_100_BIN2_UTF8' \t\t\t\t\t\t) \t\t\tWHEN system_type_name = 'varbinary(8000)' \t\t\t\tTHEN CONCAT ( \t\t\t\t\t\tQUOTENAME([name]) \t\t\t\t\t\t,' ' \t\t\t\t\t\t,REPLACE(system_type_name, '8000', 'MAX') \t\t\t\t\t\t) \t\t\tELSE CONCAT ( \t\t\t\t\tQUOTENAME([name]) \t\t\t\t\t,' ' \t\t\t\t\t,system_type_name \t\t\t\t\t) \t\t\tEND AS ColumnDatatypeWithMax \t\t,[name] \tFROM #InformationSchemaTempTable \t) AS a /*SELECT @GetMaxValueStatement*/ /*SELECT @GetColumnList*/  DECLARE @sqlUnpivot NVARCHAR(MAX)  SET @sqlUnpivot = CONCAT ( \t\t'SELECT ''' \t\t,@TableName \t\t,''' AS TABLE_NAME, unpvt.col AS COLUMN_NAME, CASE WHEN unpvt.datatype > 8000 THEN ''MAX'' ELSE CONVERT(NVARCHAR(100), unpvt.datatype) END AS DATATYPE_MAX FROM  ( ' \t\t,@GetMaxValueStatement \t\t,' ) AS a ' \t\t,CHAR(13) \t\t,' UNPIVOT ( datatype FOR col IN  ( ' \t\t,@GetColumnList \t\t,') ) AS unpvt' \t\t)  DROP TABLE  IF EXISTS #tmpBus; \tCREATE TABLE #tmpBus ( \t\tTABLE_CLEAN NVARCHAR(1000) \t\t,COLUMN_NAME NVARCHAR(1000) \t\t,DATATYPE_MAX NVARCHAR(1000) \t\t);  INSERT INTO #tmpBus EXEC (@sqlUnpivot)  DECLARE @createFinalView NVARCHAR(MAX) DECLARE @openrowsetValue NVARCHAR(MAX)  SELECT @createFinalView = CONCAT ( \t\t'CREATE TABLE ' \t\t,QUOTENAME(@SchemaName) \t\t,'.' \t\t,QUOTENAME(@TableName) \t\t,' (' \t\t,STRING_AGG(ColumnFullDefinition, ',') \t\t,') WITH ( DISTRIBUTION = ROUND_ROBIN, HEAP)' \t\t) \t,@openrowsetValue = CONCAT ( \t\t'FROM OPENROWSET(BULK ''''' \t\t,@FolderPath \t\t,''''', FORMAT=''''PARQUET'''') WITH (' \t\t,STRING_AGG(CONVERT(NVARCHAR(MAX), ColumnFullDefinition), ',') \t\t) FROM ( \tSELECT @TableName AS table_name \t\t,c.[name] \t\t,UPPER(TYPE_NAME(c.system_type_id)) AS DataType \t\t,CONCAT ( \t\t\tQUOTENAME(c.[name]) \t\t\t,' ' \t\t\t,CASE  \t\t\t\tWHEN TYPE_NAME(c.system_type_id) IN ( \t\t\t\t\t\t'int' \t\t\t\t\t\t,'bigint' \t\t\t\t\t\t,'smallint' \t\t\t\t\t\t,'tinyint' \t\t\t\t\t\t,'bit' \t\t\t\t\t\t,'decimal' \t\t\t\t\t\t,'numeric' \t\t\t\t\t\t,'float' \t\t\t\t\t\t,'real' \t\t\t\t\t\t,'datetime2' \t\t\t\t\t\t,'date' \t\t\t\t\t\t) \t\t\t\t\tTHEN UPPER(c.system_type_name) \t\t\t\tELSE CONCAT ( \t\t\t\t\t\tUPPER(TYPE_NAME(c.system_type_id)) \t\t\t\t\t\t,'(' \t\t\t\t\t\t,a.DATATYPE_MAX \t\t\t\t\t\t,') ' \t\t\t\t\t\t) \t\t\t\tEND \t\t\t) AS ColumnFullDefinition \tFROM #InformationSchemaTempTable AS c \tJOIN #tmpBus AS a ON a.COLUMN_NAME = c.[name] \tORDER BY column_ordinal OFFSET 0 ROWS \t) AS a /*SELECT @createFinalView*/ /*INSERT INTO #CreateViewsDDL*/  SELECT @SchemaName AS SchemaName \t,@TableName AS TableName \t,CONCAT ( \t\t'IF OBJECT_ID(''' \t\t,QUOTENAME(@SchemaName) \t\t,'.' \t\t,QUOTENAME(@TableName) \t\t,''', ''U'') IS NOT NULL DROP TABLE ' \t\t,QUOTENAME(@SchemaName) \t\t,'.' \t\t,QUOTENAME(@TableName) \t\t,'; ' \t\t,@createFinalView \t\t,';' \t\t) AS CreateTableDDL;"
			},
			"CTAS": {
				"type": "String",
				"defaultValue": "EXECUTE AS USER = '{ResourceClassName}'  DECLARE @SchemaNameStaging NVARCHAR(MAX) = '{SchemaNameStaging}' \t,@TableNameStaging NVARCHAR(MAX) = '{TableNameStaging}' \t,@TableNameFinal NVARCHAR(MAX) = '{TableNameFinal}' \t,@RowCount BIGINT \t,@TableDataSpaceGB DECIMAL(36, 2);  WITH base AS ( \tSELECT s.name AS [schema_name] \t\t,t.name AS [table_name] \t\t,QUOTENAME(s.name) + '.' + QUOTENAME(t.name) AS [two_part_name] \t\t,tp.[distribution_policy_desc] AS [distribution_policy_name] \t\t,c.[name] AS [distribution_column] \t\t,i.[type_desc] AS [index_type_desc] \t\t,[reserved_page_count] AS reserved_space_page_count \t\t,[used_data] AS data_space_page_count \t\t,([reserved_page_count] - ([used_data] + ([reserved_page_count] - [used_page_count]))) AS index_space_page_count \t\t,([reserved_page_count] - [used_page_count]) AS unused_space_page_count \t\t,nps.[row_count] AS [row_count] \tFROM sys.schemas s \tJOIN sys.tables t ON s.[schema_id] = t.[schema_id] \tJOIN sys.indexes i ON t.[object_id] = i.[object_id] \t\tAND i.[index_id] <= 1 \tJOIN sys.pdw_table_distribution_properties tp ON t.[object_id] = tp.[object_id] \tJOIN sys.pdw_table_mappings tm ON t.[object_id] = tm.[object_id] \tJOIN sys.pdw_nodes_tables nt ON tm.[physical_name] = nt.[name] \tJOIN sys.dm_pdw_nodes pn ON nt.[pdw_node_id] = pn.[pdw_node_id] \tJOIN sys.pdw_distributions di ON nt.[distribution_id] = di.[distribution_id] \tJOIN ( \t\tSELECT [object_id] \t\t\t,[pdw_node_id] \t\t\t,[distribution_id] \t\t\t,SUM(row_count) AS row_count \t\t\t,SUM([reserved_page_count]) AS [reserved_page_count] \t\t\t,SUM([used_data]) AS [used_data] \t\t\t,SUM([used_page_count]) AS [used_page_count] \t\tFROM ( \t\t\tSELECT [object_id] \t\t\t\t,[pdw_node_id] \t\t\t\t,[distribution_id] \t\t\t\t,MAX(row_count) AS row_count \t\t\t\t,SUM([reserved_page_count]) AS [reserved_page_count] \t\t\t\t,MAX([used_page_count]) AS [used_data] \t\t\t\t,SUM([used_page_count]) AS [used_page_count] \t\t\tFROM sys.dm_pdw_nodes_db_partition_stats \t\t\tGROUP BY [object_id] \t\t\t\t,[pdw_node_id] \t\t\t\t,[distribution_id] \t\t\t) AS a \t\tGROUP BY [object_id] \t\t\t,[pdw_node_id] \t\t\t,[distribution_id] \t\t) AS nps ON nt.[object_id] = nps.[object_id] \t\tAND nt.[pdw_node_id] = nps.[pdw_node_id] \t\tAND nt.[distribution_id] = nps.[distribution_id] \tLEFT JOIN ( \t\tSELECT * \t\tFROM sys.pdw_column_distribution_properties \t\tWHERE distribution_ordinal = 1 \t\t) cdp ON t.[object_id] = cdp.[object_id] \tLEFT JOIN sys.columns c ON cdp.[object_id] = c.[object_id] \t\tAND cdp.[column_id] = c.[column_id] \tWHERE pn.[type] = 'COMPUTE' \t\tAND s.name = @SchemaNameStaging \t\tAND t.name = @TableNameStaging \t) \t,size AS ( \tSELECT [schema_name] \t\t,[table_name] \t\t,[distribution_policy_name] \t\t,[distribution_column] \t\t,[index_type_desc] \t\t,[index_space_page_count] \t\t,([reserved_space_page_count] * 8.0) / 1000000 AS [reserved_space_GB] \t\t,([unused_space_page_count] * 8.0) / 1000000 AS [unused_space_GB] \t\t,([data_space_page_count] * 8.0) / 1000000 AS [data_space_GB] \t\t,([index_space_page_count] * 8.0) / 1000000 AS [index_space_GB] \t\t,[row_count] AS row_count \tFROM base \t) SELECT @RowCount = SUM(row_count) /*AS [RowCount]*/ \t,@TableDataSpaceGB = SUM(data_space_GB) /*AS TableDataSpaceGB*/ FROM size GROUP BY schema_name \t,table_name \t,distribution_policy_name \t,distribution_column \t,index_type_desc ORDER BY schema_name \t,table_name \t,SUM(reserved_space_GB) DESC;/*SELECT @RowCount, @TableDataSpaceGB*/  DECLARE @sql NVARCHAR(MAX) \t,@Distribution NVARCHAR(MAX) \t,@IndexType NVARCHAR(MAX)  SELECT @sql = sqlString FROM ( \tSELECT CONCAT ( \t\t\t'IF OBJECT_ID(''tempdb..#temp'') IS NOT NULL DROP TABLE #temp; CREATE TABLE #temp WITH (DISTRIBUTION = ROUND_ROBIN, HEAP) AS  \t\t\t\t\tSELECT \t\t\t\t\t\t\t* \t\t\t\t\t\t\t,CASE WHEN DataTypeName = ''int'' THEN 1 \t\t\t\t\t\t\t\tWHEN DataTypeName IN (''numeric'', ''decimal'') AND ScaleValue = 0 THEN 1 \t\t\t\t\t\t\t\tWHEN DataTypeName = ''bigint'' THEN 2 \t\t\t\t\t\t\t\tWHEN DataTypeName = ''smallint'' THEN 2 \t\t\t\t\t\t\t\tWHEN DataTypeName IN (''char'', ''varchar'', ''nvarchar'')  \t\t\t\t\t\t\t\t\tTHEN \t\t\t\t\t\t\t\t\t\tCASE WHEN CharacterLength <= 5 THEN 3 \t\t\t\t\t\t\t\t\t\t\t\tWHEN CharacterLength <= 10 THEN 4 \t\t\t\t\t\t\t\t\t\t\t\tWHEN CharacterLength <= 20 THEN 6 \t\t\t\t\t\t\t\t\t\t\t\tWHEN CharacterLength <= 50 THEN 8 \t\t\t\t\t\t\t\t\t\t\t\tELSE 10 \t\t\t\t\t\t\t\t\t\t\tEND \t\t\t\t\t\t\t\tELSE 10 \t\t\t\t\t\t\t\tEND \t\t\t\t\t\t\t+  \t\t\t\t\t\t\tCASE WHEN NullCountPercentage < 0.01 THEN 1 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.02 THEN 2 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.03 THEN 3 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.04 THEN 4 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.05 THEN 5 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.06 THEN 6 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.07 THEN 7 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.08 THEN 8 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.09 THEN 9 \t\t\t\t\t\t\t\tELSE 10 \t\t\t\t\t\t\t\tEND \t\t\t\t\t\t\t+  \t\t\t\t\t\t\tCASE WHEN UniqueValueCount > 60 AND UniqueValueCountPercentage > 0.000001 THEN 1 ELSE 3\t\t\t\t\t\t\tEND + RANK() OVER (ORDER BY NullCountPercentage ASC, CASE WHEN TableRowCount > 60000000 AND UniqueValueCount >= 600 THEN -1*UniqueValueCountPercentage ELSE UniqueValueCountPercentage END DESC, UniqueValueCount DESC, ABS(LEN(''' \t\t\t,@TableNameStaging \t\t\t,''') - LEN(ColName)))/10.0 \t\t\t\t\t\t\tAS WeightedScore \t\t\t\t\t\tFROM \t\t\t\t\t\t( \t\t\t\t\t\t\tSELECT\t* \t\t\t\t\t\t\t\t\t,NullCount/(NULLIF(TableRowCount, 0)*1.0) AS NullCountPercentage \t\t\t\t\t\t\t\t\t,UniqueValueCount/(NULLIF(TableRowCount, 0)*1.0) AS UniqueValueCountPercentage  \t\t\t\t\t\t\tFROM (' \t\t\t,STRING_AGG(CONVERT(NVARCHAR(MAX), CONCAT ( \t\t\t\t\t\t'SELECT ''' \t\t\t\t\t\t,COLUMN_NAME \t\t\t\t\t\t,''' AS ColName, ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' AS DataTypeName \t\t\t\t\t\t\t\t,CASE  \t\t\t\t\t\t\t\t\t  WHEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' IN (''varchar'', ''char'') THEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' + ''('' + CASE WHEN ' \t\t\t\t\t\t,COALESCE(CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' = -1 THEN ''max'' ELSE CAST(' \t\t\t\t\t\t,COALESCE(CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' AS VARCHAR(25)) END + '')''  \t\t\t\t\t\t\t\t\t  WHEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' IN (''nvarchar'',''nchar'') THEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' + ''('' + CASE WHEN ' \t\t\t\t\t\t,COALESCE(CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' = -1 THEN ''max'' ELSE CAST(' \t\t\t\t\t\t,COALESCE(CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' / 2 AS VARCHAR(25)) END + '')''       \t\t\t\t\t\t\t\t\t  WHEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' IN (''decimal'', ''numeric'') THEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' + ''('' + CAST(' \t\t\t\t\t\t,COALESCE(CAST(NUMERIC_PRECISION AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' AS VARCHAR(25)) + '', '' + CAST(' \t\t\t\t\t\t,COALESCE(CAST(NUMERIC_SCALE AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' AS VARCHAR(25)) + '')'' \t\t\t\t\t\t\t\t\t  WHEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' IN (''datetime2'') THEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' + ''('' + CAST(' \t\t\t\t\t\t,COALESCE(CAST(NUMERIC_SCALE AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' AS VARCHAR(25)) + '')'' \t\t\t\t\t\t\t\t\t  ELSE ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' \t\t\t\t\t\t\t\t\tEND AS DataTypeFull \t\t\t\t\t\t\t\t,' \t\t\t\t\t\t,COALESCE(CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' AS CharacterLength \t\t\t\t\t\t\t\t,' \t\t\t\t\t\t,COALESCE(CAST(NUMERIC_PRECISION AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' AS PrecisionValue, ' \t\t\t\t\t\t,COALESCE(CAST(NUMERIC_SCALE AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' AS ScaleValue \t\t\t\t\t\t\t\t,COUNT_BIG(DISTINCT ' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,') AS UniqueValueCount, COALESCE(SUM(CONVERT(BIGINT, CASE WHEN ' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,' IS NULL THEN 1 ELSE 0 END)), CONVERT(BIGINT, 0)) AS NullCount  \t\t\t\t\t\t \t\t\t\t\t\t,COALESCE(CONVERT(NVARCHAR(MAX), MIN(' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,')), '''') AS [MinValue] \t\t\t\t\t\t,COALESCE(CONVERT(NVARCHAR(MAX), MAX(' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,')), '''') AS [MaxValue] \t\t\t\t\t\t,COALESCE(MIN(DATALENGTH(' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,')), 0) AS [MinLength] \t\t\t\t\t\t,COALESCE(MAX(DATALENGTH(' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,')), 0) AS [MaxLength] \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t,AVG(CASE WHEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' IN (''char'', ''varchar'', ''nvarchar'') THEN CONVERT(NUMERIC(30,2), NULL) ELSE ' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,' END) AS DataAverage \t\t\t\t\t\t \t\t\t\t\t\t,STDEVP(CASE WHEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' IN (''char'', ''varchar'', ''nvarchar'') THEN CONVERT(FLOAT, NULL) ELSE ' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,' END) AS DataStdevp\t\t\t\t\t\t,' \t\t\t\t\t\t,@RowCount \t\t\t\t\t\t,' AS TableRowCount,' \t\t\t\t\t\t,@TableDataSpaceGB \t\t\t\t\t\t,' AS TableDataSpaceGB  \t\t\t\t\t\t\t\tFROM ' \t\t\t\t\t\t,QUOTENAME(TABLE_SCHEMA) \t\t\t\t\t\t,'.' \t\t\t\t\t\t,QUOTENAME(TABLE_NAME) \t\t\t\t\t\t,'' \t\t\t\t\t\t)), ' UNION ') \t\t\t,') AS a) AS a' \t\t\t,' OPTION (LABEL = ''' \t\t\t,CONCAT ( \t\t\t\t'Data Profile - ' \t\t\t\t,QUOTENAME(@SchemaNameStaging) \t\t\t\t,'.' \t\t\t\t,QUOTENAME(@TableNameStaging) \t\t\t\t,' - {PipelineRunId}' \t\t\t\t) \t\t\t,''')' \t\t\t) AS sqlString \tFROM INFORMATION_SCHEMA.COLUMNS \tWHERE TABLE_SCHEMA = @SchemaNameStaging \t\tAND TABLE_NAME = @TableNameStaging \t\tAND ( \t\t\tDATA_TYPE IN ( \t\t\t\t'int' \t\t\t\t,'bigint' \t\t\t\t,'char' \t\t\t\t,'varchar' \t\t\t\t,'nvarchar' \t\t\t\t) \t\t\tOR NUMERIC_SCALE = 0 \t\t\t) /*AND COLUMN_NAME NOT LIKE '%date%' \t\tAND COLUMN_NAME NOT LIKE '%time%'*/ \t) AS a;/*SELECT @sql*/  EXEC (@sql);  INSERT INTO logging.DataProfile SELECT '{PipelineRunId}' AS PipelineRunId \t,'{PipelineStartDate}' AS PipelineStartDate \t,'{PipelineStartDateTime}' AS PipelineStartDateTime \t,@SchemaNameStaging AS SchemaName \t,@TableNameStaging AS TableName \t,ColName \t,DataTypeName \t,DataTypeFull \t,CharacterLength \t,PrecisionValue \t,ScaleValue \t,UniqueValueCount \t,NullCount \t,MinValue \t,MaxValue \t,MinLength \t,MaxLength \t,DataAverage \t,DataStdevp \t,TableRowCount \t,TableDataSpaceGB \t,WeightedScore \t,CONCAT ( \t\t@sql \t\t,'; SELECT * FROM #temp ORDER BY WeightedScore' \t\t) AS SqlCommandDataProfile \t,CONCAT ( \t\t'CREATE TABLE [' \t\t,@SchemaNameStaging \t\t,'].[' \t\t,@TableNameFinal \t\t,'_' \t\t,ColName \t\t,'] WITH (DISTRIBUTION = HASH(' \t\t,QUOTENAME(ColName) \t\t,'), CLUSTERED COLUMNSTORE INDEX) AS SELECT * FROM [' \t\t,@SchemaNameStaging \t\t,'].[' \t\t,@TableNameFinal \t\t,'];' \t\t,' IF OBJECT_ID(''' \t\t,@SchemaNameStaging \t\t,'.' \t\t,@TableNameFinal \t\t,''', ''U'') IS NOT NULL DROP TABLE [' \t\t,@SchemaNameStaging \t\t,'].[' \t\t,@TableNameFinal \t\t,'];' \t\t,' RENAME OBJECT [' \t\t,@SchemaNameStaging \t\t,'].[' \t\t,@TableNameFinal \t\t,'_' \t\t,ColName \t\t,'] TO [' \t\t,@SchemaNameStaging \t\t,'].[' \t\t,@TableNameFinal \t\t,'];' \t\t) AS SqlCommandCTAS \t,GETDATE() AS RowInsertDateTime FROM #temp;  IF @RowCount >= 60000000 BEGIN /*Run this if row count is greater than 60M and data size on disk is less that 2GB*/ \tSELECT @sql = 'SELECT TOP 1 @Distribution = CONCAT('' WITH ( DISTRIBUTION = HASH('', ColName, ''), CLUSTERED COLUMNSTORE INDEX ) '') FROM #temp WHERE ColName NOT LIKE ''%date%'' AND ColName NOT LIKE ''%time%'' ORDER BY WeightedScore' \tFROM #temp \tWHERE ( \t\t\tColName NOT LIKE '%time%' \t\t\tAND ColName NOT LIKE '%date%' \t\t\t) /*Will need to be tweaked*/  \tEXEC sp_executesql @Query = @sql \t\t,@Params = N'@Distribution NVARCHAR(MAX) OUTPUT' \t\t,@Distribution = @Distribution OUTPUT END ELSE IF @RowCount < 60000000 \tAND @TableDataSpaceGB > 2 BEGIN \tSELECT @sql = 'SELECT TOP 1 @Distribution = CONCAT('' WITH ( DISTRIBUTION = HASH('', ColName, ''), CLUSTERED INDEX('', ColName, '') ) '') FROM #temp ORDER BY WeightedScore' \tFROM #temp  \tEXEC sp_executesql @Query = @sql \t\t,@Params = N'@Distribution NVARCHAR(MAX) OUTPUT' \t\t,@Distribution = @Distribution OUTPUT END ELSE BEGIN \tSELECT @sql = 'SELECT TOP 1 @Distribution = CONCAT('' WITH ( DISTRIBUTION = REPLICATE, CLUSTERED INDEX('', ColName, '') ) '') FROM #temp ORDER BY WeightedScore' \tFROM #temp  \tEXEC sp_executesql @Query = @sql \t\t,@Params = N'@Distribution NVARCHAR(MAX) OUTPUT' \t\t,@Distribution = @Distribution OUTPUT END  SELECT @Distribution AS DistributionIndex;"
			},
			"sqlDelete": {
				"type": "String",
				"defaultValue": "DECLARE @SchemaName NVARCHAR(1000) = '{SchemaName}' \t,@TableName NVARCHAR(1000) = '{TableName}' \t,@UserName NVARCHAR(1000) = '{UserName}' \t,@KeyColumns NVARCHAR(1000) = '{KeyColumns}' DECLARE @sql NVARCHAR(MAX)  SELECT @sql = CONCAT ( \t\t'DELETE trg FROM ' \t\t,QUOTENAME(@SchemaName) \t\t,'.' \t\t,QUOTENAME(@TableName) \t\t,' AS trg JOIN ' \t\t,QUOTENAME(@SchemaName) \t\t,'.[' \t\t,@TableName \t\t,'_delete] AS src ON ' \t\t,( \t\t\tSELECT STRING_AGG(CONCAT ( \t\t\t\t\t\t'trg.' \t\t\t\t\t\t,TRIM(value) \t\t\t\t\t\t,' = ' \t\t\t\t\t\t,'src.' \t\t\t\t\t\t,TRIM(value) \t\t\t\t\t\t), ' AND ') \t\t\tFROM STRING_SPLIT(@KeyColumns, ',') \t\t\t) \t\t);  PRINT (@sql);  EXEC (@sql);"
			},
			"sqlUpdate": {
				"type": "String",
				"defaultValue": "DECLARE @SchemaName NVARCHAR(1000) = '{SchemaName}' \t,@TableName NVARCHAR(1000) = '{TableName}' \t,@UserName NVARCHAR(1000) = '{UserName}' \t,@KeyColumns NVARCHAR(1000) = '{KeyColumns}' DECLARE @sql NVARCHAR(MAX)  SET @sql = ( \t\tSELECT CONCAT ( \t\t\t\t'EXECUTE AS user = ''' \t\t\t\t,@UserName \t\t\t\t,''' UPDATE trg SET ' \t\t\t\t,STRING_AGG(CASE  \t\t\t\t\t\tWHEN IsKeyColumnFlag = 0 \t\t\t\t\t\t\tTHEN CONCAT ( \t\t\t\t\t\t\t\t\tQUOTENAME(COLUMN_NAME) \t\t\t\t\t\t\t\t\t,' = src.' \t\t\t\t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t\t\t\t) \t\t\t\t\t\tELSE NULL \t\t\t\t\t\tEND, ',') \t\t\t\t,' FROM ' \t\t\t\t,QUOTENAME(TABLE_SCHEMA) \t\t\t\t,'.' \t\t\t\t,QUOTENAME(TABLE_NAME) \t\t\t\t,' AS trg' \t\t\t\t,' JOIN ' \t\t\t\t,QUOTENAME(TABLE_SCHEMA) \t\t\t\t,'.[' \t\t\t\t,TABLE_NAME \t\t\t\t,'_update] AS src' \t\t\t\t,' ON ' \t\t\t\t,( \t\t\t\t\tSELECT STRING_AGG(CONCAT ( \t\t\t\t\t\t\t\t'trg.' \t\t\t\t\t\t\t\t,value \t\t\t\t\t\t\t\t,' = ' \t\t\t\t\t\t\t\t,'src.' \t\t\t\t\t\t\t\t,value \t\t\t\t\t\t\t\t), ' AND ') \t\t\t\t\tFROM STRING_SPLIT(@KeyColumns, ',') \t\t\t\t\t) \t\t\t\t) \t\tFROM ( \t\t\tSELECT TABLE_SCHEMA \t\t\t\t,TABLE_NAME \t\t\t\t,COLUMN_NAME \t\t\t\t,CASE  \t\t\t\t\tWHEN COLUMN_NAME IN ( \t\t\t\t\t\t\tSELECT TRIM(value) \t\t\t\t\t\t\tFROM STRING_SPLIT(@KeyColumns, ',') \t\t\t\t\t\t\t) \t\t\t\t\t\tTHEN 1 \t\t\t\t\tELSE 0 \t\t\t\t\tEND IsKeyColumnFlag \t\t\tFROM INFORMATION_SCHEMA.COLUMNS \t\t\tWHERE TABLE_SCHEMA = @SchemaName \t\t\t\tAND TABLE_NAME = @TableName \t\t\t) AS a \t\tGROUP BY TABLE_SCHEMA \t\t\t,TABLE_NAME \t\t)  PRINT (@sql);  EXEC (@sql);"
			},
			"sqlInsert": {
				"type": "String",
				"defaultValue": "DECLARE @SchemaName NVARCHAR(1000) = '{SchemaName}' \t,@TableName NVARCHAR(1000) = '{TableName}' \t,@UserName NVARCHAR(1000) = '{UserName}' DECLARE @sql NVARCHAR(MAX)  SET @sql = ( SELECT CONCAT ( \t\t'EXECUTE AS user = ''' \t\t,@UserName \t\t,''' INSERT INTO ' \t\t,QUOTENAME(TABLE_SCHEMA) \t\t,'.' \t\t,QUOTENAME(TABLE_NAME) \t\t,' (' \t\t,STRING_AGG(QUOTENAME(COLUMN_NAME), ',') \t\t,')' \t\t,' SELECT ' \t\t,STRING_AGG(QUOTENAME(COLUMN_NAME), ',') \t\t,' FROM ' \t\t,QUOTENAME(TABLE_SCHEMA) \t\t,'.[' \t\t,TABLE_NAME \t\t,'_insert]' \t\t) FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = @SchemaName \tAND TABLE_NAME = @TableName GROUP BY TABLE_SCHEMA \t,TABLE_NAME )  PRINT (@sql);  EXEC (@sql);"
			},
			"sqlAlterColumn": {
				"type": "String",
				"defaultValue": "DECLARE @SchemaName NVARCHAR(1000) = '{SchemaName}' \t,@TableNameBase NVARCHAR(1000) = '{TableNameBase}' \t,@TableNameStage NVARCHAR(1000) = '{TableNameStage}' DECLARE @sqlUpdateColumn NVARCHAR(MAX) \t,@sqlAddColumn NVARCHAR(MAX) \t,@sqlDropColumn NVARCHAR(MAX);  WITH cteBase AS ( \tSELECT s.[name] AS SchemaName \t\t,t.[name] AS TableName \t\t,c.[name] AS ColumnName \t\t,CASE  \t\t\tWHEN tp.[name] IN ( \t\t\t\t\t'varchar' \t\t\t\t\t,'char' \t\t\t\t\t) \t\t\t\tTHEN tp.[name] + '(' + CASE  \t\t\t\t\t\tWHEN c.max_length = - 1 \t\t\t\t\t\t\tTHEN 'max' \t\t\t\t\t\tELSE CAST(c.max_length AS VARCHAR(25)) \t\t\t\t\t\tEND + ')' \t\t\tWHEN tp.[name] IN ( \t\t\t\t\t'nvarchar' \t\t\t\t\t,'nchar' \t\t\t\t\t) \t\t\t\tTHEN tp.[name] + '(' + CASE  \t\t\t\t\t\tWHEN c.max_length = - 1 \t\t\t\t\t\t\tTHEN 'max' \t\t\t\t\t\tELSE CAST(c.max_length / 2 AS VARCHAR(25)) \t\t\t\t\t\tEND + ')' \t\t\tWHEN tp.[name] IN ( \t\t\t\t\t'decimal' \t\t\t\t\t,'numeric' \t\t\t\t\t) \t\t\t\tTHEN tp.[name] + '(' + CAST(c.[precision] AS VARCHAR(25)) + ', ' + CAST(c.[scale] AS VARCHAR(25)) + ')' \t\t\tWHEN tp.[name] IN ('datetime2') \t\t\t\tTHEN tp.[name] + '(' + CAST(c.[scale] AS VARCHAR(25)) + ')' \t\t\tELSE tp.[name] \t\t\tEND AS DataTypeFull \t\t,tp.[name] AS [RawType] \t\t,c.max_length AS [MaxLength] \t\t,c.[precision] AS [Precision] \t\t,c.scale AS [Scale] \tFROM sys.tables t \tJOIN sys.schemas s ON t.schema_id = s.schema_id \tJOIN sys.columns c ON t.object_id = c.object_id \tJOIN sys.types tp ON c.user_type_id = tp.user_type_id \tWHERE s.[name] = @SchemaName \t\tAND t.[name] = @TableNameBase \t) \t,cteStage AS ( \tSELECT s.[name] AS SchemaName \t\t,t.[name] AS TableName \t\t,c.[name] AS ColumnName \t\t,CASE  \t\t\tWHEN tp.[name] IN ( \t\t\t\t\t'varchar' \t\t\t\t\t,'char' \t\t\t\t\t) \t\t\t\tTHEN tp.[name] + '(' + CASE  \t\t\t\t\t\tWHEN c.max_length = - 1 \t\t\t\t\t\t\tTHEN 'max' \t\t\t\t\t\tELSE CAST(c.max_length AS VARCHAR(25)) \t\t\t\t\t\tEND + ')' \t\t\tWHEN tp.[name] IN ( \t\t\t\t\t'nvarchar' \t\t\t\t\t,'nchar' \t\t\t\t\t) \t\t\t\tTHEN tp.[name] + '(' + CASE  \t\t\t\t\t\tWHEN c.max_length = - 1 \t\t\t\t\t\t\tTHEN 'max' \t\t\t\t\t\tELSE CAST(c.max_length / 2 AS VARCHAR(25)) \t\t\t\t\t\tEND + ')' \t\t\tWHEN tp.[name] IN ( \t\t\t\t\t'decimal' \t\t\t\t\t,'numeric' \t\t\t\t\t) \t\t\t\tTHEN tp.[name] + '(' + CAST(c.[precision] AS VARCHAR(25)) + ', ' + CAST(c.[scale] AS VARCHAR(25)) + ')' \t\t\tWHEN tp.[name] IN ('datetime2') \t\t\t\tTHEN tp.[name] + '(' + CAST(c.[scale] AS VARCHAR(25)) + ')' \t\t\tELSE tp.[name] \t\t\tEND AS DataTypeFull \t\t,tp.[name] AS [RawType] \t\t,c.max_length AS [MaxLength] \t\t,c.[precision] AS [Precision] \t\t,c.scale AS [Scale] \tFROM sys.tables t \tJOIN sys.schemas s ON t.schema_id = s.schema_id \tJOIN sys.columns c ON t.object_id = c.object_id \tJOIN sys.types tp ON c.user_type_id = tp.user_type_id \tWHERE s.[name] = @SchemaName \t\tAND t.[name] = @TableNameStage \t) SELECT @sqlUpdateColumn = CONCAT ( \t\t'EXECUTE AS USER = ''Userstaticrc10''; ' \t\t,STRING_AGG(CASE  \t\t\t\tWHEN s.DataTypeFull <> b.DataTypeFull \t\t\t\t\tAND s.MaxLength > b.MaxLength \t\t\t\t\tTHEN CONCAT ( \t\t\t\t\t\t\t'ALTER TABLE ' \t\t\t\t\t\t\t,QUOTENAME(b.SchemaName) \t\t\t\t\t\t\t,'.' \t\t\t\t\t\t\t,QUOTENAME(b.TableName) \t\t\t\t\t\t\t,' ALTER COLUMN ' \t\t\t\t\t\t\t,QUOTENAME(s.ColumnName) \t\t\t\t\t\t\t,' ' \t\t\t\t\t\t\t,s.DataTypeFull \t\t\t\t\t\t\t,';' \t\t\t\t\t\t\t) \t\t\t\tELSE NULL \t\t\t\tEND, ' ') \t\t) \t,@sqlAddColumn = CONCAT ( \t\t'EXECUTE AS USER = ''Userstaticrc10''; ' \t\t,STRING_AGG(CASE  \t\t\t\tWHEN b.ColumnName IS NULL \t\t\t\t\tTHEN CONCAT ( \t\t\t\t\t\t\t'ALTER TABLE ' \t\t\t\t\t\t\t,QUOTENAME(s.SchemaName) \t\t\t\t\t\t\t,'.' \t\t\t\t\t\t\t,QUOTENAME(@TableNameBase) \t\t\t\t\t\t\t,' ADD ' \t\t\t\t\t\t\t,QUOTENAME(s.ColumnName) \t\t\t\t\t\t\t,' ' \t\t\t\t\t\t\t,s.DataTypeFull \t\t\t\t\t\t\t,';' \t\t\t\t\t\t\t) \t\t\t\tELSE NULL \t\t\t\tEND, ' ') \t\t) \t,@sqlDropColumn = CONCAT ( \t\t'EXECUTE AS USER = ''Userstaticrc10''; ' \t\t,STRING_AGG(CASE  \t\t\t\tWHEN s.ColumnName IS NULL \t\t\t\t\tTHEN CONCAT ( \t\t\t\t\t\t\t'ALTER TABLE ' \t\t\t\t\t\t\t,QUOTENAME(b.SchemaName) \t\t\t\t\t\t\t,'.' \t\t\t\t\t\t\t,QUOTENAME(b.TableName) \t\t\t\t\t\t\t,' DROP COLUMN ' \t\t\t\t\t\t\t,QUOTENAME(b.ColumnName) \t\t\t\t\t\t\t,';' \t\t\t\t\t\t\t) \t\t\t\tELSE NULL \t\t\t\tEND, ' ') \t\t) FROM cteBase AS b FULL JOIN cteStage AS s ON s.ColumnName = b.ColumnName;  PRINT (@sqlUpdateColumn); PRINT (@sqlAddColumn); PRINT (@sqlDropColumn);  EXEC (@sqlUpdateColumn); EXEC (@sqlAddColumn);  EXEC (@sqlDropColumn)"
			}
		},
		"folder": {
			"name": "Synapse Lakehouse Sync"
		},
		"annotations": [],
		"lastPublishTime": "2022-09-07T18:28:28Z"
	},
	"type": "Microsoft.Synapse/workspaces/pipelines"
}