{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "synapsesynccav"
		},
		"LS_Synapse_Managed_Identity_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'LS_Synapse_Managed_Identity'"
		},
		"synapsesynccav-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synapsesynccav-WorkspaceDefaultSqlServer'"
		},
		"synapsesynccav-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://synapsesynccav.dfs.core.windows.net/"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline Passthrough')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Notebook1",
						"type": "DatabricksNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebookPath": "/Passthrough from Synapse Pipeline"
						},
						"linkedServiceName": {
							"referenceName": "AzureDatabricks1",
							"type": "LinkedServiceReference"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2022-09-07T16:50:28Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDatabricks1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SynapseLakehouseSync')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "The pipeline that orchestrates the artifacts the sync the delta table to a Synapse dedicated pool.",
				"activities": [
					{
						"name": "Lookup - Sync Pools",
						"description": "Query the csv data in the StorageAccountNameMetadata parameter ADLS location. This gets the table names and locations for the data to be sync'd to the Synapse dedicated pool.",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": {
									"value": "@concat('SELECT PoolName, SchemaName, TableName, KeyColumns\n,DeltaDataADLSFullPath, DeltaDataDatabricksKeyVaultScope, DeltaDataAzureKeyVaultSecretName\n,SynapseSyncDataADLSFullPath, SynapseSyncDataDatabricksKeyVaultScope, SynapseSyncDataAzureKeyVaultSecretName\n,''', pipeline().DataFactory, ''' AS SynapseWorkspaceName\n FROM OPENROWSET(\n        BULK ''', pipeline().parameters.StorageAccountNameMetadata, ''',\n        FORMAT = ''CSV'',\n\t\tPARSER_VERSION = ''2.0'', \n        HEADER_ROW = TRUE\n    ) AS [result]')",
									"type": "Expression"
								},
								"queryTimeout": "24:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "DS_Synapse_Managed_Identity",
								"type": "DatasetReference",
								"parameters": {
									"ServerName": {
										"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
										"type": "Expression"
									},
									"DatabaseName": "master"
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "ForEach - Lakehouse Table",
						"description": "Loop through the items in the PoolSchemaTableArray variable. This will call the Azure Databricks notebook that will read and write the CDC changes from each delta table to a staging location in ADLS.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "ForEach - Pool Schema",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "ForEach - Create Delta Sync Tracking Tables",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@variables('MetadataArray')",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Spark - Synapse Lakehouse Sync ADLS",
									"description": "An Azure Databricks notebook that will read the change data feed of the delta table and write the changes out to ADLS. ",
									"type": "DatabricksNotebook",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"notebookPath": "/Synapse Lakehouse Sync/Synapse Lakehouse Sync ADLS",
										"baseParameters": {
											"SynapseLakehouseSyncParameters": {
												"value": "@string(item())",
												"type": "Expression"
											}
										}
									},
									"linkedServiceName": {
										"referenceName": "LS_AzureDatabricks_Managed_Identity",
										"type": "LinkedServiceReference"
									}
								},
								{
									"name": "Append variable - DatabricksOutputArray",
									"description": "Append the output from the Spark - Synapse Lakehouse Sync ADLS activity to the DatabricksOutputArray variable. This is used downstream in the pipeline to identify what changes types have occurred (full_load or incremental) and used for logging purposes.",
									"type": "AppendVariable",
									"dependsOn": [
										{
											"activity": "Spark - Synapse Lakehouse Sync ADLS",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"variableName": "DatabricksOutputArray",
										"value": {
											"value": "@activity('Spark - Synapse Lakehouse Sync ADLS').Output.runOutput",
											"type": "Expression"
										}
									}
								}
							]
						}
					},
					{
						"name": "Set Runtime Variables",
						"description": "Set the pipeline runtime variables. Used for logging/tracking purposes to identify what pipeline run loaded the data.",
						"type": "SetVariable",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"variableName": "PipelineValues",
							"value": {
								"value": "@array(json(concat('{\n\"PipelineRunId\": \"', pipeline().RunId ,'\"'\n,',\"PipelineStartDate\": \"', formatDateTime(convertFromUtc(pipeline().TriggerTime, 'Eastern Standard Time'), 'yyyyMMdd'), '\"'\n,',\"PipelineStartDateTime\": \"', formatDateTime(convertFromUtc(pipeline().TriggerTime, 'Eastern Standard Time'), 'yyyy-MM-dd HH:mm:ss'), '\"'\n,'}')))",
								"type": "Expression"
							}
						}
					},
					{
						"name": "ForEach - Pool Schema",
						"description": "Loop through each unique combination PoolName and SchemaName from the Lookup - Distinct Pools Schema activity.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Lookup - Distinct Pools Schema",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Lookup - Distinct Pools Schema').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Create Schema if not exists",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@CONCAT('EXECUTE AS user = ''Userstaticrc10'' \nIF NOT EXISTS (SELECT 1 FROM sys.schemas WHERE [name] = ''', item().SchemaName, ''') EXEC(''CREATE SCHEMA ', item().SchemaName, ''');SELECT 1 AS a')",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@item().PoolName",
													"type": "Expression"
												}
											}
										}
									}
								}
							]
						}
					},
					{
						"name": "ForEach - Pool Schema Table",
						"description": "Loop through the rows from the Lookup - Sync Pools activity and query the Synapse dedicated pool to check if the table already exists. Append the output to the PoolSchemaTableArray variable.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Lookup - Sync Pools",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Set Runtime Variables",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Lookup - Sync Pools').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Lookup - Check if table exists",
									"description": "Query the Synapse dedicated pool to check if the table already exists.",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat('SELECT COALESCE((SELECT ''True'' FROM sys.tables WHERE SCHEMA_NAME(schema_id) = ''', item().SchemaName, ''' AND [name] = ''', item().TableName, '''), ''False'') AS ExistsFlagSynapse\n\t\t,''', item().PoolName, ''' AS PoolName\n\t\t,''', item().SchemaName, ''' AS SchemaName\n\t\t,''', item().TableName, ''' AS TableName\n\t\t,''', item().KeyColumns, ''' AS KeyColumns\n\t\t,''', item().DeltaDataADLSFullPath, ''' AS DeltaDataADLSFullPath\n\t\t,''', item().DeltaDataDatabricksKeyVaultScope, ''' AS DeltaDataDatabricksKeyVaultScope\n\t\t,''', item().DeltaDataAzureKeyVaultSecretName, ''' AS DeltaDataAzureKeyVaultSecretName\n\t\t,''', item().SynapseSyncDataADLSFullPath, ''' AS SynapseSyncDataADLSFullPath\n\t\t,''', item().SynapseSyncDataDatabricksKeyVaultScope, ''' AS SynapseSyncDataDatabricksKeyVaultScope\n\t\t,''', item().SynapseSyncDataAzureKeyVaultSecretName, ''' AS SynapseSyncDataAzureKeyVaultSecretName\n\t\t,''', item().SynapseWorkspaceName, ''' AS SynapseWorkspaceName\n\t\t,''', variables('DropTableFlag'), ''' AS DropTableFlag\n')",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@item().PoolName",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": true
									}
								},
								{
									"name": "Append variable - MetadataArray",
									"description": "Append the output from the Lookup - Check if table exists activity to the MetadataArray variable.",
									"type": "AppendVariable",
									"dependsOn": [
										{
											"activity": "Lookup - Check if table exists",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"variableName": "MetadataArray",
										"value": {
											"value": "@activity('Lookup - Check if table exists').output.firstRow",
											"type": "Expression"
										}
									}
								}
							]
						}
					},
					{
						"name": "ForEach - Create and Load the Synapse Table",
						"description": "Loop through each item from the DatabricksOutputArray variable and call the SynapseLakehouseSyncTableLoad pipeline if changes were detected from the previous load.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "ForEach - Lakehouse Table",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "ForEach - Create Synapse Logging Tables",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@variables('DatabricksOutputArray')",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "If Condition - Changes Detected",
									"description": "Check if the ChangeTypes item is empty. If its empty, then no changes have occurred on the delta tables since the last Synapse sync process. We do not need to call the SynapseLakehouseSyncTableLoad pipeline in this case.",
									"type": "IfCondition",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"expression": {
											"value": "@not(empty(item().ChangeTypes))",
											"type": "Expression"
										},
										"ifTrueActivities": [
											{
												"name": "Execute - SynapseLakehouseSyncTableLoad",
												"description": "Execute the SynapseLakehouseSyncTableLoad pipeline to load the data staged in ADLS to the Synapse dedicated pool.",
												"type": "ExecutePipeline",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"pipeline": {
														"referenceName": "SynapseLakehouseSyncTableLoad",
														"type": "PipelineReference"
													},
													"waitOnCompletion": true,
													"parameters": {
														"PipelineValue": {
															"value": "@variables('PipelineValues')",
															"type": "Expression"
														},
														"DatabricksOutput": {
															"value": "@item()",
															"type": "Expression"
														}
													}
												}
											}
										]
									}
								}
							]
						}
					},
					{
						"name": "ForEach - Change Tracking Table",
						"description": "Loop through the items in from the Lookup - Get Distinct SynapseLakehouseTracking Tables activity. This is used to run the vacuum and optimize commands on the each of the _SynapseLakehouseSync delta tables.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Lookup - Get Distinct SynapseLakehouseTracking Tables",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Lookup - Get Distinct SynapseLakehouseTracking Tables').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Spark - Synapse Lakehouse Sync Tracking Table Optimize",
									"description": "Run the vacuum with 0 hour retention and optimize commands for the _SynapseLakehouseSync delta table.",
									"type": "DatabricksNotebook",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"notebookPath": "/Synapse Lakehouse Sync/Synapse Lakehouse Sync Tracking Table Optimize",
										"baseParameters": {
											"SynapseLakehouseSyncParameters": {
												"value": "@string(item())",
												"type": "Expression"
											}
										}
									},
									"linkedServiceName": {
										"referenceName": "LS_AzureDatabricks_Managed_Identity",
										"type": "LinkedServiceReference"
									}
								}
							]
						}
					},
					{
						"name": "Lookup - Get Distinct SynapseLakehouseTracking Tables",
						"description": "Get the distinct PoolName and SyncFolderPathFull combinations from the Lookup - Sync Pool activity. This is used to run the vacuum and optimize commands on the each of the _SynapseLakehouseSync delta tables.",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "ForEach - Create and Load the Synapse Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": {
									"value": "@concat('DECLARE @json NVARCHAR(MAX)  = '''\n, activity('Lookup - Sync Pools').output.value\n, ''' \n\n;WITH cte AS\n(\n\tSELECT DISTINCT PoolName, SynapseWorkspaceName\n\t\t,SynapseSyncDataADLSFullPath, SynapseSyncDataDatabricksKeyVaultScope, SynapseSyncDataAzureKeyVaultSecretName\n\tFROM OPENJSON(@json)\n\tWITH\n\t(\n\t\tPoolName NVARCHAR(1000) ''$.PoolName''\n\t\t,SynapseWorkspaceName NVARCHAR(1000) ''$.SynapseWorkspaceName''\n\t\t,DeltaDataADLSFullPath NVARCHAR(1000) ''$.DeltaDataADLSFullPath''\n\t\t,SynapseSyncDataADLSFullPath NVARCHAR(1000) ''$.SynapseSyncDataADLSFullPath''\n\t\t,SynapseSyncDataDatabricksKeyVaultScope NVARCHAR(1000) ''$.SynapseSyncDataDatabricksKeyVaultScope''\n\t\t,SynapseSyncDataAzureKeyVaultSecretName NVARCHAR(1000) ''$.SynapseSyncDataAzureKeyVaultSecretName''\n\t)\n)\nSELECT *\n\t\t--,(SELECT * FROM cte FOR JSON AUTO, WITHOUT_ARRAY_WRAPPER) AS JsonString\nFROM cte\n')",
									"type": "Expression"
								},
								"queryTimeout": "24:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "DS_Synapse_Managed_Identity",
								"type": "DatasetReference",
								"parameters": {
									"ServerName": {
										"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
										"type": "Expression"
									},
									"DatabaseName": "master"
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "ForEach - Create Delta Sync Tracking Tables",
						"description": "Loop through each unique combination PoolName and SyncFolderPathFull from the Lookup - Distinct Pools ChangesFolderPathFull activity.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Lookup - Distinct Pools ChangesFolderPathFull",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Lookup - Distinct Pools ChangesFolderPathFull').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Spark - Synapse Lakehouse Sync Create Tracking Table",
									"type": "DatabricksNotebook",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"notebookPath": "/Synapse Lakehouse Sync/Synapse Lakehouse Sync Create Tracking Table",
										"baseParameters": {
											"SynapseLakehouseSyncParameters": {
												"value": "@item().JsonString",
												"type": "Expression"
											}
										}
									},
									"linkedServiceName": {
										"referenceName": "LS_AzureDatabricks_Managed_Identity",
										"type": "LinkedServiceReference"
									}
								}
							]
						}
					},
					{
						"name": "Lookup - Distinct Pools",
						"description": "Get the distinct PoolName from the Lookup - Sync Pool activity. This is used to create the logging table in Synapse that is used to track row counts of the source delta tables and the tables in Synapse.",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "ForEach - Pool Schema Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": {
									"value": "@concat('DECLARE @sql NVARCHAR(MAX)\n\nSET @sql = ''', activity('Lookup - Sync Pools').output.value, '''\n\nSELECT DISTINCT PoolName\nFROM OPENJSON(@sql)\nWITH\n(\n\tPoolName NVARCHAR(1000) ''$.PoolName''\n)\n')",
									"type": "Expression"
								},
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "DS_Synapse_Managed_Identity",
								"type": "DatasetReference",
								"parameters": {
									"ServerName": {
										"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
										"type": "Expression"
									},
									"DatabaseName": "master"
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "ForEach - Create Synapse Logging Tables",
						"description": "Loop through the items in from the Lookup - Distinct Pools activity. This will create the logging tables for each pool defined in the loading metadata.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Lookup - Distinct Pools",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Lookup - Distinct Pools').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Create Log Tables If Not Exists",
									"description": "Create the two logging tables in the Synapse dedicated pool.",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": "EXECUTE AS USER = 'Userstaticrc10';\n\nIF NOT EXISTS (SELECT 1 FROM sys.schemas WHERE [name] = 'logging')\n    EXEC ('CREATE SCHEMA [logging]')\n;\n\nIF OBJECT_ID('logging.DataProfile', 'U') IS NULL\nCREATE TABLE logging.DataProfile\n(\n\tId INT IDENTITY(1,1) NOT NULL\n\t,PipelineRunId NVARCHAR(50) NOT NULL\n\t,PipelineStartDate INT NOT NULL\n\t,PipelineStartDateTime DATETIME2(0) NOT NULL\n    ,SchemaName NVARCHAR(100) NOT NULL\n    ,TableName NVARCHAR(100) NOT NULL\n\t,ColumnName NVARCHAR(100) NOT NULL\n\t,DataTypeName NVARCHAR(100) NOT NULL\n\t,DataTypeFull NVARCHAR(100) NOT NULL\n\t,CharacterLength INT NULL\n\t,PrecisionValue INT NULL\t\n\t,ScaleValue INT NULL\t\n\t,UniqueValueCount BIGINT NOT NULL\n\t,NullCount BIGINT NOT NULL\n\t,MinValue NVARCHAR(MAX)\n\t,MaxValue NVARCHAR(MAX)\n\t,MinLength INT\n\t,MaxLength INT\n\t,DataAverage NUMERIC(30,2)\n\t,DataStdevp FLOAT\n\t,TableRowCount BIGINT NOT NULL\n\t,TableDataSpaceGB NUMERIC(20,2) NOT NULL\n\t,WeightedScore NUMERIC(30,4)\n\t,SqlCommandDataProfile NVARCHAR(MAX) NOT NULL\n\t,SqlCommandCTAS NVARCHAR(MAX) NOT NULL\n    ,RowInsertDateTime DATETIME2(0) NOT NULL\n)\nWITH (DISTRIBUTION = ROUND_ROBIN, CLUSTERED INDEX(PipelineStartDateTime, PipelineRunId)\n)\n;\n\nIF OBJECT_ID('logging.SynapseLakehouseSync', 'U') IS NULL\nCREATE TABLE logging.SynapseLakehouseSync\n(\n\tId BIGINT IDENTITY(1,1) NOT NULL\n\t,PoolName NVARCHAR(100) NOT NULL\n    ,SchemaName NVARCHAR(100) NOT NULL\n    ,TableName NVARCHAR(100) NOT NULL\n\t,TableRowCountADLS BIGINT NULL\n\t,TableRowCountSynapse BIGINT NULL\n\t,DeltaDataADLSFullPath NVARCHAR(1000)\n\t,SynapseSyncDataADLSFullPath NVARCHAR(1000)\n    ,RowInsertDateTime DATETIME2(0) NOT NULL\n)\nWITH \n(\n\tDISTRIBUTION = ROUND_ROBIN, CLUSTERED INDEX(RowInsertDateTime)\n)\n;\nSELECT 1 AS a",
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@item().PoolName",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": false
									}
								}
							]
						}
					},
					{
						"name": "Lookup - Distinct Pools ChangesFolderPathFull",
						"description": "Get the distinct PoolName and SyncFolderPathFull combinations from the Lookup - Sync Pool activity. This is used to create the _SynapseLakehouseTracking delta table used to track each Synapse sync load.",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "ForEach - Pool Schema Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": {
									"value": "@concat('DECLARE @sql NVARCHAR(MAX)\n\nSET @sql = ''', activity('Lookup - Sync Pools').output.value, '''\n\n;WITH cte AS\n(\n\tSELECT DISTINCT PoolName, SynapseWorkspaceName, SynapseSyncDataADLSFullPath, SynapseSyncDataDatabricksKeyVaultScope, SynapseSyncDataAzureKeyVaultSecretName\n\tFROM OPENJSON(@sql)\n\tWITH\n\t(\n\t\tPoolName NVARCHAR(1000) ''$.PoolName''\n\t\t,SynapseWorkspaceName NVARCHAR(1000) ''$.SynapseWorkspaceName''\n\t\t,SynapseSyncDataADLSFullPath NVARCHAR(1000) ''$.SynapseSyncDataADLSFullPath''\n\t\t,SynapseSyncDataDatabricksKeyVaultScope NVARCHAR(1000) ''$.SynapseSyncDataDatabricksKeyVaultScope''\n\t\t,SynapseSyncDataAzureKeyVaultSecretName NVARCHAR(1000) ''$.SynapseSyncDataAzureKeyVaultSecretName''\n\t)\n)\nSELECT *\n\t\t,(SELECT * FROM cte FOR JSON AUTO, WITHOUT_ARRAY_WRAPPER) AS JsonString\nFROM cte\n')",
									"type": "Expression"
								},
								"queryTimeout": "24:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "DS_Synapse_Managed_Identity",
								"type": "DatasetReference",
								"parameters": {
									"ServerName": {
										"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
										"type": "Expression"
									},
									"DatabaseName": "master"
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Lookup - Distinct Pools Schema",
						"description": "Get the distinct PoolName and SchemaName combinations from the Lookup - Sync Pool activity. This is used to create the schemas in each pool for the data to be loaded.",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "ForEach - Pool Schema Table",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": {
									"value": "@concat('DECLARE @sql NVARCHAR(MAX)\n\nSET @sql = ''', activity('Lookup - Sync Pools').output.value, '''\n\nSELECT DISTINCT PoolName, SchemaName\nFROM OPENJSON(@sql)\nWITH\n(\n\tPoolName NVARCHAR(1000) ''$.PoolName''\n\t,SchemaName NVARCHAR(1000) ''$.SchemaName''\n)\n')",
									"type": "Expression"
								},
								"queryTimeout": "24:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "DS_Synapse_Managed_Identity",
								"type": "DatasetReference",
								"parameters": {
									"ServerName": {
										"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
										"type": "Expression"
									},
									"DatabaseName": "master"
								}
							},
							"firstRowOnly": false
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"StorageAccountNameMetadata": {
						"type": "string",
						"defaultValue": "https://synapsesynccav.dfs.core.windows.net/data/Synapse_Lakehouse_Sync_Metadata.csv"
					}
				},
				"variables": {
					"DatabricksOutputArray": {
						"type": "Array"
					},
					"PipelineValues": {
						"type": "Array"
					},
					"MetadataArray": {
						"type": "Array"
					},
					"DropTableFlag": {
						"type": "Boolean",
						"defaultValue": true
					}
				},
				"folder": {
					"name": "Synapse Lakehouse Sync"
				},
				"annotations": [],
				"lastPublishTime": "2022-09-07T19:39:10Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DS_Synapse_Managed_Identity')]",
				"[concat(variables('workspaceId'), '/linkedServices/LS_AzureDatabricks_Managed_Identity')]",
				"[concat(variables('workspaceId'), '/pipelines/SynapseLakehouseSyncTableLoad')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SynapseLakehouseSyncTableLoad')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "The pipeline that orchestrates the staging and loading of the data from the delta change data feed to the Synapse dedicate pool tables.",
				"activities": [
					{
						"name": "Filter - Deletes",
						"description": "Filter ChangeTypesArray parameter to tables that have delete as a change type.",
						"type": "Filter",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.DatabricksOutput['ChangeTypes']",
								"type": "Expression"
							},
							"condition": {
								"value": "@contains(item(), 'delete')",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Filter - Update",
						"description": "Filter ChangeTypesArray parameter to tables that have updates as a change type.",
						"type": "Filter",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.DatabricksOutput['ChangeTypes']",
								"type": "Expression"
							},
							"condition": {
								"value": "@contains(item(), 'update_postimage')",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Filter - Insert",
						"description": "Filter ChangeTypesArray parameter to tables that have insert as a change type.",
						"type": "Filter",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.DatabricksOutput['ChangeTypes']",
								"type": "Expression"
							},
							"condition": {
								"value": "@contains(item(), 'insert')",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Filter - Full Load",
						"description": "Loop through tables that had full_load changes to the records.",
						"type": "Filter",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.DatabricksOutput['ChangeTypes']",
								"type": "Expression"
							},
							"condition": {
								"value": "@contains(item(), 'full_load')",
								"type": "Expression"
							}
						}
					},
					{
						"name": "ForEach - Pool - Deletes",
						"description": "Loop through tables that had delete changes to the records. The pipeline is at the table grain already but the loop is a way to separate and wrap the different steps.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Filter - Insert",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Filter - Update",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Filter - Deletes",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Filter - Deletes').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Get Resource Class Required",
									"description": "Profile the data that was loaded into the staging table and get determine the minimum static resource that should be used to load the data.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Execute COPY INTO Staging Statement",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat(replace(replace(replace(variables('GetResourceClass')\n, '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_delete'))\n, '{PipelineId}', pipeline().parameters.PipelineValue[0].PipelineRunId)\n)",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": true
									}
								},
								{
									"name": "Create Staging Table DDL - Serverless",
									"description": "Profile the data using the Synapse serverless engine and generate the optimal table DDL.",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@replace(replace(replace(variables('SqlCommandCreateStagingTable'), '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_delete'))\n,  '{FolderPathFull}', concat(pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['SynapseWorkspaceName'], '_', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName'], '/_change_type=delete'))",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": "master"
											}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "Create Staging Tables",
									"description": "Execute the DDL for the creation of the staging table from the previous step in the Synapse dedicated pool.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Create Staging Table DDL - Serverless",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' '\n, activity('Create Staging Table DDL - Serverless').output.value[0].CreateTableDDL, '; SELECT 1 AS a')",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "Execute COPY INTO Staging Statement",
									"description": "Execute the COPY INTO statement to move the data from ADLS to the newly created staging table.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Create Staging Tables",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' COPY INTO ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_delete'\n, '] FROM '''\n, pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['SynapseWorkspaceName'], '_', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName']\n, '/_change_type=delete'\n, ''' WITH ( FILE_TYPE = ''PARQUET'', MAXERRORS = 0, COMPRESSION = ''snappy'', IDENTITY_INSERT = ''OFF'', CREDENTIAL = (IDENTITY = ''Managed Identity'')) OPTION (LABEL = ''COPY - ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_delete', '] - '\n, pipeline().parameters.PipelineValue[0].PipelineRunId, ''');SELECT 1 AS a')",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "Script - Delete",
									"description": "Execute the delete statement on the final table from the data that was staged.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Get Resource Class Required",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat(replace(replace(replace(replace(variables('sqlDelete')\n,'{SchemaName}', pipeline().parameters.DatabricksOutput['SchemaName'])\n,'{TableName}', pipeline().parameters.DatabricksOutput['TableName'])\n,'{UserName}', activity('Get Resource Class Required').output.firstRow.UserName)\n,'{KeyColumns}', pipeline().parameters.DatabricksOutput['KeyColumns'])\n,' SELECT 1 AS a'\n)",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": true
									}
								}
							]
						}
					},
					{
						"name": "ForEach - Pool - Updates",
						"description": "Loop through tables that had update changes to the records. The pipeline is at the table grain already but the loop is a way to separate and wrap the different steps.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "ForEach - Pool - Deletes",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Filter - Update').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Get Resource Class Required - Update",
									"description": "Profile the data that was loaded into the staging table and get determine the minimum static resource that should be used to load the data.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Execute COPY INTO Staging Statement - Update",
											"dependencyConditions": [
												"Succeeded"
											]
										},
										{
											"activity": "Alter Base Tables Column Data Types - Update",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat(replace(replace(replace(variables('GetResourceClass')\n, '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_update'))\n, '{PipelineId}', pipeline().parameters.PipelineValue[0].PipelineRunId)\n)",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": true
									}
								},
								{
									"name": "Create Staging Table DDL - Serverless - Update",
									"description": "Profile the data using the Synapse serverless engine and generate the optimal table DDL.",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@replace(replace(replace(variables('SqlCommandCreateStagingTable'), '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_update'))\n,  '{FolderPathFull}', concat(pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['SynapseWorkspaceName'], '_', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName'], '/_change_type=update_postimage'))\n",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": "master"
											}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "Create Staging Tables - Update",
									"description": "Execute the DDL for the creation of the staging table from the previous step in the Synapse dedicated pool.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Create Staging Table DDL - Serverless - Update",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' '\n, activity('Create Staging Table DDL - Serverless - Update').output.value[0].CreateTableDDL, '; SELECT 1 AS a')",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "Execute COPY INTO Staging Statement - Update",
									"description": "Execute the COPY INTO statement to move the data from ADLS to the newly created staging table.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Create Staging Tables - Update",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' COPY INTO ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_update'\n, '] FROM '''\n, pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['SynapseWorkspaceName'], '_', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName']\n, '/_change_type=update_postimage'\n, ''' WITH ( FILE_TYPE = ''PARQUET'', MAXERRORS = 0, COMPRESSION = ''snappy'', IDENTITY_INSERT = ''OFF'', CREDENTIAL = (IDENTITY = ''Managed Identity'')) OPTION (LABEL = ''COPY - ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_update', '] - '\n, pipeline().parameters.PipelineValue[0].PipelineRunId, ''');SELECT 1 AS a')",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "Script - Update",
									"description": "Execute the update statement on the final table from the data that was staged.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Get Resource Class Required - Update",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat(replace(replace(replace(replace(variables('sqlUpdate')\n,'{SchemaName}', pipeline().parameters.DatabricksOutput['SchemaName'])\n,'{TableName}', pipeline().parameters.DatabricksOutput['TableName'])\n,'{KeyColumns}', pipeline().parameters.DatabricksOutput['KeyColumns'])\n,'{UserName}', activity('Get Resource Class Required - Update').output.firstRow.UserName)\n,' SELECT 1 AS a'\n)",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": true
									}
								},
								{
									"name": "Alter Base Tables Column Data Types - Update",
									"description": "Update the final tables data types for the column if the data staged has a larger datatype defined. For example, the final table has varchar(10) but the staging table has varchar(20). We then alter the final table to be varchar(20).",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Create Staging Tables - Update",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat(replace(replace(replace(variables('sqlAlterColumn')\n, '{SchemaName}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameBase}', pipeline().parameters.DatabricksOutput['TableName'])\n, '{TableNameStage}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_update'))\n, '; SELECT 1 AS a')\n",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": false
									}
								}
							]
						}
					},
					{
						"name": "ForEach - Pool - Inserts",
						"description": "Loop through tables that had insert changes to the records. The pipeline is at the table grain already but the loop is a way to separate and wrap the different steps.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "ForEach - Pool - Updates",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Filter - Insert').output.value",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Get Resource Class Required - Insert",
									"description": "Profile the data that was loaded into the staging table and get determine the minimum static resource that should be used to load the data.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Execute COPY INTO Staging Statement - Insert",
											"dependencyConditions": [
												"Succeeded"
											]
										},
										{
											"activity": "Alter Base Tables Column Data Types - Insert",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat(replace(replace(replace(variables('GetResourceClass')\n, '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_insert'))\n, '{PipelineId}', pipeline().parameters.PipelineValue[0].PipelineRunId)\n)",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": true
									}
								},
								{
									"name": "Create Staging Table DDL - Serverless - Insert",
									"description": "Profile the data using the Synapse serverless engine and generate the optimal table DDL.",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@replace(replace(replace(variables('SqlCommandCreateStagingTable'), '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_insert'))\n, '{FolderPathFull}', concat(pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['SynapseWorkspaceName'], '_', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName'], '/_change_type=insert'))",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": "master"
											}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "Create Staging Tables - Insert",
									"description": "Execute the DDL for the creation of the staging table from the previous step in the Synapse dedicated pool.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Create Staging Table DDL - Serverless - Insert",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' '\n, activity('Create Staging Table DDL - Serverless - Insert').output.value[0].CreateTableDDL, '; SELECT 1 AS a')",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "Execute COPY INTO Staging Statement - Insert",
									"description": "Execute the COPY INTO statement to move the data from ADLS to the newly created staging table.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Create Staging Tables - Insert",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' COPY INTO ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_insert'\n, '] FROM '''\n, pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['SynapseWorkspaceName'], '_', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName']\n, '/_change_type=insert'\n, ''' WITH ( FILE_TYPE = ''PARQUET'', MAXERRORS = 0, COMPRESSION = ''snappy'', IDENTITY_INSERT = ''OFF'', CREDENTIAL = (IDENTITY = ''Managed Identity'')) OPTION (LABEL = ''COPY - ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_insert', '] - '\n, pipeline().parameters.PipelineValue[0].PipelineRunId, ''');SELECT 1 AS a')",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "Script - Insert",
									"description": "Execute the insert statement on the final table from the data that was staged.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Get Resource Class Required - Insert",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat(replace(replace(replace(replace(variables('sqlInsert')\n,'{SchemaName}', pipeline().parameters.DatabricksOutput['SchemaName'])\n,'{TableName}', pipeline().parameters.DatabricksOutput['TableName'])\n,'{KeyColumns}', pipeline().parameters.DatabricksOutput['KeyColumns'])\n,'{UserName}', activity('Get Resource Class Required - Insert').output.firstRow.UserName)\n,' SELECT 1 AS a'\n)\n",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": true
									}
								},
								{
									"name": "Alter Base Tables Column Data Types - Insert",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Create Staging Tables - Insert",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat(replace(replace(replace(variables('sqlAlterColumn')\n, '{SchemaName}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameBase}', pipeline().parameters.DatabricksOutput['TableName'])\n, '{TableNameStage}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_insert'))\n, '; SELECT 1 AS a')\n",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": false
									}
								}
							]
						}
					},
					{
						"name": "ForEach - Create and Load the Synapse Table",
						"description": "Loop through tables that had full_load changes to the records. The pipeline is at the table grain already but the loop is a way to separate and wrap the different steps.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "Filter - Full Load",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Filter - Full Load').output.value",
								"type": "Expression"
							},
							"isSequential": false,
							"activities": [
								{
									"name": "Get Resource Class Required - Full Load",
									"description": "Profile the data that was loaded into the staging table and get determine the minimum static resource that should be used to load the data.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Execute COPY INTO Staging Statement - Full Load",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat(replace(replace(replace(variables('GetResourceClass')\n, '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_full_load'))\n, '{PipelineId}', pipeline().parameters.PipelineValue[0].PipelineRunId)\n)",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": true
									}
								},
								{
									"name": "Create Staging Table DDL - Serverless - Full Load",
									"description": "Profile the data using the Synapse serverless engine and generate the optimal table DDL.",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@replace(replace(replace(variables('SqlCommandCreateStagingTable'), '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_full_load'))\n,  '{FolderPathFull}', concat(pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['SynapseWorkspaceName'], '_', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName'], '/_change_type=full_load'))",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": "master"
											}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "Create Staging Tables - Full Load",
									"description": "Execute the DDL for the creation of the staging table from the previous step in the Synapse dedicated pool.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Create Staging Table DDL - Serverless - Full Load",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' '\n, activity('Create Staging Table DDL - Serverless - Full Load').output.value[0].CreateTableDDL, '; SELECT 1 AS a')",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "Execute COPY INTO Staging Statement - Full Load",
									"description": "Execute the COPY INTO statement to move the data from ADLS to the newly created staging table.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Create Staging Tables - Full Load",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat('EXECUTE AS user = ''Userstaticrc10'' COPY INTO ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_full_load'\n, '] FROM '''\n, pipeline().parameters.DatabricksOutput['SynapseSync_https'], '/', pipeline().parameters.DatabricksOutput['SynapseWorkspaceName'], '_', pipeline().parameters.DatabricksOutput['PoolName'], '_SynapseLakehouseSync/', pipeline().parameters.DatabricksOutput['SchemaName'], '/', pipeline().parameters.DatabricksOutput['TableName']\n, '/_change_type=full_load'\n, ''' WITH ( FILE_TYPE = ''PARQUET'', MAXERRORS = 0, COMPRESSION = ''snappy'', IDENTITY_INSERT = ''OFF'', CREDENTIAL = (IDENTITY = ''Managed Identity'')) OPTION (LABEL = ''COPY - ['\n, pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_full_load', '] - '\n, pipeline().parameters.PipelineValue[0].PipelineRunId, ''');SELECT 1 AS a')",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "Profile Staging Table",
									"description": "Profile the staging table and try to determine the best distribution and index.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Get Resource Class Required - Full Load",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat(replace(replace(replace(replace(replace(replace(replace(variables('CTAS')\n\t, '{ResourceClassName}', activity('Get Resource Class Required - Full Load').output.firstRow.UserName)\n\t, '{SchemaNameStaging}', pipeline().parameters.DatabricksOutput['SchemaName'])\n\t, '{TableNameStaging}', concat(pipeline().parameters.DatabricksOutput['TableName'], '_full_load'))\n\t, '{TableNameFinal}', pipeline().parameters.DatabricksOutput['TableName'])\n\t, '{PipelineRunId}', pipeline().parameters.PipelineValue[0].PipelineRunId)\n\t, '{PipelineStartDate}', pipeline().parameters.PipelineValue[0].PipelineStartDate)\n\t, '{PipelineStartDateTime}', pipeline().parameters.PipelineValue[0].PipelineStartDateTime)\n)",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										}
									}
								},
								{
									"name": "Create Final Tables - Auto Distribution and Index",
									"description": "Run the CTAS command to load the data from the staging table to the final table with the appropriate distribution and index defined.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Drop Table if Exists",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@CONCAT('/*Auto Distribution/Index*/ EXECUTE AS user = ''', activity('Get Resource Class Required - Full Load').output.firstRow.UserName\n, ''' IF OBJECT_ID(''', pipeline().parameters.DatabricksOutput['SchemaName'], '.', pipeline().parameters.DatabricksOutput['TableName'], ''', ''U'') IS NOT NULL DROP TABLE [', pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName']\n, ']; CREATE TABLE [', pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '] ', activity('Profile Staging Table').output.firstRow.DistributionIndex,' AS SELECT * FROM [', pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_full_load]  OPTION (LABEL = ''CTAS - [', pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '] - ', pipeline().parameters.PipelineValue[0].PipelineRunId, ''');SELECT 1 AS a')\n\n",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "Drop Table if Exists",
									"description": "Drop the staging table if it exists in Synapse dedicated pool.",
									"type": "Lookup",
									"dependsOn": [
										{
											"activity": "Profile Staging Table",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat('IF OBJECT_ID(''', pipeline().parameters.DatabricksOutput['SchemaName'], '.', pipeline().parameters.DatabricksOutput['TableName'], ''', ''U'') IS NOT NULL\n\tDROP TABLE [', pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '];'\n, ' SELECT 1 AS a')",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										},
										"firstRowOnly": false
									}
								}
							]
						}
					},
					{
						"name": "ForEach - Drop Temp Tables",
						"description": "If the DropTableFlag parameter is true, then drop the staging tables that are created during the loading process after the data has been loaded. This will keep the Synapse Dedicated Pool clean.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "ForEach - Create and Load the Synapse Table",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "ForEach - Pool - Inserts",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@if(bool(pipeline().parameters.DatabricksOutput['DropTableFlag'])\n, pipeline().parameters.DatabricksOutput['ChangeTypes']\n, take(array(''), 0)\n)",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "Drop Staging Tables",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "SqlDWSource",
											"sqlReaderQuery": {
												"value": "@concat('IF OBJECT_ID(''', pipeline().parameters.DatabricksOutput['SchemaName'], '.', pipeline().parameters.DatabricksOutput['TableName'], '_', if(equals(item(), 'update_postimage'), 'update', item()), ''', ''U'') IS NOT NULL\n    DROP TABLE [', pipeline().parameters.DatabricksOutput['SchemaName'], '].[', pipeline().parameters.DatabricksOutput['TableName'], '_', if(equals(item(), 'update_postimage'), 'update', item()), '];'\n, ' SELECT 1 AS a')",
												"type": "Expression"
											},
											"queryTimeout": "24:00:00",
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "DS_Synapse_Managed_Identity",
											"type": "DatasetReference",
											"parameters": {
												"ServerName": {
													"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
													"type": "Expression"
												},
												"DatabaseName": {
													"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
													"type": "Expression"
												}
											}
										}
									}
								}
							]
						}
					},
					{
						"name": "Spark - Synapse Lakehouse Sync Log Success",
						"description": "Call the Azure Databricks notebook to log in the _SynapseLakehouseSync delta table that the data was loaded successfully into Synapse.",
						"type": "DatabricksNotebook",
						"dependsOn": [
							{
								"activity": "ForEach - Drop Temp Tables",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Add TableRowCountSynapse to Json Object",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebookPath": "/Synapse Lakehouse Sync/Synapse Lakehouse Sync Tracking Table Log Success",
							"baseParameters": {
								"SynapseLakehouseSyncParameters": {
									"value": "@string(activity('Add TableRowCountSynapse to Json Object').output.value[0].JsonString)",
									"type": "Expression"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "LS_AzureDatabricks_Managed_Identity",
							"type": "LinkedServiceReference"
						}
					},
					{
						"name": "Log Row Count - SynapseLakehouseSync",
						"description": "Log the row count for the table in the logging.SynapseLakehouseSyncRecordCount logging table.",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "ForEach - Create and Load the Synapse Table",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "ForEach - Pool - Inserts",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": {
									"value": "@concat('EXECUTE AS USER = ''Userstaticrc10''; \nDECLARE @RowInsertDateTime DATETIME2(0) = GETDATE()\n    ,@TableRowCountSynapse BIGINT = (SELECT COUNT_BIG(*) FROM ', pipeline().parameters.DatabricksOutput['SchemaName'], '.', pipeline().parameters.DatabricksOutput['TableName'], ')\nINSERT logging.SynapseLakehouseSync (PoolName, SchemaName, TableName, TableRowCountADLS, TableRowCountSynapse, RowInsertDateTime)\nVALUES (\n''', pipeline().parameters.DatabricksOutput['PoolName'],''',\n''', pipeline().parameters.DatabricksOutput['SchemaName'],''',\n''', pipeline().parameters.DatabricksOutput['TableName'],''',\n', pipeline().parameters.DatabricksOutput['TableRowCountADLS'], \n', @TableRowCountSynapse, @RowInsertDateTime)\n; \n\nSELECT @TableRowCountSynapse AS TableRowCountSynapse\n')",
									"type": "Expression"
								},
								"queryTimeout": "24:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "DS_Synapse_Managed_Identity",
								"type": "DatasetReference",
								"parameters": {
									"ServerName": {
										"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
										"type": "Expression"
									},
									"DatabaseName": {
										"value": "@pipeline().parameters.DatabricksOutput['PoolName']",
										"type": "Expression"
									}
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Add TableRowCountSynapse to Json Object",
						"description": "Add the TableRowCountSynapse a json object.",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "Log Row Count - SynapseLakehouseSync",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": {
									"value": "@concat('\nDECLARE @json NVARCHAR(MAX) = '''\n, string(pipeline().parameters.DatabricksOutput)\n, ''' \n\n;WITH cte AS\n(\n\tSELECT PoolName, SynapseWorkspaceName, SynapseLakehouseSyncKey, ', activity('Log Row Count - SynapseLakehouseSync').output.value[0].TableRowCountSynapse, ' AS TableRowCountSynapse\n\t\t,SynapseSyncDataADLSFullPath, SynapseSyncDataDatabricksKeyVaultScope, SynapseSyncDataAzureKeyVaultSecretName\n\tFROM OPENJSON(@json)\n\tWITH\n\t(\n\t\tPoolName NVARCHAR(1000) ''$.PoolName''\n\t\t,SynapseWorkspaceName NVARCHAR(1000) ''$.SynapseWorkspaceName''\n\t\t,SynapseLakehouseSyncKey NVARCHAR(1000) ''$.SynapseLakehouseSyncKey''\n\t\t,SynapseSyncDataADLSFullPath NVARCHAR(1000) ''$.SynapseSyncDataADLSFullPath''\n\t\t,SynapseSyncDataDatabricksKeyVaultScope NVARCHAR(1000) ''$.SynapseSyncDataDatabricksKeyVaultScope''\n\t\t,SynapseSyncDataAzureKeyVaultSecretName NVARCHAR(1000) ''$.SynapseSyncDataAzureKeyVaultSecretName''\n\t)\n)\nSELECT *, (SELECT * FROM cte FOR JSON AUTO, WITHOUT_ARRAY_WRAPPER) AS JsonString\nFROM cte\n')",
									"type": "Expression"
								},
								"queryTimeout": "24:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "DS_Synapse_Managed_Identity",
								"type": "DatasetReference",
								"parameters": {
									"ServerName": {
										"value": "@concat(pipeline().DataFactory, '-ondemand.sql.azuresynapse.net')",
										"type": "Expression"
									},
									"DatabaseName": "master"
								}
							},
							"firstRowOnly": false
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"PipelineValue": {
						"type": "array",
						"defaultValue": [
							{
								"PipelineRunId": "d0aff143-70d5-428d-a5d9-a784efe71db8",
								"PipelineStartDate": "20220908",
								"PipelineStartDateTime": "2022-09-08 09:58:34"
							}
						]
					},
					"DatabricksOutput": {
						"type": "object",
						"defaultValue": {
							"SynapseSync_abfss": "abfss://data@synapsesynccav.dfs.core.windows.net/Sample",
							"SynapseSync_https": "https://synapsesynccav.dfs.core.windows.net/data/Sample",
							"ChangeTypes": [
								"full_load"
							],
							"TableRowCountADLS": 606,
							"ExistsFlagSynapse": "True",
							"PoolName": "DataWarehouse",
							"SchemaName": "AdventureWorks",
							"TableName": "DimProduct",
							"KeyColumns": "_Id",
							"DeltaDataADLSFullPath": "abfss://data@synapsesynccav.dfs.core.windows.net/Sample/AdventureWorks/DimProduct",
							"DeltaDataDatabricksKeyVaultScope": "DataLakeStorageKey",
							"DeltaDataAzureKeyVaultSecretName": "DataLakeStorageKey",
							"SynapseSyncDataADLSFullPath": "abfss://data@synapsesynccav.dfs.core.windows.net/Sample/",
							"SynapseSyncDataDatabricksKeyVaultScope": "DataLakeStorageKey",
							"SynapseSyncDataAzureKeyVaultSecretName": "DataLakeStorageKey",
							"SynapseWorkspaceName": "synapsesynccav",
							"DropTableFlag": "True",
							"SynapseLakehouseSyncKey": "cdae7bfac3427a0029b86bada994290d"
						}
					}
				},
				"variables": {
					"GetResourceClass": {
						"type": "String",
						"defaultValue": "DECLARE @DWU VARCHAR(8) \t,@SCHEMA_NAME VARCHAR(128) = '{SchemaNameStaging}' \t,@TABLE_NAME VARCHAR(128) = '{TableNameStaging}' \t,@Optmial_Resource_Class VARCHAR(100) \t,@Set_Resource_Class VARCHAR(1000)  SELECT @DWU = 'DW' + CAST(CASE  \t\t\tWHEN Mem > 4 \t\t\t\tTHEN Nodes * 500 \t\t\tELSE Mem * 100 \t\t\tEND AS VARCHAR(10)) + 'c' FROM ( \tSELECT Nodes = count(DISTINCT n.pdw_node_id) /*,Mem=max(i.committed_target_kb/1000/1000/60)*/ \t\t,Mem = CAST(max(i.committed_target_kb / 1000.0 / 1000 / 60) AS DECIMAL(10, 0)) \tFROM sys.dm_pdw_nodes n \tCROSS APPLY sys.dm_pdw_nodes_os_sys_info i \tWHERE type = 'COMPUTE' \t) AS A  IF OBJECT_ID('tempdb..#ref') IS NOT NULL BEGIN \tDROP TABLE #ref; END;  CREATE TABLE #ref \tWITH (DISTRIBUTION = ROUND_ROBIN) AS WITH alloc AS ( \t\tSELECT 'DW100c' AS DWU \t\t\t,4 AS max_queries \t\t\t,4 AS max_slots \t\t\t,1 AS slots_used_smallrc \t\t\t,1 AS slots_used_mediumrc \t\t\t,2 AS slots_used_largerc \t\t\t,4 AS slots_used_xlargerc \t\t\t,1 AS slots_used_staticrc10 \t\t\t,2 AS slots_used_staticrc20 \t\t\t,4 AS slots_used_staticrc30 \t\t\t,4 AS slots_used_staticrc40 \t\t\t,4 AS slots_used_staticrc50 \t\t\t,4 AS slots_used_staticrc60 \t\t\t,4 AS slots_used_staticrc70 \t\t\t,4 AS slots_used_staticrc80 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW200c' \t\t\t,8 \t\t\t,8 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,8 \t\t\t,8 \t\t\t,8 \t\t\t,8 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW300c' \t\t\t,12 \t\t\t,12 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,8 \t\t\t,8 \t\t\t,8 \t\t\t,8 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW400c' \t\t\t,16 \t\t\t,16 \t\t\t,1 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,16 \t\t\t,16 \t\t\t,16 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW500c' \t\t\t,20 \t\t\t,20 \t\t\t,1 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,16 \t\t\t,16 \t\t\t,16 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW1000c' \t\t\t,32 \t\t\t,40 \t\t\t,1 \t\t\t,4 \t\t\t,8 \t\t\t,28 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,32 \t\t\t,32 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW1500c' \t\t\t,32 \t\t\t,60 \t\t\t,1 \t\t\t,6 \t\t\t,13 \t\t\t,42 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,32 \t\t\t,32 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW2000c' \t\t\t,48 \t\t\t,80 \t\t\t,2 \t\t\t,8 \t\t\t,17 \t\t\t,56 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,64 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW2500c' \t\t\t,48 \t\t\t,100 \t\t\t,3 \t\t\t,10 \t\t\t,22 \t\t\t,70 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,64 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW3000c' \t\t\t,64 \t\t\t,120 \t\t\t,3 \t\t\t,12 \t\t\t,26 \t\t\t,84 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,64 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW5000c' \t\t\t,64 \t\t\t,200 \t\t\t,6 \t\t\t,20 \t\t\t,44 \t\t\t,140 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,128 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW6000c' \t\t\t,128 \t\t\t,240 \t\t\t,7 \t\t\t,24 \t\t\t,52 \t\t\t,168 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,128 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW7500c' \t\t\t,128 \t\t\t,300 \t\t\t,9 \t\t\t,30 \t\t\t,66 \t\t\t,210 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,128 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW10000c' \t\t\t,128 \t\t\t,400 \t\t\t,12 \t\t\t,40 \t\t\t,88 \t\t\t,280 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,128 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW15000c' \t\t\t,128 \t\t\t,600 \t\t\t,18 \t\t\t,60 \t\t\t,132 \t\t\t,420 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,128 \t\t \t\tUNION ALL \t\t \t\tSELECT 'DW30000c' \t\t\t,128 \t\t\t,1200 \t\t\t,36 \t\t\t,120 \t\t\t,264 \t\t\t,840 \t\t\t,1 \t\t\t,2 \t\t\t,4 \t\t\t,8 \t\t\t,16 \t\t\t,32 \t\t\t,64 \t\t\t,128 \t\t) \t,map AS ( \t\tSELECT CONVERT(VARCHAR(20), 'SloDWGroupSmall') AS wg_name \t\t\t,slots_used_smallrc AS slots_used \t\tFROM alloc \t\tWHERE DWU = @DWU \t\t \t\tUNION ALL \t\t \t\tSELECT CONVERT(VARCHAR(20), 'SloDWGroupMedium') AS wg_name \t\t\t,slots_used_mediumrc AS slots_used \t\tFROM alloc \t\tWHERE DWU = @DWU \t\t \t\tUNION ALL \t\t \t\tSELECT CONVERT(VARCHAR(20), 'SloDWGroupLarge') AS wg_name \t\t\t,slots_used_largerc AS slots_used \t\tFROM alloc \t\tWHERE DWU = @DWU \t\t \t\tUNION ALL \t\t \t\tSELECT CONVERT(VARCHAR(20), 'SloDWGroupXLarge') AS wg_name \t\t\t,slots_used_xlargerc AS slots_used \t\tFROM alloc \t\tWHERE DWU = @DWU \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC00' \t\t\t,1 \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC01' \t\t\t,2 \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC02' \t\t\t,4 \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC03' \t\t\t,8 \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC04' \t\t\t,16 \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC05' \t\t\t,32 \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC06' \t\t\t,64 \t\t \t\tUNION ALL \t\t \t\tSELECT 'SloDWGroupC07' \t\t\t,128 \t\t) \t,ref AS ( \t\tSELECT a1.* \t\t\t,m1.wg_name AS wg_name_smallrc \t\t\t,m1.slots_used * 250 AS tgt_mem_grant_MB_smallrc \t\t\t,m2.wg_name AS wg_name_mediumrc \t\t\t,m2.slots_used * 250 AS tgt_mem_grant_MB_mediumrc \t\t\t,m3.wg_name AS wg_name_largerc \t\t\t,m3.slots_used * 250 AS tgt_mem_grant_MB_largerc \t\t\t,m4.wg_name AS wg_name_xlargerc \t\t\t,m4.slots_used * 250 AS tgt_mem_grant_MB_xlargerc \t\t\t,m5.wg_name AS wg_name_staticrc10 \t\t\t,m5.slots_used * 250 AS tgt_mem_grant_MB_staticrc10 \t\t\t,m6.wg_name AS wg_name_staticrc20 \t\t\t,m6.slots_used * 250 AS tgt_mem_grant_MB_staticrc20 \t\t\t,m7.wg_name AS wg_name_staticrc30 \t\t\t,m7.slots_used * 250 AS tgt_mem_grant_MB_staticrc30 \t\t\t,m8.wg_name AS wg_name_staticrc40 \t\t\t,m8.slots_used * 250 AS tgt_mem_grant_MB_staticrc40 \t\t\t,m9.wg_name AS wg_name_staticrc50 \t\t\t,m9.slots_used * 250 AS tgt_mem_grant_MB_staticrc50 \t\t\t,m10.wg_name AS wg_name_staticrc60 \t\t\t,m10.slots_used * 250 AS tgt_mem_grant_MB_staticrc60 \t\t\t,m11.wg_name AS wg_name_staticrc70 \t\t\t,m11.slots_used * 250 AS tgt_mem_grant_MB_staticrc70 \t\t\t,m12.wg_name AS wg_name_staticrc80 \t\t\t,m12.slots_used * 250 AS tgt_mem_grant_MB_staticrc80 \t\tFROM alloc a1 \t\tJOIN map m1 ON a1.slots_used_smallrc = m1.slots_used \t\t\tAND m1.wg_name = 'SloDWGroupSmall' \t\tJOIN map m2 ON a1.slots_used_mediumrc = m2.slots_used \t\t\tAND m2.wg_name = 'SloDWGroupMedium' \t\tJOIN map m3 ON a1.slots_used_largerc = m3.slots_used \t\t\tAND m3.wg_name = 'SloDWGroupLarge' \t\tJOIN map m4 ON a1.slots_used_xlargerc = m4.slots_used \t\t\tAND m4.wg_name = 'SloDWGroupXLarge' \t\tJOIN map m5 ON a1.slots_used_staticrc10 = m5.slots_used \t\t\tAND m5.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tJOIN map m6 ON a1.slots_used_staticrc20 = m6.slots_used \t\t\tAND m6.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tJOIN map m7 ON a1.slots_used_staticrc30 = m7.slots_used \t\t\tAND m7.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tJOIN map m8 ON a1.slots_used_staticrc40 = m8.slots_used \t\t\tAND m8.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tJOIN map m9 ON a1.slots_used_staticrc50 = m9.slots_used \t\t\tAND m9.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tJOIN map m10 ON a1.slots_used_staticrc60 = m10.slots_used \t\t\tAND m10.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tJOIN map m11 ON a1.slots_used_staticrc70 = m11.slots_used \t\t\tAND m11.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tJOIN map m12 ON a1.slots_used_staticrc80 = m12.slots_used \t\t\tAND m12.wg_name NOT IN ( \t\t\t\t'SloDWGroupSmall' \t\t\t\t,'SloDWGroupMedium' \t\t\t\t,'SloDWGroupLarge' \t\t\t\t,'SloDWGroupXLarge' \t\t\t\t) \t\tWHERE a1.DWU = @DWU \t\t)  SELECT DWU \t,max_queries \t,max_slots \t,slots_used \t,wg_name \t,tgt_mem_grant_MB \t,up1 AS rc \t,( \t\tROW_NUMBER() OVER ( \t\t\tPARTITION BY DWU ORDER BY DWU \t\t\t) \t\t) AS rc_id FROM ( \tSELECT DWU \t\t,max_queries \t\t,max_slots \t\t,slots_used \t\t,wg_name \t\t,tgt_mem_grant_MB \t\t,REVERSE(SUBSTRING(REVERSE(wg_names), 1, CHARINDEX('_', REVERSE(wg_names), 1) - 1)) AS up1 \t\t,REVERSE(SUBSTRING(REVERSE(tgt_mem_grant_MBs), 1, CHARINDEX('_', REVERSE(tgt_mem_grant_MBs), 1) - 1)) AS up2 \t\t,REVERSE(SUBSTRING(REVERSE(slots_used_all), 1, CHARINDEX('_', REVERSE(slots_used_all), 1) - 1)) AS up3 \tFROM ref AS r1 \tUNPIVOT(wg_name FOR wg_names IN ( \t\t\t\twg_name_smallrc \t\t\t\t,wg_name_mediumrc \t\t\t\t,wg_name_largerc \t\t\t\t,wg_name_xlargerc \t\t\t\t,wg_name_staticrc10 \t\t\t\t,wg_name_staticrc20 \t\t\t\t,wg_name_staticrc30 \t\t\t\t,wg_name_staticrc40 \t\t\t\t,wg_name_staticrc50 \t\t\t\t,wg_name_staticrc60 \t\t\t\t,wg_name_staticrc70 \t\t\t\t,wg_name_staticrc80 \t\t\t\t)) AS r2 \tUNPIVOT(tgt_mem_grant_MB FOR tgt_mem_grant_MBs IN ( \t\t\t\ttgt_mem_grant_MB_smallrc \t\t\t\t,tgt_mem_grant_MB_mediumrc \t\t\t\t,tgt_mem_grant_MB_largerc \t\t\t\t,tgt_mem_grant_MB_xlargerc \t\t\t\t,tgt_mem_grant_MB_staticrc10 \t\t\t\t,tgt_mem_grant_MB_staticrc20 \t\t\t\t,tgt_mem_grant_MB_staticrc30 \t\t\t\t,tgt_mem_grant_MB_staticrc40 \t\t\t\t,tgt_mem_grant_MB_staticrc50 \t\t\t\t,tgt_mem_grant_MB_staticrc60 \t\t\t\t,tgt_mem_grant_MB_staticrc70 \t\t\t\t,tgt_mem_grant_MB_staticrc80 \t\t\t\t)) AS r3 \tUNPIVOT(slots_used FOR slots_used_all IN ( \t\t\t\tslots_used_smallrc \t\t\t\t,slots_used_mediumrc \t\t\t\t,slots_used_largerc \t\t\t\t,slots_used_xlargerc \t\t\t\t,slots_used_staticrc10 \t\t\t\t,slots_used_staticrc20 \t\t\t\t,slots_used_staticrc30 \t\t\t\t,slots_used_staticrc40 \t\t\t\t,slots_used_staticrc50 \t\t\t\t,slots_used_staticrc60 \t\t\t\t,slots_used_staticrc70 \t\t\t\t,slots_used_staticrc80 \t\t\t\t)) AS r4 \t) a WHERE up1 = up2 \tAND up1 = up3; WITH dmv AS ( \tSELECT rp.name AS rp_name \t\t,rp.max_memory_kb * 1.0 / 1048576 AS rp_max_mem_GB \t\t,(rp.max_memory_kb * 1.0 / 1024) * (request_max_memory_grant_percent / 100) AS max_memory_grant_MB \t\t,(rp.max_memory_kb * 1.0 / 1048576) * (request_max_memory_grant_percent / 100) AS max_memory_grant_GB \t\t,wg.name AS wg_name \t\t,wg.importance AS importance \t\t,wg.request_max_memory_grant_percent AS request_max_memory_grant_percent \tFROM sys.dm_pdw_nodes_resource_governor_workload_groups wg \tJOIN sys.dm_pdw_nodes_resource_governor_resource_pools rp ON wg.pdw_node_id = rp.pdw_node_id \t\tAND wg.pool_id = rp.pool_id \tWHERE rp.name = 'SloDWPool' \tGROUP BY rp.name \t\t,rp.max_memory_kb \t\t,wg.name \t\t,wg.importance \t\t,wg.request_max_memory_grant_percent \t) /* Creating resource class name mapping.*/ \t,NAMES AS ( \tSELECT 'smallrc' AS resource_class \t\t,1 AS rc_id \t \tUNION ALL \t \tSELECT 'mediumrc' \t\t,2 \t \tUNION ALL \t \tSELECT 'largerc' \t\t,3 \t \tUNION ALL \t \tSELECT 'xlargerc' \t\t,4 \t \tUNION ALL \t \tSELECT 'staticrc10' \t\t,5 \t \tUNION ALL \t \tSELECT 'staticrc20' \t\t,6 \t \tUNION ALL \t \tSELECT 'staticrc30' \t\t,7 \t \tUNION ALL \t \tSELECT 'staticrc40' \t\t,8 \t \tUNION ALL \t \tSELECT 'staticrc50' \t\t,9 \t \tUNION ALL \t \tSELECT 'staticrc60' \t\t,10 \t \tUNION ALL \t \tSELECT 'staticrc70' \t\t,11 \t \tUNION ALL \t \tSELECT 'staticrc80' \t\t,12 \t) \t,base AS ( \tSELECT schema_name \t\t,table_name \t\t,SUM(column_count) AS column_count \t\t,CONVERT(BIGINT, ISNULL(SUM(short_string_column_count), 0)) AS short_string_column_count \t\t,CONVERT(BIGINT, ISNULL(SUM(long_string_column_count), 0)) AS long_string_column_count \tFROM ( \t\tSELECT sm.name AS schema_name \t\t\t,tb.name AS table_name \t\t\t,COUNT(co.column_id) AS column_count \t\t\t,CASE  \t\t\t\tWHEN co.system_type_id IN ( \t\t\t\t\t\t36 \t\t\t\t\t\t,43 \t\t\t\t\t\t,106 \t\t\t\t\t\t,108 \t\t\t\t\t\t,165 \t\t\t\t\t\t,167 \t\t\t\t\t\t,173 \t\t\t\t\t\t,175 \t\t\t\t\t\t,231 \t\t\t\t\t\t,239 \t\t\t\t\t\t) \t\t\t\t\tAND co.max_length <= 32 \t\t\t\t\tTHEN COUNT(co.column_id) \t\t\t\tEND AS short_string_column_count \t\t\t,CASE  \t\t\t\tWHEN co.system_type_id IN ( \t\t\t\t\t\t165 \t\t\t\t\t\t,167 \t\t\t\t\t\t,173 \t\t\t\t\t\t,175 \t\t\t\t\t\t,231 \t\t\t\t\t\t,239 \t\t\t\t\t\t) \t\t\t\t\tAND co.max_length > 32 \t\t\t\t\tAND co.max_length <= 8000 \t\t\t\t\tTHEN COUNT(co.column_id) \t\t\t\tEND AS long_string_column_count \t\tFROM sys.schemas AS sm \t\tJOIN sys.tables AS tb ON sm.[schema_id] = tb.[schema_id] \t\tJOIN sys.columns AS co ON tb.[object_id] = co.[object_id] \t\tWHERE tb.name = CASE  \t\t\t\tWHEN @TABLE_NAME IS NULL \t\t\t\t\tTHEN tb.name \t\t\t\tELSE @TABLE_NAME \t\t\t\tEND \t\t\tAND sm.name = CASE  \t\t\t\tWHEN @SCHEMA_NAME IS NULL \t\t\t\t\tTHEN sm.name \t\t\t\tELSE @SCHEMA_NAME \t\t\t\tEND \t\tGROUP BY sm.name \t\t\t,tb.name \t\t\t,co.system_type_id \t\t\t,co.max_length \t\t) a \tGROUP BY schema_name \t\t,table_name \t) \t,size AS ( \tSELECT schema_name \t\t,table_name \t\t,75497472 AS table_overhead \t\t,column_count * 1048576 * 8 AS column_size \t\t,short_string_column_count * 1048576 * 32 AS short_string_size \t\t,long_string_column_count * 16777216 AS long_string_size \tFROM base \t \tUNION \t \tSELECT CASE  \t\t\tWHEN COUNT(*) = 0 \t\t\t\tTHEN 'EMPTY' \t\t\tEND AS schema_name \t\t,CASE  \t\t\tWHEN COUNT(*) = 0 \t\t\t\tTHEN 'EMPTY' \t\t\tEND AS table_name \t\t,CASE  \t\t\tWHEN COUNT(*) = 0 \t\t\t\tTHEN 0 \t\t\tEND AS table_overhead \t\t,CASE  \t\t\tWHEN COUNT(*) = 0 \t\t\t\tTHEN 0 \t\t\tEND AS column_size \t\t,CASE  \t\t\tWHEN COUNT(*) = 0 \t\t\t\tTHEN 0 \t\t\tEND AS short_string_size \t\t,CASE  \t\t\tWHEN COUNT(*) = 0 \t\t\t\tTHEN 0 \t\t\tEND AS long_string_size \tFROM base \t) \t,load_multiplier AS ( \tSELECT CASE  \t\t\tWHEN FLOOR(8 * (CAST(CAST(REPLACE(REPLACE(@DWU, 'DW', ''), 'c', '') AS INT) AS FLOAT) / 6000)) > 0 /*AND CHARINDEX(@DWU, 'c') = 0*/ \t\t\t\tAND CHARINDEX('c', @DWU) = 0 \t\t\t\tTHEN FLOOR(8 * (CAST(CAST(REPLACE(REPLACE(@DWU, 'DW', ''), 'c', '') AS INT) AS FLOAT) / 6000)) \t\t\tELSE 1 \t\t\tEND AS multiplication_factor \t) SELECT @Optmial_Resource_Class = MAX(closest_rc_in_increasing_order) FROM ( \tSELECT schema_name AS schema_name \t\t,table_name AS table_name \t\t,MIN(rc.resource_class) AS closest_rc_in_increasing_order \tFROM size AS s \t\t,load_multiplier \t\t,#ref r1 \t\t,NAMES rc \tWHERE r1.rc_id = rc.rc_id \t\tAND rc.rc_id >= 5 /*only use static rc*/ \t\tAND r1.tgt_mem_grant_MB - CAST((table_overhead * 1.0 + column_size + short_string_size + long_string_size) * multiplication_factor / 1048576 AS DECIMAL(18, 2)) > 0 /*only get rc where we have enough memory*/ \t\tAND schema_name IS NOT NULL \t\tAND table_name IS NOT NULL \tGROUP BY schema_name \t\t,table_name \t) AS a;  DECLARE @TotalRowCount BIGINT;  SELECT @TotalRowCount = SUM(CONVERT(BIGINT, nps.[row_count])) FROM sys.schemas s JOIN sys.tables t ON s.[schema_id] = t.[schema_id] JOIN sys.indexes i ON t.[object_id] = i.[object_id] \tAND i.[index_id] <= 1 JOIN sys.pdw_table_distribution_properties tp ON t.[object_id] = tp.[object_id] JOIN sys.pdw_table_mappings tm ON t.[object_id] = tm.[object_id] JOIN sys.pdw_nodes_tables nt ON tm.[physical_name] = nt.[name] JOIN sys.dm_pdw_nodes pn ON nt.[pdw_node_id] = pn.[pdw_node_id] JOIN sys.pdw_distributions di ON nt.[distribution_id] = di.[distribution_id] JOIN ( \tSELECT [object_id] \t\t,[pdw_node_id] \t\t,[distribution_id] \t\t,SUM(row_count) AS row_count \tFROM ( \t\tSELECT [object_id] \t\t\t,[pdw_node_id] \t\t\t,[distribution_id] \t\t\t,MAX(row_count) AS row_count \t\tFROM sys.dm_pdw_nodes_db_partition_stats \t\tGROUP BY [object_id] \t\t\t,[pdw_node_id] \t\t\t,[distribution_id] \t\t) AS a \tGROUP BY [object_id] \t\t,[pdw_node_id] \t\t,[distribution_id] \t) AS nps ON nt.[object_id] = nps.[object_id] \tAND nt.[pdw_node_id] = nps.[pdw_node_id] \tAND nt.[distribution_id] = nps.[distribution_id] WHERE pn.[type] = 'COMPUTE' \tAND s.name = @SCHEMA_NAME \tAND t.name = @TABLE_NAME GROUP BY s.name \t,t.name;  SELECT CONCAT ( \t\t'User' \t\t,COALESCE(CASE  \t\t\t\tWHEN REPLACE(@Optmial_Resource_Class, 'staticrc', '') < 80 \t\t\t\t\tAND @TotalRowCount >= 60000000 \t\t\t\t\tTHEN CONCAT ( \t\t\t\t\t\t\t'staticrc' \t\t\t\t\t\t\t,REPLACE(@Optmial_Resource_Class, 'staticrc', '') + 10 \t\t\t\t\t\t\t) \t\t\t\tWHEN @TotalRowCount < 60000000 \t\t\t\t\tTHEN 'staticrc10' \t\t\t\tELSE @Optmial_Resource_Class \t\t\t\tEND, 'staticrc80') \t\t) AS UserName"
					},
					"SqlCommandCreateStagingTable": {
						"type": "String",
						"defaultValue": "IF OBJECT_ID('tempdb..#tables') IS NOT NULL \tDROP TABLE #tables;  CREATE TABLE #tables ( \tSchemaName NVARCHAR(100) \t,TableName NVARCHAR(100) \t,FolderPath NVARCHAR(1000) \t);  INSERT INTO #tables VALUES ( \t'{SchemaNameStaging}' \t,'{TableNameStaging}' \t,'{FolderPathFull}' \t)  IF OBJECT_ID('tempdb..#CreateViewsDDL') IS NOT NULL \tDROP TABLE #CreateViewsDDL;  CREATE TABLE #CreateViewsDDL ( \tSchemaName NVARCHAR(100) \t,ViewName NVARCHAR(100) \t,ViewDDL NVARCHAR(MAX) \t);  DECLARE @cnt INT = 1 DECLARE @sqlCreateView NVARCHAR(MAX) DECLARE @SchemaName NVARCHAR(100) DECLARE @TableName NVARCHAR(100) DECLARE @FolderPath NVARCHAR(1000)  SELECT @SchemaName = SchemaName \t,@TableName = TableName \t,@FolderPath = FolderPath \t,@sqlCreateView = CONCAT ( \t\t'sp_describe_first_result_set @tsql=N''SELECT * FROM OPENROWSET(BULK ''''' \t\t,FolderPath \t\t,''''' , FORMAT=''''PARQUET'''') AS r''' \t\t) FROM #tables;  IF OBJECT_ID('tempdb..#InformationSchemaTempTable', 'U') IS NOT NULL \tDROP TABLE #InformationSchemaTempTable;  CREATE TABLE #InformationSchemaTempTable ( \tis_hidden BIT NOT NULL \t,column_ordinal INT NOT NULL \t,name SYSNAME NULL \t,is_nullable BIT NOT NULL \t,system_type_id INT NOT NULL \t,system_type_name NVARCHAR(256) NULL \t,max_length SMALLINT NOT NULL \t,precision TINYINT NOT NULL \t,scale TINYINT NOT NULL \t,collation_name SYSNAME NULL \t,user_type_id INT NULL \t,user_type_database SYSNAME NULL \t,user_type_schema SYSNAME NULL \t,user_type_name SYSNAME NULL \t,assembly_qualified_type_name NVARCHAR(4000) \t,xml_collection_id INT NULL \t,xml_collection_database SYSNAME NULL \t,xml_collection_schema SYSNAME NULL \t,xml_collection_name SYSNAME NULL \t,is_xml_document BIT NOT NULL \t,is_case_sensitive BIT NOT NULL \t,is_fixed_length_clr_type BIT NOT NULL \t,source_server SYSNAME NULL \t,source_database SYSNAME NULL \t,source_schema SYSNAME NULL \t,source_table SYSNAME NULL \t,source_column SYSNAME NULL \t,is_identity_column BIT NULL \t,is_part_of_unique_key BIT NULL \t,is_updateable BIT NULL \t,is_computed_column BIT NULL \t,is_sparse_column_set BIT NULL \t,ordinal_in_order_by_list SMALLINT NULL \t,order_by_list_length SMALLINT NULL \t,order_by_is_descending SMALLINT NULL \t,tds_type_id INT NOT NULL \t,tds_length INT NOT NULL \t,tds_collation_id INT NULL \t,tds_collation_sort_id TINYINT NULL \t);  INSERT INTO #InformationSchemaTempTable EXEC (@sqlCreateView) /*SELECT * FROM #InformationSchemaTempTable*/  DECLARE @GetMaxValueStatement NVARCHAR(MAX) DECLARE @GetColumnList NVARCHAR(MAX)  SELECT @GetMaxValueStatement = CONVERT(NVARCHAR(MAX), CONCAT ( \t\t\t'SELECT ' \t\t\t,STRING_AGG(ColumnMaxLength, ',') \t\t\t,' FROM OPENROWSET(BULK ''' \t\t\t,@FolderPath \t\t\t,''' , FORMAT=''PARQUET'') WITH (' \t\t\t,STRING_AGG(CONVERT(NVARCHAR(MAX), ColumnDatatypeWithMax), ',') \t\t\t,') AS r' \t\t\t)) \t,@GetColumnList = STRING_AGG(QUOTENAME([name]), ',') FROM ( \tSELECT CASE  \t\t\tWHEN system_type_name LIKE ('%char%') \t\t\t\tOR system_type_name = 'varbinary(8000)' \t\t\t\tTHEN CONCAT ( \t\t\t\t\t\t'CONVERT(BIGINT, COALESCE(NULLIF(MAX(DATALENGTH(' \t\t\t\t\t\t,QUOTENAME([name]) \t\t\t\t\t\t,')), 0), 1)) AS ' \t\t\t\t\t\t,QUOTENAME([name]) \t\t\t\t\t\t) \t\t\tELSE CONCAT ( \t\t\t\t\t'COALESCE(CONVERT(BIGINT, SUM(0)), 0) AS ' \t\t\t\t\t,QUOTENAME([name]) \t\t\t\t\t) \t\t\tEND AS ColumnMaxLength \t\t,CASE  \t\t\tWHEN system_type_name LIKE ('%char%') \t\t\t\tTHEN CONCAT ( \t\t\t\t\t\tQUOTENAME([name]) \t\t\t\t\t\t,' ' \t\t\t\t\t\t,REPLACE(system_type_name, '8000', 'MAX') \t\t\t\t\t\t,' COLLATE Latin1_General_100_BIN2_UTF8' \t\t\t\t\t\t) \t\t\tWHEN system_type_name = 'varbinary(8000)' \t\t\t\tTHEN CONCAT ( \t\t\t\t\t\tQUOTENAME([name]) \t\t\t\t\t\t,' ' \t\t\t\t\t\t,REPLACE(system_type_name, '8000', 'MAX') \t\t\t\t\t\t) \t\t\tELSE CONCAT ( \t\t\t\t\tQUOTENAME([name]) \t\t\t\t\t,' ' \t\t\t\t\t,system_type_name \t\t\t\t\t) \t\t\tEND AS ColumnDatatypeWithMax \t\t,[name] \tFROM #InformationSchemaTempTable \t) AS a /*SELECT @GetMaxValueStatement*/ /*SELECT @GetColumnList*/  DECLARE @sqlUnpivot NVARCHAR(MAX)  SET @sqlUnpivot = CONCAT ( \t\t'SELECT ''' \t\t,@TableName \t\t,''' AS TABLE_NAME, unpvt.col AS COLUMN_NAME, CASE WHEN unpvt.datatype > 8000 THEN ''MAX'' ELSE CONVERT(NVARCHAR(100), unpvt.datatype) END AS DATATYPE_MAX FROM  ( ' \t\t,@GetMaxValueStatement \t\t,' ) AS a ' \t\t,CHAR(13) \t\t,' UNPIVOT ( datatype FOR col IN  ( ' \t\t,@GetColumnList \t\t,') ) AS unpvt' \t\t)  DROP TABLE  IF EXISTS #tmpBus; \tCREATE TABLE #tmpBus ( \t\tTABLE_CLEAN NVARCHAR(1000) \t\t,COLUMN_NAME NVARCHAR(1000) \t\t,DATATYPE_MAX NVARCHAR(1000) \t\t);  INSERT INTO #tmpBus EXEC (@sqlUnpivot)  DECLARE @createFinalView NVARCHAR(MAX) DECLARE @openrowsetValue NVARCHAR(MAX)  SELECT @createFinalView = CONCAT ( \t\t'CREATE TABLE ' \t\t,QUOTENAME(@SchemaName) \t\t,'.' \t\t,QUOTENAME(@TableName) \t\t,' (' \t\t,STRING_AGG(ColumnFullDefinition, ',') \t\t,') WITH ( DISTRIBUTION = ROUND_ROBIN, HEAP)' \t\t) \t,@openrowsetValue = CONCAT ( \t\t'FROM OPENROWSET(BULK ''''' \t\t,@FolderPath \t\t,''''', FORMAT=''''PARQUET'''') WITH (' \t\t,STRING_AGG(CONVERT(NVARCHAR(MAX), ColumnFullDefinition), ',') \t\t) FROM ( \tSELECT @TableName AS table_name \t\t,c.[name] \t\t,UPPER(TYPE_NAME(c.system_type_id)) AS DataType \t\t,CONCAT ( \t\t\tQUOTENAME(c.[name]) \t\t\t,' ' \t\t\t,CASE  \t\t\t\tWHEN TYPE_NAME(c.system_type_id) IN ( \t\t\t\t\t\t'int' \t\t\t\t\t\t,'bigint' \t\t\t\t\t\t,'smallint' \t\t\t\t\t\t,'tinyint' \t\t\t\t\t\t,'bit' \t\t\t\t\t\t,'decimal' \t\t\t\t\t\t,'numeric' \t\t\t\t\t\t,'float' \t\t\t\t\t\t,'real' \t\t\t\t\t\t,'datetime2' \t\t\t\t\t\t,'date' \t\t\t\t\t\t) \t\t\t\t\tTHEN UPPER(c.system_type_name) \t\t\t\tELSE CONCAT ( \t\t\t\t\t\tUPPER(TYPE_NAME(c.system_type_id)) \t\t\t\t\t\t,'(' \t\t\t\t\t\t,a.DATATYPE_MAX \t\t\t\t\t\t,') ' \t\t\t\t\t\t) \t\t\t\tEND \t\t\t) AS ColumnFullDefinition \tFROM #InformationSchemaTempTable AS c \tJOIN #tmpBus AS a ON a.COLUMN_NAME = c.[name] \tORDER BY column_ordinal OFFSET 0 ROWS \t) AS a /*SELECT @createFinalView*/ /*INSERT INTO #CreateViewsDDL*/  SELECT @SchemaName AS SchemaName \t,@TableName AS TableName \t,CONCAT ( \t\t'IF OBJECT_ID(''' \t\t,QUOTENAME(@SchemaName) \t\t,'.' \t\t,QUOTENAME(@TableName) \t\t,''', ''U'') IS NOT NULL DROP TABLE ' \t\t,QUOTENAME(@SchemaName) \t\t,'.' \t\t,QUOTENAME(@TableName) \t\t,'; ' \t\t,@createFinalView \t\t,';' \t\t) AS CreateTableDDL;"
					},
					"CTAS": {
						"type": "String",
						"defaultValue": "EXECUTE AS USER = '{ResourceClassName}'  DECLARE @SchemaNameStaging NVARCHAR(MAX) = '{SchemaNameStaging}' \t,@TableNameStaging NVARCHAR(MAX) = '{TableNameStaging}' \t,@TableNameFinal NVARCHAR(MAX) = '{TableNameFinal}' \t,@RowCount BIGINT \t,@TableDataSpaceGB DECIMAL(36, 2);  WITH base AS ( \tSELECT s.name AS [schema_name] \t\t,t.name AS [table_name] \t\t,QUOTENAME(s.name) + '.' + QUOTENAME(t.name) AS [two_part_name] \t\t,tp.[distribution_policy_desc] AS [distribution_policy_name] \t\t,c.[name] AS [distribution_column] \t\t,i.[type_desc] AS [index_type_desc] \t\t,[reserved_page_count] AS reserved_space_page_count \t\t,[used_data] AS data_space_page_count \t\t,([reserved_page_count] - ([used_data] + ([reserved_page_count] - [used_page_count]))) AS index_space_page_count \t\t,([reserved_page_count] - [used_page_count]) AS unused_space_page_count \t\t,nps.[row_count] AS [row_count] \tFROM sys.schemas s \tJOIN sys.tables t ON s.[schema_id] = t.[schema_id] \tJOIN sys.indexes i ON t.[object_id] = i.[object_id] \t\tAND i.[index_id] <= 1 \tJOIN sys.pdw_table_distribution_properties tp ON t.[object_id] = tp.[object_id] \tJOIN sys.pdw_table_mappings tm ON t.[object_id] = tm.[object_id] \tJOIN sys.pdw_nodes_tables nt ON tm.[physical_name] = nt.[name] \tJOIN sys.dm_pdw_nodes pn ON nt.[pdw_node_id] = pn.[pdw_node_id] \tJOIN sys.pdw_distributions di ON nt.[distribution_id] = di.[distribution_id] \tJOIN ( \t\tSELECT [object_id] \t\t\t,[pdw_node_id] \t\t\t,[distribution_id] \t\t\t,SUM(row_count) AS row_count \t\t\t,SUM([reserved_page_count]) AS [reserved_page_count] \t\t\t,SUM([used_data]) AS [used_data] \t\t\t,SUM([used_page_count]) AS [used_page_count] \t\tFROM ( \t\t\tSELECT [object_id] \t\t\t\t,[pdw_node_id] \t\t\t\t,[distribution_id] \t\t\t\t,MAX(row_count) AS row_count \t\t\t\t,SUM([reserved_page_count]) AS [reserved_page_count] \t\t\t\t,MAX([used_page_count]) AS [used_data] \t\t\t\t,SUM([used_page_count]) AS [used_page_count] \t\t\tFROM sys.dm_pdw_nodes_db_partition_stats \t\t\tGROUP BY [object_id] \t\t\t\t,[pdw_node_id] \t\t\t\t,[distribution_id] \t\t\t) AS a \t\tGROUP BY [object_id] \t\t\t,[pdw_node_id] \t\t\t,[distribution_id] \t\t) AS nps ON nt.[object_id] = nps.[object_id] \t\tAND nt.[pdw_node_id] = nps.[pdw_node_id] \t\tAND nt.[distribution_id] = nps.[distribution_id] \tLEFT JOIN ( \t\tSELECT * \t\tFROM sys.pdw_column_distribution_properties \t\tWHERE distribution_ordinal = 1 \t\t) cdp ON t.[object_id] = cdp.[object_id] \tLEFT JOIN sys.columns c ON cdp.[object_id] = c.[object_id] \t\tAND cdp.[column_id] = c.[column_id] \tWHERE pn.[type] = 'COMPUTE' \t\tAND s.name = @SchemaNameStaging \t\tAND t.name = @TableNameStaging \t) \t,size AS ( \tSELECT [schema_name] \t\t,[table_name] \t\t,[distribution_policy_name] \t\t,[distribution_column] \t\t,[index_type_desc] \t\t,[index_space_page_count] \t\t,([reserved_space_page_count] * 8.0) / 1000000 AS [reserved_space_GB] \t\t,([unused_space_page_count] * 8.0) / 1000000 AS [unused_space_GB] \t\t,([data_space_page_count] * 8.0) / 1000000 AS [data_space_GB] \t\t,([index_space_page_count] * 8.0) / 1000000 AS [index_space_GB] \t\t,[row_count] AS row_count \tFROM base \t) SELECT @RowCount = SUM(row_count) /*AS [RowCount]*/ \t,@TableDataSpaceGB = SUM(data_space_GB) /*AS TableDataSpaceGB*/ FROM size GROUP BY schema_name \t,table_name \t,distribution_policy_name \t,distribution_column \t,index_type_desc ORDER BY schema_name \t,table_name \t,SUM(reserved_space_GB) DESC;/*SELECT @RowCount, @TableDataSpaceGB*/  DECLARE @sql NVARCHAR(MAX) \t,@Distribution NVARCHAR(MAX) \t,@IndexType NVARCHAR(MAX)  SELECT @sql = sqlString FROM ( \tSELECT CONCAT ( \t\t\t'IF OBJECT_ID(''tempdb..#temp'') IS NOT NULL DROP TABLE #temp; CREATE TABLE #temp WITH (DISTRIBUTION = ROUND_ROBIN, HEAP) AS  \t\t\t\t\tSELECT \t\t\t\t\t\t\t* \t\t\t\t\t\t\t,CASE WHEN DataTypeName = ''int'' THEN 1 \t\t\t\t\t\t\t\tWHEN DataTypeName IN (''numeric'', ''decimal'') AND ScaleValue = 0 THEN 1 \t\t\t\t\t\t\t\tWHEN DataTypeName = ''bigint'' THEN 2 \t\t\t\t\t\t\t\tWHEN DataTypeName = ''smallint'' THEN 2 \t\t\t\t\t\t\t\tWHEN DataTypeName IN (''char'', ''varchar'', ''nvarchar'')  \t\t\t\t\t\t\t\t\tTHEN \t\t\t\t\t\t\t\t\t\tCASE WHEN CharacterLength <= 5 THEN 3 \t\t\t\t\t\t\t\t\t\t\t\tWHEN CharacterLength <= 10 THEN 4 \t\t\t\t\t\t\t\t\t\t\t\tWHEN CharacterLength <= 20 THEN 6 \t\t\t\t\t\t\t\t\t\t\t\tWHEN CharacterLength <= 50 THEN 8 \t\t\t\t\t\t\t\t\t\t\t\tELSE 10 \t\t\t\t\t\t\t\t\t\t\tEND \t\t\t\t\t\t\t\tELSE 10 \t\t\t\t\t\t\t\tEND \t\t\t\t\t\t\t+  \t\t\t\t\t\t\tCASE WHEN NullCountPercentage < 0.01 THEN 1 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.02 THEN 2 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.03 THEN 3 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.04 THEN 4 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.05 THEN 5 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.06 THEN 6 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.07 THEN 7 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.08 THEN 8 \t\t\t\t\t\t\t\tWHEN NullCountPercentage < 0.09 THEN 9 \t\t\t\t\t\t\t\tELSE 10 \t\t\t\t\t\t\t\tEND \t\t\t\t\t\t\t+  \t\t\t\t\t\t\tCASE WHEN UniqueValueCount > 60 AND UniqueValueCountPercentage > 0.000001 THEN 1 ELSE 3\t\t\t\t\t\t\tEND + RANK() OVER (ORDER BY NullCountPercentage ASC, CASE WHEN TableRowCount > 60000000 AND UniqueValueCount >= 600 THEN -1*UniqueValueCountPercentage ELSE UniqueValueCountPercentage END DESC, UniqueValueCount DESC, ABS(LEN(''' \t\t\t,@TableNameStaging \t\t\t,''') - LEN(ColName)))/10.0 \t\t\t\t\t\t\tAS WeightedScore \t\t\t\t\t\tFROM \t\t\t\t\t\t( \t\t\t\t\t\t\tSELECT\t* \t\t\t\t\t\t\t\t\t,NullCount/(NULLIF(TableRowCount, 0)*1.0) AS NullCountPercentage \t\t\t\t\t\t\t\t\t,UniqueValueCount/(NULLIF(TableRowCount, 0)*1.0) AS UniqueValueCountPercentage  \t\t\t\t\t\t\tFROM (' \t\t\t,STRING_AGG(CONVERT(NVARCHAR(MAX), CONCAT ( \t\t\t\t\t\t'SELECT ''' \t\t\t\t\t\t,COLUMN_NAME \t\t\t\t\t\t,''' AS ColName, ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' AS DataTypeName \t\t\t\t\t\t\t\t,CASE  \t\t\t\t\t\t\t\t\t  WHEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' IN (''varchar'', ''char'') THEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' + ''('' + CASE WHEN ' \t\t\t\t\t\t,COALESCE(CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' = -1 THEN ''max'' ELSE CAST(' \t\t\t\t\t\t,COALESCE(CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' AS VARCHAR(25)) END + '')''  \t\t\t\t\t\t\t\t\t  WHEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' IN (''nvarchar'',''nchar'') THEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' + ''('' + CASE WHEN ' \t\t\t\t\t\t,COALESCE(CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' = -1 THEN ''max'' ELSE CAST(' \t\t\t\t\t\t,COALESCE(CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' / 2 AS VARCHAR(25)) END + '')''       \t\t\t\t\t\t\t\t\t  WHEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' IN (''decimal'', ''numeric'') THEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' + ''('' + CAST(' \t\t\t\t\t\t,COALESCE(CAST(NUMERIC_PRECISION AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' AS VARCHAR(25)) + '', '' + CAST(' \t\t\t\t\t\t,COALESCE(CAST(NUMERIC_SCALE AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' AS VARCHAR(25)) + '')'' \t\t\t\t\t\t\t\t\t  WHEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' IN (''datetime2'') THEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' + ''('' + CAST(' \t\t\t\t\t\t,COALESCE(CAST(NUMERIC_SCALE AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' AS VARCHAR(25)) + '')'' \t\t\t\t\t\t\t\t\t  ELSE ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' \t\t\t\t\t\t\t\t\tEND AS DataTypeFull \t\t\t\t\t\t\t\t,' \t\t\t\t\t\t,COALESCE(CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' AS CharacterLength \t\t\t\t\t\t\t\t,' \t\t\t\t\t\t,COALESCE(CAST(NUMERIC_PRECISION AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' AS PrecisionValue, ' \t\t\t\t\t\t,COALESCE(CAST(NUMERIC_SCALE AS VARCHAR(10)), 'NULL') \t\t\t\t\t\t,' AS ScaleValue \t\t\t\t\t\t\t\t,COUNT_BIG(DISTINCT ' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,') AS UniqueValueCount, COALESCE(SUM(CONVERT(BIGINT, CASE WHEN ' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,' IS NULL THEN 1 ELSE 0 END)), CONVERT(BIGINT, 0)) AS NullCount  \t\t\t\t\t\t \t\t\t\t\t\t,COALESCE(CONVERT(NVARCHAR(MAX), MIN(' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,')), '''') AS [MinValue] \t\t\t\t\t\t,COALESCE(CONVERT(NVARCHAR(MAX), MAX(' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,')), '''') AS [MaxValue] \t\t\t\t\t\t,COALESCE(MIN(DATALENGTH(' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,')), 0) AS [MinLength] \t\t\t\t\t\t,COALESCE(MAX(DATALENGTH(' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,')), 0) AS [MaxLength] \t\t\t\t\t\t \t\t\t\t\t\t \t\t\t\t\t\t,AVG(CASE WHEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' IN (''char'', ''varchar'', ''nvarchar'') THEN CONVERT(NUMERIC(30,2), NULL) ELSE ' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,' END) AS DataAverage \t\t\t\t\t\t \t\t\t\t\t\t,STDEVP(CASE WHEN ''' \t\t\t\t\t\t,DATA_TYPE \t\t\t\t\t\t,''' IN (''char'', ''varchar'', ''nvarchar'') THEN CONVERT(FLOAT, NULL) ELSE ' \t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t,' END) AS DataStdevp\t\t\t\t\t\t,' \t\t\t\t\t\t,@RowCount \t\t\t\t\t\t,' AS TableRowCount,' \t\t\t\t\t\t,@TableDataSpaceGB \t\t\t\t\t\t,' AS TableDataSpaceGB  \t\t\t\t\t\t\t\tFROM ' \t\t\t\t\t\t,QUOTENAME(TABLE_SCHEMA) \t\t\t\t\t\t,'.' \t\t\t\t\t\t,QUOTENAME(TABLE_NAME) \t\t\t\t\t\t,'' \t\t\t\t\t\t)), ' UNION ') \t\t\t,') AS a) AS a' \t\t\t,' OPTION (LABEL = ''' \t\t\t,CONCAT ( \t\t\t\t'Data Profile - ' \t\t\t\t,QUOTENAME(@SchemaNameStaging) \t\t\t\t,'.' \t\t\t\t,QUOTENAME(@TableNameStaging) \t\t\t\t,' - {PipelineRunId}' \t\t\t\t) \t\t\t,''')' \t\t\t) AS sqlString \tFROM INFORMATION_SCHEMA.COLUMNS \tWHERE TABLE_SCHEMA = @SchemaNameStaging \t\tAND TABLE_NAME = @TableNameStaging \t\tAND ( \t\t\tDATA_TYPE IN ( \t\t\t\t'int' \t\t\t\t,'bigint' \t\t\t\t,'char' \t\t\t\t,'varchar' \t\t\t\t,'nvarchar' \t\t\t\t) \t\t\tOR NUMERIC_SCALE = 0 \t\t\t) /*AND COLUMN_NAME NOT LIKE '%date%' \t\tAND COLUMN_NAME NOT LIKE '%time%'*/ \t) AS a;/*SELECT @sql*/  EXEC (@sql);  INSERT INTO logging.DataProfile SELECT '{PipelineRunId}' AS PipelineRunId \t,'{PipelineStartDate}' AS PipelineStartDate \t,'{PipelineStartDateTime}' AS PipelineStartDateTime \t,@SchemaNameStaging AS SchemaName \t,@TableNameStaging AS TableName \t,ColName \t,DataTypeName \t,DataTypeFull \t,CharacterLength \t,PrecisionValue \t,ScaleValue \t,UniqueValueCount \t,NullCount \t,MinValue \t,MaxValue \t,MinLength \t,MaxLength \t,DataAverage \t,DataStdevp \t,TableRowCount \t,TableDataSpaceGB \t,WeightedScore \t,CONCAT ( \t\t@sql \t\t,'; SELECT * FROM #temp ORDER BY WeightedScore' \t\t) AS SqlCommandDataProfile \t,CONCAT ( \t\t'CREATE TABLE [' \t\t,@SchemaNameStaging \t\t,'].[' \t\t,@TableNameFinal \t\t,'_' \t\t,ColName \t\t,'] WITH (DISTRIBUTION = HASH(' \t\t,QUOTENAME(ColName) \t\t,'), CLUSTERED COLUMNSTORE INDEX) AS SELECT * FROM [' \t\t,@SchemaNameStaging \t\t,'].[' \t\t,@TableNameFinal \t\t,'];' \t\t,' IF OBJECT_ID(''' \t\t,@SchemaNameStaging \t\t,'.' \t\t,@TableNameFinal \t\t,''', ''U'') IS NOT NULL DROP TABLE [' \t\t,@SchemaNameStaging \t\t,'].[' \t\t,@TableNameFinal \t\t,'];' \t\t,' RENAME OBJECT [' \t\t,@SchemaNameStaging \t\t,'].[' \t\t,@TableNameFinal \t\t,'_' \t\t,ColName \t\t,'] TO [' \t\t,@SchemaNameStaging \t\t,'].[' \t\t,@TableNameFinal \t\t,'];' \t\t) AS SqlCommandCTAS \t,GETDATE() AS RowInsertDateTime FROM #temp;  IF @RowCount >= 60000000 BEGIN /*Run this if row count is greater than 60M and data size on disk is less that 2GB*/ \tSELECT @sql = 'SELECT TOP 1 @Distribution = CONCAT('' WITH ( DISTRIBUTION = HASH('', ColName, ''), CLUSTERED COLUMNSTORE INDEX ) '') FROM #temp WHERE ColName NOT LIKE ''%date%'' AND ColName NOT LIKE ''%time%'' ORDER BY WeightedScore' \tFROM #temp \tWHERE ( \t\t\tColName NOT LIKE '%time%' \t\t\tAND ColName NOT LIKE '%date%' \t\t\t) /*Will need to be tweaked*/  \tEXEC sp_executesql @Query = @sql \t\t,@Params = N'@Distribution NVARCHAR(MAX) OUTPUT' \t\t,@Distribution = @Distribution OUTPUT END ELSE IF @RowCount < 60000000 \tAND @TableDataSpaceGB > 2 BEGIN \tSELECT @sql = 'SELECT TOP 1 @Distribution = CONCAT('' WITH ( DISTRIBUTION = HASH('', ColName, ''), CLUSTERED INDEX('', ColName, '') ) '') FROM #temp ORDER BY WeightedScore' \tFROM #temp  \tEXEC sp_executesql @Query = @sql \t\t,@Params = N'@Distribution NVARCHAR(MAX) OUTPUT' \t\t,@Distribution = @Distribution OUTPUT END ELSE BEGIN \tSELECT @sql = 'SELECT TOP 1 @Distribution = CONCAT('' WITH ( DISTRIBUTION = REPLICATE, CLUSTERED INDEX('', ColName, '') ) '') FROM #temp ORDER BY WeightedScore' \tFROM #temp  \tEXEC sp_executesql @Query = @sql \t\t,@Params = N'@Distribution NVARCHAR(MAX) OUTPUT' \t\t,@Distribution = @Distribution OUTPUT END  SELECT @Distribution AS DistributionIndex;"
					},
					"sqlDelete": {
						"type": "String",
						"defaultValue": "DECLARE @SchemaName NVARCHAR(1000) = '{SchemaName}' \t,@TableName NVARCHAR(1000) = '{TableName}' \t,@UserName NVARCHAR(1000) = '{UserName}' \t,@KeyColumns NVARCHAR(1000) = '{KeyColumns}' DECLARE @sql NVARCHAR(MAX)  SELECT @sql = CONCAT ( \t\t'DELETE trg FROM ' \t\t,QUOTENAME(@SchemaName) \t\t,'.' \t\t,QUOTENAME(@TableName) \t\t,' AS trg JOIN ' \t\t,QUOTENAME(@SchemaName) \t\t,'.[' \t\t,@TableName \t\t,'_delete] AS src ON ' \t\t,( \t\t\tSELECT STRING_AGG(CONCAT ( \t\t\t\t\t\t'trg.' \t\t\t\t\t\t,TRIM(value) \t\t\t\t\t\t,' = ' \t\t\t\t\t\t,'src.' \t\t\t\t\t\t,TRIM(value) \t\t\t\t\t\t), ' AND ') \t\t\tFROM STRING_SPLIT(@KeyColumns, ',') \t\t\t) \t\t);  PRINT (@sql);  EXEC (@sql);"
					},
					"sqlUpdate": {
						"type": "String",
						"defaultValue": "DECLARE @SchemaName NVARCHAR(1000) = '{SchemaName}' \t,@TableName NVARCHAR(1000) = '{TableName}' \t,@UserName NVARCHAR(1000) = '{UserName}' \t,@KeyColumns NVARCHAR(1000) = '{KeyColumns}' DECLARE @sql NVARCHAR(MAX)  SET @sql = ( \t\tSELECT CONCAT ( \t\t\t\t'EXECUTE AS user = ''' \t\t\t\t,@UserName \t\t\t\t,''' UPDATE trg SET ' \t\t\t\t,STRING_AGG(CASE  \t\t\t\t\t\tWHEN IsKeyColumnFlag = 0 \t\t\t\t\t\t\tTHEN CONCAT ( \t\t\t\t\t\t\t\t\tQUOTENAME(COLUMN_NAME) \t\t\t\t\t\t\t\t\t,' = src.' \t\t\t\t\t\t\t\t\t,QUOTENAME(COLUMN_NAME) \t\t\t\t\t\t\t\t\t) \t\t\t\t\t\tELSE NULL \t\t\t\t\t\tEND, ',') \t\t\t\t,' FROM ' \t\t\t\t,QUOTENAME(TABLE_SCHEMA) \t\t\t\t,'.' \t\t\t\t,QUOTENAME(TABLE_NAME) \t\t\t\t,' AS trg' \t\t\t\t,' JOIN ' \t\t\t\t,QUOTENAME(TABLE_SCHEMA) \t\t\t\t,'.[' \t\t\t\t,TABLE_NAME \t\t\t\t,'_update] AS src' \t\t\t\t,' ON ' \t\t\t\t,( \t\t\t\t\tSELECT STRING_AGG(CONCAT ( \t\t\t\t\t\t\t\t'trg.' \t\t\t\t\t\t\t\t,value \t\t\t\t\t\t\t\t,' = ' \t\t\t\t\t\t\t\t,'src.' \t\t\t\t\t\t\t\t,value \t\t\t\t\t\t\t\t), ' AND ') \t\t\t\t\tFROM STRING_SPLIT(@KeyColumns, ',') \t\t\t\t\t) \t\t\t\t) \t\tFROM ( \t\t\tSELECT TABLE_SCHEMA \t\t\t\t,TABLE_NAME \t\t\t\t,COLUMN_NAME \t\t\t\t,CASE  \t\t\t\t\tWHEN COLUMN_NAME IN ( \t\t\t\t\t\t\tSELECT TRIM(value) \t\t\t\t\t\t\tFROM STRING_SPLIT(@KeyColumns, ',') \t\t\t\t\t\t\t) \t\t\t\t\t\tTHEN 1 \t\t\t\t\tELSE 0 \t\t\t\t\tEND IsKeyColumnFlag \t\t\tFROM INFORMATION_SCHEMA.COLUMNS \t\t\tWHERE TABLE_SCHEMA = @SchemaName \t\t\t\tAND TABLE_NAME = @TableName \t\t\t) AS a \t\tGROUP BY TABLE_SCHEMA \t\t\t,TABLE_NAME \t\t)  PRINT (@sql);  EXEC (@sql);"
					},
					"sqlInsert": {
						"type": "String",
						"defaultValue": "DECLARE @SchemaName NVARCHAR(1000) = '{SchemaName}' \t,@TableName NVARCHAR(1000) = '{TableName}' \t,@UserName NVARCHAR(1000) = '{UserName}' DECLARE @sql NVARCHAR(MAX)  SET @sql = ( SELECT CONCAT ( \t\t'EXECUTE AS user = ''' \t\t,@UserName \t\t,''' INSERT INTO ' \t\t,QUOTENAME(TABLE_SCHEMA) \t\t,'.' \t\t,QUOTENAME(TABLE_NAME) \t\t,' (' \t\t,STRING_AGG(QUOTENAME(COLUMN_NAME), ',') \t\t,')' \t\t,' SELECT ' \t\t,STRING_AGG(QUOTENAME(COLUMN_NAME), ',') \t\t,' FROM ' \t\t,QUOTENAME(TABLE_SCHEMA) \t\t,'.[' \t\t,TABLE_NAME \t\t,'_insert]' \t\t) FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = @SchemaName \tAND TABLE_NAME = @TableName GROUP BY TABLE_SCHEMA \t,TABLE_NAME )  PRINT (@sql);  EXEC (@sql);"
					},
					"sqlAlterColumn": {
						"type": "String",
						"defaultValue": "DECLARE @SchemaName NVARCHAR(1000) = '{SchemaName}' \t,@TableNameBase NVARCHAR(1000) = '{TableNameBase}' \t,@TableNameStage NVARCHAR(1000) = '{TableNameStage}' DECLARE @sqlUpdateColumn NVARCHAR(MAX) \t,@sqlAddColumn NVARCHAR(MAX) \t,@sqlDropColumn NVARCHAR(MAX);  WITH cteBase AS ( \tSELECT s.[name] AS SchemaName \t\t,t.[name] AS TableName \t\t,c.[name] AS ColumnName \t\t,CASE  \t\t\tWHEN tp.[name] IN ( \t\t\t\t\t'varchar' \t\t\t\t\t,'char' \t\t\t\t\t) \t\t\t\tTHEN tp.[name] + '(' + CASE  \t\t\t\t\t\tWHEN c.max_length = - 1 \t\t\t\t\t\t\tTHEN 'max' \t\t\t\t\t\tELSE CAST(c.max_length AS VARCHAR(25)) \t\t\t\t\t\tEND + ')' \t\t\tWHEN tp.[name] IN ( \t\t\t\t\t'nvarchar' \t\t\t\t\t,'nchar' \t\t\t\t\t) \t\t\t\tTHEN tp.[name] + '(' + CASE  \t\t\t\t\t\tWHEN c.max_length = - 1 \t\t\t\t\t\t\tTHEN 'max' \t\t\t\t\t\tELSE CAST(c.max_length / 2 AS VARCHAR(25)) \t\t\t\t\t\tEND + ')' \t\t\tWHEN tp.[name] IN ( \t\t\t\t\t'decimal' \t\t\t\t\t,'numeric' \t\t\t\t\t) \t\t\t\tTHEN tp.[name] + '(' + CAST(c.[precision] AS VARCHAR(25)) + ', ' + CAST(c.[scale] AS VARCHAR(25)) + ')' \t\t\tWHEN tp.[name] IN ('datetime2') \t\t\t\tTHEN tp.[name] + '(' + CAST(c.[scale] AS VARCHAR(25)) + ')' \t\t\tELSE tp.[name] \t\t\tEND AS DataTypeFull \t\t,tp.[name] AS [RawType] \t\t,c.max_length AS [MaxLength] \t\t,c.[precision] AS [Precision] \t\t,c.scale AS [Scale] \tFROM sys.tables t \tJOIN sys.schemas s ON t.schema_id = s.schema_id \tJOIN sys.columns c ON t.object_id = c.object_id \tJOIN sys.types tp ON c.user_type_id = tp.user_type_id \tWHERE s.[name] = @SchemaName \t\tAND t.[name] = @TableNameBase \t) \t,cteStage AS ( \tSELECT s.[name] AS SchemaName \t\t,t.[name] AS TableName \t\t,c.[name] AS ColumnName \t\t,CASE  \t\t\tWHEN tp.[name] IN ( \t\t\t\t\t'varchar' \t\t\t\t\t,'char' \t\t\t\t\t) \t\t\t\tTHEN tp.[name] + '(' + CASE  \t\t\t\t\t\tWHEN c.max_length = - 1 \t\t\t\t\t\t\tTHEN 'max' \t\t\t\t\t\tELSE CAST(c.max_length AS VARCHAR(25)) \t\t\t\t\t\tEND + ')' \t\t\tWHEN tp.[name] IN ( \t\t\t\t\t'nvarchar' \t\t\t\t\t,'nchar' \t\t\t\t\t) \t\t\t\tTHEN tp.[name] + '(' + CASE  \t\t\t\t\t\tWHEN c.max_length = - 1 \t\t\t\t\t\t\tTHEN 'max' \t\t\t\t\t\tELSE CAST(c.max_length / 2 AS VARCHAR(25)) \t\t\t\t\t\tEND + ')' \t\t\tWHEN tp.[name] IN ( \t\t\t\t\t'decimal' \t\t\t\t\t,'numeric' \t\t\t\t\t) \t\t\t\tTHEN tp.[name] + '(' + CAST(c.[precision] AS VARCHAR(25)) + ', ' + CAST(c.[scale] AS VARCHAR(25)) + ')' \t\t\tWHEN tp.[name] IN ('datetime2') \t\t\t\tTHEN tp.[name] + '(' + CAST(c.[scale] AS VARCHAR(25)) + ')' \t\t\tELSE tp.[name] \t\t\tEND AS DataTypeFull \t\t,tp.[name] AS [RawType] \t\t,c.max_length AS [MaxLength] \t\t,c.[precision] AS [Precision] \t\t,c.scale AS [Scale] \tFROM sys.tables t \tJOIN sys.schemas s ON t.schema_id = s.schema_id \tJOIN sys.columns c ON t.object_id = c.object_id \tJOIN sys.types tp ON c.user_type_id = tp.user_type_id \tWHERE s.[name] = @SchemaName \t\tAND t.[name] = @TableNameStage \t) SELECT @sqlUpdateColumn = CONCAT ( \t\t'EXECUTE AS USER = ''Userstaticrc10''; ' \t\t,STRING_AGG(CASE  \t\t\t\tWHEN s.DataTypeFull <> b.DataTypeFull \t\t\t\t\tAND s.MaxLength > b.MaxLength \t\t\t\t\tTHEN CONCAT ( \t\t\t\t\t\t\t'ALTER TABLE ' \t\t\t\t\t\t\t,QUOTENAME(b.SchemaName) \t\t\t\t\t\t\t,'.' \t\t\t\t\t\t\t,QUOTENAME(b.TableName) \t\t\t\t\t\t\t,' ALTER COLUMN ' \t\t\t\t\t\t\t,QUOTENAME(s.ColumnName) \t\t\t\t\t\t\t,' ' \t\t\t\t\t\t\t,s.DataTypeFull \t\t\t\t\t\t\t,';' \t\t\t\t\t\t\t) \t\t\t\tELSE NULL \t\t\t\tEND, ' ') \t\t) \t,@sqlAddColumn = CONCAT ( \t\t'EXECUTE AS USER = ''Userstaticrc10''; ' \t\t,STRING_AGG(CASE  \t\t\t\tWHEN b.ColumnName IS NULL \t\t\t\t\tTHEN CONCAT ( \t\t\t\t\t\t\t'ALTER TABLE ' \t\t\t\t\t\t\t,QUOTENAME(s.SchemaName) \t\t\t\t\t\t\t,'.' \t\t\t\t\t\t\t,QUOTENAME(@TableNameBase) \t\t\t\t\t\t\t,' ADD ' \t\t\t\t\t\t\t,QUOTENAME(s.ColumnName) \t\t\t\t\t\t\t,' ' \t\t\t\t\t\t\t,s.DataTypeFull \t\t\t\t\t\t\t,';' \t\t\t\t\t\t\t) \t\t\t\tELSE NULL \t\t\t\tEND, ' ') \t\t) \t,@sqlDropColumn = CONCAT ( \t\t'EXECUTE AS USER = ''Userstaticrc10''; ' \t\t,STRING_AGG(CASE  \t\t\t\tWHEN s.ColumnName IS NULL \t\t\t\t\tTHEN CONCAT ( \t\t\t\t\t\t\t'ALTER TABLE ' \t\t\t\t\t\t\t,QUOTENAME(b.SchemaName) \t\t\t\t\t\t\t,'.' \t\t\t\t\t\t\t,QUOTENAME(b.TableName) \t\t\t\t\t\t\t,' DROP COLUMN ' \t\t\t\t\t\t\t,QUOTENAME(b.ColumnName) \t\t\t\t\t\t\t,';' \t\t\t\t\t\t\t) \t\t\t\tELSE NULL \t\t\t\tEND, ' ') \t\t) FROM cteBase AS b FULL JOIN cteStage AS s ON s.ColumnName = b.ColumnName;  PRINT (@sqlUpdateColumn); PRINT (@sqlAddColumn); PRINT (@sqlDropColumn);  EXEC (@sqlUpdateColumn); EXEC (@sqlAddColumn);  EXEC (@sqlDropColumn)"
					}
				},
				"folder": {
					"name": "Synapse Lakehouse Sync"
				},
				"annotations": [],
				"lastPublishTime": "2022-09-07T18:28:28Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_AzureDatabricks_Managed_Identity')]",
				"[concat(variables('workspaceId'), '/datasets/DS_Synapse_Managed_Identity')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SynapseLakehouseSync_Tutorial')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "The pipeline that orchestrates converting parquet files to delta format with change data feed enabled, does a full load of the data, simulates data changes to the source delta tables, run the SynapseLakehouseSync pipeline again after no data changes occurred to verify no new data is added into Synapse.",
				"activities": [
					{
						"name": "Spark - Convert Parquet to Delta Tables",
						"description": "Run an Azure Databricks notebook that will convert 6 tables in parquet format to delta 2.0 format with change data feed enabled and adding an _Id column.",
						"type": "DatabricksNotebook",
						"dependsOn": [
							{
								"activity": "Drop Synapse Tables If Exist",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebookPath": "/Synapse Lakehouse Sync Tutorial/Convert Parquet to Delta Tables - AdventureWorks",
							"baseParameters": {
								"SynapseLakehouseSyncParameters": {
									"value": "@concat('{\"DeltaDataFolderPathFull\":\"', 'abfss://', 'data', '@', pipeline().DataFactory, '.dfs.core.windows.net/Sample/', '\"'\n,', \"DatabaseName\":\"AdventureWorks\", \"SynapseWorkspaceName\":\"', pipeline().DataFactory\n, '\", \"ParquetDataADLSFullPath\":\"abfss://data@synapsesynccav.dfs.core.windows.net/Sample/\", \"ParquetDataDatabricksKeyVaultScope\":\"DataLakeStorageKey\", \"ParquetDataAzureKeyVaultSecretName\":\"DataLakeStorageKey\"}')",
									"type": "Expression"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "LS_AzureDatabricks_Managed_Identity",
							"type": "LinkedServiceReference"
						}
					},
					{
						"name": "Execute - Full Load - SynapseLakehouseSync",
						"description": "Execute the SynapseLakehouseSync pipeline to sync the AdventureWorks delta tables to the Synapse dedicated pool. This will be a full load since the data will not exist in Synapse yet.",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Spark - Convert Parquet to Delta Tables",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "SynapseLakehouseSync",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"StorageAccountNameMetadata": "https://synapsesynccav.dfs.core.windows.net/data/Synapse_Lakehouse_Sync_Metadata.csv"
							}
						}
					},
					{
						"name": "Spark - Simulate Data Changes",
						"description": "Run an Azure Databricks notebook that simulates data changes (deletes, updates, and inserts) to the AdventureWorks delta tables. ",
						"type": "DatabricksNotebook",
						"dependsOn": [
							{
								"activity": "Execute - Full Load - SynapseLakehouseSync",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebookPath": "/Synapse Lakehouse Sync Tutorial/Simulate Data Changes - AdventureWorks",
							"baseParameters": {
								"SynapseLakehouseSyncParameters": {
									"value": "@concat('{\"DeltaDataFolderPathFull\":\"', 'abfss://', 'data', '@', pipeline().DataFactory, '.dfs.core.windows.net/Sample/', '\"'\n,', \"DatabaseName\":\"AdventureWorks\", \"SynapseWorkspaceName\":\"', pipeline().DataFactory\n, '\", \"ParquetDataADLSFullPath\":\"abfss://data@synapsesynccav.dfs.core.windows.net/Sample/\", \"ParquetDataDatabricksKeyVaultScope\":\"DataLakeStorageKey\", \"ParquetDataAzureKeyVaultSecretName\":\"DataLakeStorageKey\"}')",
									"type": "Expression"
								}
							}
						},
						"linkedServiceName": {
							"referenceName": "LS_AzureDatabricks_Managed_Identity",
							"type": "LinkedServiceReference"
						}
					},
					{
						"name": "Execute - Changes - SynapseLakehouseSync",
						"description": "Execute the SynapseLakehouseSync pipeline to sync the AdventureWorks delta tables to the Synapse dedicated pool. This will be an incremental load capturing the data changes that occurred in the previous step.",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Spark - Simulate Data Changes",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "SynapseLakehouseSync",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"StorageAccountNameMetadata": "https://synapsesynccav.dfs.core.windows.net/data/Synapse_Lakehouse_Sync_Metadata.csv"
							}
						}
					},
					{
						"name": "Check table row counts for both loads",
						"description": "Query the Synapse logging table to get the row counts from the delta tables and the table in Synapse for each sync process.",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "Execute - Changes - SynapseLakehouseSync",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": "SELECT *, ABS(TableRowCountADLS - TableRowCountSynapse) AS Diff\nFROM logging.SynapseLakehouseSync\nORDER BY TableName, RowInsertDateTime\n",
								"queryTimeout": "24:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "DS_Synapse_Managed_Identity",
								"type": "DatasetReference",
								"parameters": {
									"ServerName": {
										"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
										"type": "Expression"
									},
									"DatabaseName": "DataWarehouse"
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Drop Synapse Tables If Exist",
						"description": "Drop any tables that exists in the Synapse dedicated pool. This allows for the pipeline to be executed repeatedly with the same results.",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": "DECLARE @sql NVARCHAR(MAX)\n\nSELECT @sql = STRING_AGG(CONVERT(NVARCHAR(MAX), CONCAT('IF OBJECT_ID(''', TABLE_SCHEMA, '.', TABLE_NAME, ''', ''U'') IS NOT NULL DROP TABLE ', TABLE_SCHEMA, '.', TABLE_NAME, ';')), ' ')\nFROM INFORMATION_SCHEMA.TABLES\n\n--PRINT (@sql)\nEXEC (@sql);\n\nSELECT 1 AS a",
								"queryTimeout": "24:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "DS_Synapse_Managed_Identity",
								"type": "DatasetReference",
								"parameters": {
									"ServerName": {
										"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
										"type": "Expression"
									},
									"DatabaseName": "DataWarehouse"
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Wait - 10 seconds",
						"description": "Sleep for 10 seconds between Synapse Lakehouse Sync loads.",
						"type": "Wait",
						"dependsOn": [
							{
								"activity": "Check table row counts for both loads",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"waitTimeInSeconds": 1
						}
					},
					{
						"name": "Execute - Changes - SynapseLakehouseSync_copy1",
						"description": "Execute the SynapseLakehouseSync pipeline to sync the AdventureWorks delta tables to the Synapse dedicated pool. This will not load any data since no data changes have occurred on the source delta tables.",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "Wait - 10 seconds",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "SynapseLakehouseSync",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {
								"StorageAccountNameMetadata": "https://synapsesynccav.dfs.core.windows.net/data/Synapse_Lakehouse_Sync_Metadata.csv"
							}
						}
					},
					{
						"name": "Check table row counts for both loads - No Data Changes",
						"description": "Query the Synapse logging table to get the row counts from the delta tables and the table in Synapse for each sync process. This should match the original row counts from before since no data changes should have occurred this third run.",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "Execute - Changes - SynapseLakehouseSync_copy1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": "SELECT *, ABS(TableRowCountADLS - TableRowCountSynapse) AS Diff\nFROM logging.SynapseLakehouseSync\nORDER BY TableName, RowInsertDateTime\n",
								"queryTimeout": "24:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "DS_Synapse_Managed_Identity",
								"type": "DatasetReference",
								"parameters": {
									"ServerName": {
										"value": "@concat(pipeline().DataFactory, '.sql.azuresynapse.net')",
										"type": "Expression"
									},
									"DatabaseName": "DataWarehouse"
								}
							},
							"firstRowOnly": false
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "Synapse Lakehouse Sync Tutorial"
				},
				"annotations": [],
				"lastPublishTime": "2022-09-07T19:25:27Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_AzureDatabricks_Managed_Identity')]",
				"[concat(variables('workspaceId'), '/pipelines/SynapseLakehouseSync')]",
				"[concat(variables('workspaceId'), '/datasets/DS_Synapse_Managed_Identity')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_Synapse_Managed_Identity')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LS_Synapse_Managed_Identity",
					"type": "LinkedServiceReference",
					"parameters": {
						"ServerName": {
							"value": "@dataset().ServerName",
							"type": "Expression"
						},
						"DatabaseName": {
							"value": "@dataset().DatabaseName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"ServerName": {
						"type": "string"
					},
					"DatabaseName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"schema": " ",
					"table": " "
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LS_Synapse_Managed_Identity')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDatabricks1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDatabricks",
				"typeProperties": {
					"domain": "https://adb-6192618427154573.13.azuredatabricks.net",
					"authentication": "MSI",
					"workspaceResourceId": "/subscriptions/2d66c339-6faa-4b07-94b8-e85524b0c175/resourceGroups/Azure-Synapse-Lakehouse-Sync/providers/Microsoft.Databricks/workspaces/synapsesynccav",
					"existingClusterId": "0907-164004-ut1ylz6q"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_AzureDatabricks_Managed_Identity')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDatabricks",
				"typeProperties": {
					"domain": "https://adb-6192618427154573.13.azuredatabricks.net",
					"authentication": "MSI",
					"workspaceResourceId": "/subscriptions/2d66c339-6faa-4b07-94b8-e85524b0c175/resourceGroups/Azure-Synapse-Lakehouse-Sync/providers/Microsoft.Databricks/workspaces/synapsesynccav",
					"existingClusterId": "0903-004756-h7mcvtud"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_Synapse_Managed_Identity')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This is a parameterized Linked Service using Managed Identity authentication for Integration Pipelines. It allows us to interchange Serverless SQL and multiple Dedicated SQL pools.",
				"parameters": {
					"ServerName": {
						"type": "string"
					},
					"DatabaseName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('LS_Synapse_Managed_Identity_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapsesynccav-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synapsesynccav-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapsesynccav-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('synapsesynccav-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataWarehouse')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		}
	]
}